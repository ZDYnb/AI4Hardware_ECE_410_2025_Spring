{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648b54f9",
   "metadata": {},
   "source": [
    "Ch#9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58bce2",
   "metadata": {},
   "source": [
    "here I Start to dig into the NanoGPT code construction, how really a simple gpt2 works and how the computing construction insde the code and the myster of transformer network. As I mainly focus on deployment, I will reconstruct sample.py in the nanogpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ec3eeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTConfig, GPT\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900f37d",
   "metadata": {},
   "source": [
    "As listing above, let go through first in the module in different code file and restate here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815aab54",
   "metadata": {},
   "source": [
    "The whole model is a implementation of GPT language model, all need to restate here\n",
    "# To Do: detailed anaysis of structure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af93ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819a240",
   "metadata": {},
   "source": [
    "# parameter setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0de12873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting \n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'gpt2' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"What is the answer to life, the universe, and everything?\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 5 # number of samples to draw\n",
    "max_new_tokens = 100 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b487f7",
   "metadata": {},
   "source": [
    "# Device setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97f0d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  Device type selected     : cuda\n",
      "📦  torch device name        : NVIDIA T600 Laptop GPU\n",
      "🧠  Data type (ptdtype)      : torch.bfloat16\n",
      "⚙️  Using autocast context   : autocast\n",
      "✅  TF32 matmul allowed       : True\n",
      "✅  TF32 cudnn allowed        : True\n",
      "🔬  bfloat16 supported        : True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "print(f\"🖥️  Device type selected     : {device_type}\")\n",
    "print(f\"📦  torch device name        : {torch.cuda.get_device_name() if device_type == 'cuda' else 'CPU'}\")\n",
    "print(f\"🧠  Data type (ptdtype)      : {ptdtype}\")\n",
    "print(f\"⚙️  Using autocast context   : {ctx.__class__.__name__}\")\n",
    "print(f\"✅  TF32 matmul allowed       : {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "print(f\"✅  TF32 cudnn allowed        : {torch.backends.cudnn.allow_tf32}\")\n",
    "print(f\"🔬  bfloat16 supported        : {torch.cuda.is_bf16_supported() if device_type == 'cuda' else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f75d8",
   "metadata": {},
   "source": [
    "# Model preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25883551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77224d0",
   "metadata": {},
   "source": [
    "# Runing and Bencharking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b6b485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Sample 1 | Time: 4.797s | 100 tokens | 47.97 ms/token\n",
      "What is the answer to life, the universe, and everything?\n",
      "\n",
      "In the beginning God created everything.\n",
      "\n",
      "At its creation he created and ruled over the elements and the elements were governed by Him. He made life, His creation, and His destiny. He made the world, the universe, and the universe was governed by Him. And the law was applied. The scientists and philosophers were guided, the wise men guided, and the arts guided. God is the law…and the law of the world is the laws of the world.\n",
      "\n",
      "Thus\n",
      "---------------\n",
      "⏱️ Sample 2 | Time: 1.845s | 100 tokens | 18.45 ms/token\n",
      "What is the answer to life, the universe, and everything?\n",
      "\n",
      "In the previous post, I argued that the answer is now, for more than two decades, an obvious one. To simplify the arguments, I want to offer a direct response to the claim that there are no answers to the basic question of existence. I've been arguing for a long time that there are no answers to the basic question of existence, and I've always felt that these answers are not consistent with an argument that is consistent with what is logically consistent with the whole universe. I\n",
      "---------------\n",
      "⏱️ Sample 3 | Time: 1.814s | 100 tokens | 18.14 ms/token\n",
      "What is the answer to life, the universe, and everything? Many of us think we know? But when we consider why we evolved from the laws of nature, we quickly realize how deeply intertwined we have become. This series presents questions about the nature of nature, the nature of an infinite universe, and the ontology of natural philosophy.\n",
      "\n",
      "The Philosophy of Life\n",
      "\n",
      "As the reader has probably guessed, Einstein's theory of relativity is a quite old idea. The theory was intended as an explanation for the existence of life in an infinite universe. It had\n",
      "---------------\n",
      "⏱️ Sample 4 | Time: 1.837s | 100 tokens | 18.37 ms/token\n",
      "What is the answer to life, the universe, and everything? Why do we suffer in this world; why are we led to sin? Why do we fall in sin? Why do we die in this world? Why do we fail to live in this world? Why do we go to hell? Why do we die in this world? Why do we lack a grasp of the Gospel? Why are we in a lie? The Unbearable Difficulty: With no explanation, no encouragement, no comfort, no freedom, no relief, no salvation, no joy,\n",
      "---------------\n",
      "⏱️ Sample 5 | Time: 1.807s | 100 tokens | 18.07 ms/token\n",
      "What is the answer to life, the universe, and everything?\n",
      "\n",
      "The answer to life begins with a basic belief: the universe is not.\n",
      "\n",
      "Consider it as a singularity: we built the universe around an infinite number of interconnected stars. These stars are billions of light years from us. For their billions of stars, we have the ability to perform the basic mathematical functions that are essential for a universe. For example, our planet Saturn has an internal gravitational field that is equal to the distance from Sun to Earth. Thus, our universe looks like a\n",
      "---------------\n",
      "\n",
      "✅ Benchmark Summary:\n",
      "Total time: 12.162 seconds\n",
      "Average per sample: 2.432 seconds\n",
      "Average per token: 24.32 ms/token\n",
      "Tokens per second: 41.11 tokens/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# run generation and benchmark\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        start_time = time.time()\n",
    "\n",
    "        for k in range(num_samples):\n",
    "            gen_start = time.time()\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            gen_end = time.time()\n",
    "\n",
    "            gen_time = gen_end - gen_start\n",
    "            print(f\"⏱️ Sample {k+1} | Time: {gen_time:.3f}s | {max_new_tokens} tokens | {gen_time / max_new_tokens * 1000:.2f} ms/token\")\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_token = total_time / (num_samples * max_new_tokens)\n",
    "\n",
    "        print(f\"\\n✅ Benchmark Summary:\")\n",
    "        print(f\"Total time: {total_time:.3f} seconds\")\n",
    "        print(f\"Average per sample: {total_time / num_samples:.3f} seconds\")\n",
    "        print(f\"Average per token: {avg_time_per_token * 1000:.2f} ms/token\")\n",
    "        print(f\"Tokens per second: {1.0 / avg_time_per_token:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc8795",
   "metadata": {},
   "source": [
    "python sample.py --init_from=gpt2 --start=\"What is the answer to life, the universe, and everything?\" --num_samples=5 --max_new_tokens=100 \n",
    " \n",
    "The above initial benchmark will serve as a baseline for me to make comparison later with my optimized hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223fdda",
   "metadata": {},
   "source": [
    "# Analyze the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d55f2",
   "metadata": {},
   "source": [
    "Recall from previous python Bytecode anaysis process, I decide to follow again to analyze our code using good tools\n",
    "\n",
    "First I wrap above code into a single whole python file and compile it as bytecode to do more analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b5eb87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Code Analysis\\\\__pycache__\\\\gpt2sample.cpython-310.pyc'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py_compile\n",
    "\n",
    "py_compile.compile(\"Code Analysis/gpt2sample.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c7096fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1           0 LOAD_CONST               0 ('\\nFull definition of a GPT Language Model, all of it in this single file.\\nReferences:\\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\\n2) huggingface/transformers PyTorch implementation:\\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\\n')\n",
      "              2 STORE_NAME               0 (__doc__)\n",
      "\n",
      " 10           4 LOAD_CONST               1 (0)\n",
      "              6 LOAD_CONST               2 (None)\n",
      "              8 IMPORT_NAME              1 (math)\n",
      "             10 STORE_NAME               1 (math)\n",
      "\n",
      " 11          12 LOAD_CONST               1 (0)\n",
      "             14 LOAD_CONST               2 (None)\n",
      "             16 IMPORT_NAME              2 (inspect)\n",
      "             18 STORE_NAME               2 (inspect)\n",
      "\n",
      " 12          20 LOAD_CONST               1 (0)\n",
      "             22 LOAD_CONST               3 (('dataclass',))\n",
      "             24 IMPORT_NAME              3 (dataclasses)\n",
      "             26 IMPORT_FROM              4 (dataclass)\n",
      "             28 STORE_NAME               4 (dataclass)\n",
      "             30 POP_TOP\n",
      "\n",
      " 14          32 LOAD_CONST               1 (0)\n",
      "             34 LOAD_CONST               2 (None)\n",
      "             36 IMPORT_NAME              5 (torch)\n",
      "             38 STORE_NAME               5 (torch)\n",
      "\n",
      " 15          40 LOAD_CONST               1 (0)\n",
      "             42 LOAD_CONST               2 (None)\n",
      "             44 IMPORT_NAME              6 (torch.nn)\n",
      "             46 IMPORT_FROM              7 (nn)\n",
      "             48 STORE_NAME               7 (nn)\n",
      "             50 POP_TOP\n",
      "\n",
      " 16          52 LOAD_CONST               1 (0)\n",
      "             54 LOAD_CONST               4 (('functional',))\n",
      "             56 IMPORT_NAME              6 (torch.nn)\n",
      "             58 IMPORT_FROM              8 (functional)\n",
      "             60 STORE_NAME               9 (F)\n",
      "             62 POP_TOP\n",
      "\n",
      " 18          64 LOAD_BUILD_CLASS\n",
      "             66 LOAD_CONST               5 (<code object LayerNorm at 0x000002159E130C90, file \"Code Analysis/gpt2sample.py\", line 18>)\n",
      "             68 LOAD_CONST               6 ('LayerNorm')\n",
      "             70 MAKE_FUNCTION            0\n",
      "             72 LOAD_CONST               6 ('LayerNorm')\n",
      "             74 LOAD_NAME                7 (nn)\n",
      "             76 LOAD_ATTR               10 (Module)\n",
      "             78 CALL_FUNCTION            3\n",
      "             80 STORE_NAME              11 (LayerNorm)\n",
      "\n",
      " 29          82 LOAD_BUILD_CLASS\n",
      "             84 LOAD_CONST               7 (<code object CausalSelfAttention at 0x000002159E130EA0, file \"Code Analysis/gpt2sample.py\", line 29>)\n",
      "             86 LOAD_CONST               8 ('CausalSelfAttention')\n",
      "             88 MAKE_FUNCTION            0\n",
      "             90 LOAD_CONST               8 ('CausalSelfAttention')\n",
      "             92 LOAD_NAME                7 (nn)\n",
      "             94 LOAD_ATTR               10 (Module)\n",
      "             96 CALL_FUNCTION            3\n",
      "             98 STORE_NAME              12 (CausalSelfAttention)\n",
      "\n",
      " 78         100 LOAD_BUILD_CLASS\n",
      "            102 LOAD_CONST               9 (<code object MLP at 0x000002159E1310B0, file \"Code Analysis/gpt2sample.py\", line 78>)\n",
      "            104 LOAD_CONST              10 ('MLP')\n",
      "            106 MAKE_FUNCTION            0\n",
      "            108 LOAD_CONST              10 ('MLP')\n",
      "            110 LOAD_NAME                7 (nn)\n",
      "            112 LOAD_ATTR               10 (Module)\n",
      "            114 CALL_FUNCTION            3\n",
      "            116 STORE_NAME              13 (MLP)\n",
      "\n",
      " 94         118 LOAD_BUILD_CLASS\n",
      "            120 LOAD_CONST              11 (<code object Block at 0x000002159E1312C0, file \"Code Analysis/gpt2sample.py\", line 94>)\n",
      "            122 LOAD_CONST              12 ('Block')\n",
      "            124 MAKE_FUNCTION            0\n",
      "            126 LOAD_CONST              12 ('Block')\n",
      "            128 LOAD_NAME                7 (nn)\n",
      "            130 LOAD_ATTR               10 (Module)\n",
      "            132 CALL_FUNCTION            3\n",
      "            134 STORE_NAME              14 (Block)\n",
      "\n",
      "108         136 LOAD_NAME                4 (dataclass)\n",
      "\n",
      "109         138 LOAD_BUILD_CLASS\n",
      "            140 LOAD_CONST              13 (<code object GPTConfig at 0x000002159E131370, file \"Code Analysis/gpt2sample.py\", line 108>)\n",
      "            142 LOAD_CONST              14 ('GPTConfig')\n",
      "            144 MAKE_FUNCTION            0\n",
      "            146 LOAD_CONST              14 ('GPTConfig')\n",
      "            148 CALL_FUNCTION            2\n",
      "            150 CALL_FUNCTION            1\n",
      "            152 STORE_NAME              15 (GPTConfig)\n",
      "\n",
      "118         154 LOAD_BUILD_CLASS\n",
      "            156 LOAD_CONST              15 (<code object GPT at 0x000002159E132340, file \"Code Analysis/gpt2sample.py\", line 118>)\n",
      "            158 LOAD_CONST              16 ('GPT')\n",
      "            160 MAKE_FUNCTION            0\n",
      "            162 LOAD_CONST              16 ('GPT')\n",
      "            164 LOAD_NAME                7 (nn)\n",
      "            166 LOAD_ATTR               10 (Module)\n",
      "            168 CALL_FUNCTION            3\n",
      "            170 STORE_NAME              16 (GPT)\n",
      "\n",
      "334         172 LOAD_CONST              17 ('gpt2')\n",
      "            174 STORE_NAME              17 (init_from)\n",
      "\n",
      "335         176 LOAD_CONST              18 ('out')\n",
      "            178 STORE_NAME              18 (out_dir)\n",
      "\n",
      "336         180 LOAD_CONST              19 ('What is the answer to life, the universe, and everything?')\n",
      "            182 STORE_NAME              19 (start)\n",
      "\n",
      "337         184 LOAD_CONST              20 (5)\n",
      "            186 STORE_NAME              20 (num_samples)\n",
      "\n",
      "338         188 LOAD_CONST              21 (100)\n",
      "            190 STORE_NAME              21 (max_new_tokens)\n",
      "\n",
      "339         192 LOAD_CONST              22 (0.8)\n",
      "            194 STORE_NAME              22 (temperature)\n",
      "\n",
      "340         196 LOAD_CONST              23 (200)\n",
      "            198 STORE_NAME              23 (top_k)\n",
      "\n",
      "341         200 LOAD_CONST              24 (1337)\n",
      "            202 STORE_NAME              24 (seed)\n",
      "\n",
      "342         204 LOAD_CONST              25 ('cuda')\n",
      "            206 STORE_NAME              25 (device)\n",
      "\n",
      "343         208 LOAD_NAME                5 (torch)\n",
      "            210 LOAD_ATTR               26 (cuda)\n",
      "            212 LOAD_METHOD             27 (is_available)\n",
      "            214 CALL_METHOD              0\n",
      "            216 POP_JUMP_IF_FALSE      116 (to 232)\n",
      "            218 LOAD_NAME                5 (torch)\n",
      "            220 LOAD_ATTR               26 (cuda)\n",
      "            222 LOAD_METHOD             28 (is_bf16_supported)\n",
      "            224 CALL_METHOD              0\n",
      "            226 POP_JUMP_IF_FALSE      116 (to 232)\n",
      "            228 LOAD_CONST              26 ('bfloat16')\n",
      "            230 JUMP_FORWARD             1 (to 234)\n",
      "        >>  232 LOAD_CONST              27 ('float16')\n",
      "        >>  234 STORE_NAME              29 (dtype)\n",
      "\n",
      "344         236 LOAD_CONST              28 (False)\n",
      "            238 STORE_NAME              30 (compile)\n",
      "\n",
      "348         240 LOAD_NAME                5 (torch)\n",
      "            242 LOAD_METHOD             31 (manual_seed)\n",
      "            244 LOAD_NAME               24 (seed)\n",
      "            246 CALL_METHOD              1\n",
      "            248 POP_TOP\n",
      "\n",
      "349         250 LOAD_NAME                5 (torch)\n",
      "            252 LOAD_ATTR               26 (cuda)\n",
      "            254 LOAD_METHOD             31 (manual_seed)\n",
      "            256 LOAD_NAME               24 (seed)\n",
      "            258 CALL_METHOD              1\n",
      "            260 POP_TOP\n",
      "\n",
      "350         262 LOAD_CONST              29 (True)\n",
      "            264 LOAD_NAME                5 (torch)\n",
      "            266 LOAD_ATTR               32 (backends)\n",
      "            268 LOAD_ATTR               26 (cuda)\n",
      "            270 LOAD_ATTR               33 (matmul)\n",
      "            272 STORE_ATTR              34 (allow_tf32)\n",
      "\n",
      "351         274 LOAD_CONST              29 (True)\n",
      "            276 LOAD_NAME                5 (torch)\n",
      "            278 LOAD_ATTR               32 (backends)\n",
      "            280 LOAD_ATTR               35 (cudnn)\n",
      "            282 STORE_ATTR              34 (allow_tf32)\n",
      "\n",
      "352         284 LOAD_CONST              25 ('cuda')\n",
      "            286 LOAD_NAME               25 (device)\n",
      "            288 CONTAINS_OP              0\n",
      "            290 POP_JUMP_IF_FALSE      148 (to 296)\n",
      "            292 LOAD_CONST              25 ('cuda')\n",
      "            294 JUMP_FORWARD             1 (to 298)\n",
      "        >>  296 LOAD_CONST              30 ('cpu')\n",
      "        >>  298 STORE_NAME              36 (device_type)\n",
      "\n",
      "353         300 LOAD_NAME                5 (torch)\n",
      "            302 LOAD_ATTR               37 (float32)\n",
      "            304 LOAD_NAME                5 (torch)\n",
      "            306 LOAD_ATTR               38 (bfloat16)\n",
      "            308 LOAD_NAME                5 (torch)\n",
      "            310 LOAD_ATTR               39 (float16)\n",
      "            312 LOAD_CONST              31 (('float32', 'bfloat16', 'float16'))\n",
      "            314 BUILD_CONST_KEY_MAP      3\n",
      "            316 LOAD_NAME               29 (dtype)\n",
      "            318 BINARY_SUBSCR\n",
      "            320 STORE_NAME              40 (ptdtype)\n",
      "\n",
      "354         322 LOAD_NAME               36 (device_type)\n",
      "            324 LOAD_CONST              30 ('cpu')\n",
      "            326 COMPARE_OP               2 (==)\n",
      "            328 POP_JUMP_IF_FALSE      168 (to 336)\n",
      "            330 LOAD_NAME               41 (nullcontext)\n",
      "            332 CALL_FUNCTION            0\n",
      "            334 JUMP_FORWARD             7 (to 350)\n",
      "        >>  336 LOAD_NAME                5 (torch)\n",
      "            338 LOAD_ATTR               42 (amp)\n",
      "            340 LOAD_ATTR               43 (autocast)\n",
      "            342 LOAD_NAME               36 (device_type)\n",
      "            344 LOAD_NAME               40 (ptdtype)\n",
      "            346 LOAD_CONST              32 (('device_type', 'dtype'))\n",
      "            348 CALL_FUNCTION_KW         2\n",
      "        >>  350 STORE_NAME              44 (ctx)\n",
      "\n",
      "357         352 LOAD_NAME               17 (init_from)\n",
      "            354 LOAD_CONST              33 ('resume')\n",
      "            356 COMPARE_OP               2 (==)\n",
      "            358 POP_JUMP_IF_FALSE      248 (to 496)\n",
      "\n",
      "359         360 LOAD_NAME               45 (os)\n",
      "            362 LOAD_ATTR               46 (path)\n",
      "            364 LOAD_METHOD             47 (join)\n",
      "            366 LOAD_NAME               18 (out_dir)\n",
      "            368 LOAD_CONST              34 ('ckpt.pt')\n",
      "            370 CALL_METHOD              2\n",
      "            372 STORE_NAME              48 (ckpt_path)\n",
      "\n",
      "360         374 LOAD_NAME                5 (torch)\n",
      "            376 LOAD_ATTR               49 (load)\n",
      "            378 LOAD_NAME               48 (ckpt_path)\n",
      "            380 LOAD_NAME               25 (device)\n",
      "            382 LOAD_CONST              35 (('map_location',))\n",
      "            384 CALL_FUNCTION_KW         2\n",
      "            386 STORE_NAME              50 (checkpoint)\n",
      "\n",
      "361         388 LOAD_NAME               15 (GPTConfig)\n",
      "            390 LOAD_CONST              63 (())\n",
      "            392 BUILD_MAP                0\n",
      "            394 LOAD_NAME               50 (checkpoint)\n",
      "            396 LOAD_CONST              36 ('model_args')\n",
      "            398 BINARY_SUBSCR\n",
      "            400 DICT_MERGE               1\n",
      "            402 CALL_FUNCTION_EX         1\n",
      "            404 STORE_NAME              51 (gptconf)\n",
      "\n",
      "362         406 LOAD_NAME               16 (GPT)\n",
      "            408 LOAD_NAME               51 (gptconf)\n",
      "            410 CALL_FUNCTION            1\n",
      "            412 STORE_NAME              52 (model)\n",
      "\n",
      "363         414 LOAD_NAME               50 (checkpoint)\n",
      "            416 LOAD_CONST              37 ('model')\n",
      "            418 BINARY_SUBSCR\n",
      "            420 STORE_NAME              53 (state_dict)\n",
      "\n",
      "364         422 LOAD_CONST              38 ('_orig_mod.')\n",
      "            424 STORE_NAME              54 (unwanted_prefix)\n",
      "\n",
      "365         426 LOAD_NAME               55 (list)\n",
      "            428 LOAD_NAME               53 (state_dict)\n",
      "            430 LOAD_METHOD             56 (items)\n",
      "            432 CALL_METHOD              0\n",
      "            434 CALL_FUNCTION            1\n",
      "            436 GET_ITER\n",
      "        >>  438 FOR_ITER                22 (to 484)\n",
      "            440 UNPACK_SEQUENCE          2\n",
      "            442 STORE_NAME              57 (k)\n",
      "            444 STORE_NAME              58 (v)\n",
      "\n",
      "366         446 LOAD_NAME               57 (k)\n",
      "            448 LOAD_METHOD             59 (startswith)\n",
      "            450 LOAD_NAME               54 (unwanted_prefix)\n",
      "            452 CALL_METHOD              1\n",
      "            454 POP_JUMP_IF_FALSE      241 (to 482)\n",
      "\n",
      "367         456 LOAD_NAME               53 (state_dict)\n",
      "            458 LOAD_METHOD             60 (pop)\n",
      "            460 LOAD_NAME               57 (k)\n",
      "            462 CALL_METHOD              1\n",
      "            464 LOAD_NAME               53 (state_dict)\n",
      "            466 LOAD_NAME               57 (k)\n",
      "            468 LOAD_NAME               61 (len)\n",
      "            470 LOAD_NAME               54 (unwanted_prefix)\n",
      "            472 CALL_FUNCTION            1\n",
      "            474 LOAD_CONST               2 (None)\n",
      "            476 BUILD_SLICE              2\n",
      "            478 BINARY_SUBSCR\n",
      "            480 STORE_SUBSCR\n",
      "        >>  482 JUMP_ABSOLUTE          219 (to 438)\n",
      "\n",
      "368     >>  484 LOAD_NAME               52 (model)\n",
      "            486 LOAD_METHOD             62 (load_state_dict)\n",
      "            488 LOAD_NAME               53 (state_dict)\n",
      "            490 CALL_METHOD              1\n",
      "            492 POP_TOP\n",
      "            494 JUMP_FORWARD            15 (to 526)\n",
      "\n",
      "369     >>  496 LOAD_NAME               17 (init_from)\n",
      "            498 LOAD_METHOD             59 (startswith)\n",
      "            500 LOAD_CONST              17 ('gpt2')\n",
      "            502 CALL_METHOD              1\n",
      "            504 EXTENDED_ARG             1\n",
      "            506 POP_JUMP_IF_FALSE      263 (to 526)\n",
      "\n",
      "371         508 LOAD_NAME               16 (GPT)\n",
      "            510 LOAD_METHOD             63 (from_pretrained)\n",
      "            512 LOAD_NAME               17 (init_from)\n",
      "            514 LOAD_NAME               64 (dict)\n",
      "            516 LOAD_CONST              39 (0.0)\n",
      "            518 LOAD_CONST              40 (('dropout',))\n",
      "            520 CALL_FUNCTION_KW         1\n",
      "            522 CALL_METHOD              2\n",
      "            524 STORE_NAME              52 (model)\n",
      "\n",
      "373     >>  526 LOAD_NAME               52 (model)\n",
      "            528 LOAD_METHOD             65 (eval)\n",
      "            530 CALL_METHOD              0\n",
      "            532 POP_TOP\n",
      "\n",
      "374         534 LOAD_NAME               52 (model)\n",
      "            536 LOAD_METHOD             66 (to)\n",
      "            538 LOAD_NAME               25 (device)\n",
      "            540 CALL_METHOD              1\n",
      "            542 POP_TOP\n",
      "\n",
      "375         544 LOAD_NAME               30 (compile)\n",
      "            546 EXTENDED_ARG             1\n",
      "            548 POP_JUMP_IF_FALSE      280 (to 560)\n",
      "\n",
      "376         550 LOAD_NAME                5 (torch)\n",
      "            552 LOAD_METHOD             30 (compile)\n",
      "            554 LOAD_NAME               52 (model)\n",
      "            556 CALL_METHOD              1\n",
      "            558 STORE_NAME              52 (model)\n",
      "\n",
      "379     >>  560 LOAD_CONST              28 (False)\n",
      "            562 STORE_NAME              67 (load_meta)\n",
      "\n",
      "380         564 LOAD_NAME               17 (init_from)\n",
      "            566 LOAD_CONST              33 ('resume')\n",
      "            568 COMPARE_OP               2 (==)\n",
      "            570 EXTENDED_ARG             1\n",
      "            572 POP_JUMP_IF_FALSE      317 (to 634)\n",
      "            574 LOAD_CONST              41 ('config')\n",
      "            576 LOAD_NAME               50 (checkpoint)\n",
      "            578 CONTAINS_OP              0\n",
      "            580 EXTENDED_ARG             1\n",
      "            582 POP_JUMP_IF_FALSE      317 (to 634)\n",
      "            584 LOAD_CONST              42 ('dataset')\n",
      "            586 LOAD_NAME               50 (checkpoint)\n",
      "            588 LOAD_CONST              41 ('config')\n",
      "            590 BINARY_SUBSCR\n",
      "            592 CONTAINS_OP              0\n",
      "            594 EXTENDED_ARG             1\n",
      "            596 POP_JUMP_IF_FALSE      317 (to 634)\n",
      "\n",
      "381         598 LOAD_NAME               45 (os)\n",
      "            600 LOAD_ATTR               46 (path)\n",
      "            602 LOAD_METHOD             47 (join)\n",
      "            604 LOAD_CONST              43 ('data')\n",
      "            606 LOAD_NAME               50 (checkpoint)\n",
      "            608 LOAD_CONST              41 ('config')\n",
      "            610 BINARY_SUBSCR\n",
      "            612 LOAD_CONST              42 ('dataset')\n",
      "            614 BINARY_SUBSCR\n",
      "            616 LOAD_CONST              44 ('meta.pkl')\n",
      "            618 CALL_METHOD              3\n",
      "            620 STORE_NAME              68 (meta_path)\n",
      "\n",
      "382         622 LOAD_NAME               45 (os)\n",
      "            624 LOAD_ATTR               46 (path)\n",
      "            626 LOAD_METHOD             69 (exists)\n",
      "            628 LOAD_NAME               68 (meta_path)\n",
      "            630 CALL_METHOD              1\n",
      "            632 STORE_NAME              67 (load_meta)\n",
      "\n",
      "383     >>  634 LOAD_NAME               67 (load_meta)\n",
      "            636 EXTENDED_ARG             1\n",
      "            638 POP_JUMP_IF_FALSE      373 (to 746)\n",
      "\n",
      "384         640 LOAD_NAME               70 (print)\n",
      "            642 LOAD_CONST              45 ('Loading meta from ')\n",
      "            644 LOAD_NAME               68 (meta_path)\n",
      "            646 FORMAT_VALUE             0\n",
      "            648 LOAD_CONST              46 ('...')\n",
      "            650 BUILD_STRING             3\n",
      "            652 CALL_FUNCTION            1\n",
      "            654 POP_TOP\n",
      "\n",
      "385         656 LOAD_NAME               71 (open)\n",
      "            658 LOAD_NAME               68 (meta_path)\n",
      "            660 LOAD_CONST              47 ('rb')\n",
      "            662 CALL_FUNCTION            2\n",
      "            664 SETUP_WITH              13 (to 692)\n",
      "            666 STORE_NAME              72 (f)\n",
      "\n",
      "386         668 LOAD_NAME               73 (pickle)\n",
      "            670 LOAD_METHOD             49 (load)\n",
      "            672 LOAD_NAME               72 (f)\n",
      "            674 CALL_METHOD              1\n",
      "            676 STORE_NAME              74 (meta)\n",
      "            678 POP_BLOCK\n",
      "\n",
      "385         680 LOAD_CONST               2 (None)\n",
      "            682 DUP_TOP\n",
      "            684 DUP_TOP\n",
      "            686 CALL_FUNCTION            3\n",
      "            688 POP_TOP\n",
      "            690 JUMP_FORWARD             9 (to 710)\n",
      "        >>  692 WITH_EXCEPT_START\n",
      "            694 EXTENDED_ARG             1\n",
      "            696 POP_JUMP_IF_TRUE       350 (to 700)\n",
      "            698 RERAISE                  1\n",
      "        >>  700 POP_TOP\n",
      "            702 POP_TOP\n",
      "            704 POP_TOP\n",
      "            706 POP_EXCEPT\n",
      "            708 POP_TOP\n",
      "\n",
      "388     >>  710 LOAD_NAME               74 (meta)\n",
      "            712 LOAD_CONST              48 ('stoi')\n",
      "            714 BINARY_SUBSCR\n",
      "            716 LOAD_NAME               74 (meta)\n",
      "            718 LOAD_CONST              49 ('itos')\n",
      "            720 BINARY_SUBSCR\n",
      "            722 ROT_TWO\n",
      "            724 STORE_NAME              75 (stoi)\n",
      "            726 STORE_NAME              76 (itos)\n",
      "\n",
      "389         728 LOAD_CONST              50 (<code object <lambda> at 0x000002159E1324A0, file \"Code Analysis/gpt2sample.py\", line 389>)\n",
      "            730 LOAD_CONST              51 ('<lambda>')\n",
      "            732 MAKE_FUNCTION            0\n",
      "            734 STORE_NAME              77 (encode)\n",
      "\n",
      "390         736 LOAD_CONST              52 (<code object <lambda> at 0x000002159E132600, file \"Code Analysis/gpt2sample.py\", line 390>)\n",
      "            738 LOAD_CONST              51 ('<lambda>')\n",
      "            740 MAKE_FUNCTION            0\n",
      "            742 STORE_NAME              78 (decode)\n",
      "            744 JUMP_FORWARD            17 (to 780)\n",
      "\n",
      "393     >>  746 LOAD_NAME               70 (print)\n",
      "            748 LOAD_CONST              53 ('No meta.pkl found, assuming GPT-2 encodings...')\n",
      "            750 CALL_FUNCTION            1\n",
      "            752 POP_TOP\n",
      "\n",
      "394         754 LOAD_NAME               79 (tiktoken)\n",
      "            756 LOAD_METHOD             80 (get_encoding)\n",
      "            758 LOAD_CONST              17 ('gpt2')\n",
      "            760 CALL_METHOD              1\n",
      "            762 STORE_NAME              81 (enc)\n",
      "\n",
      "395         764 LOAD_CONST              54 (<code object <lambda> at 0x000002159E1326B0, file \"Code Analysis/gpt2sample.py\", line 395>)\n",
      "            766 LOAD_CONST              51 ('<lambda>')\n",
      "            768 MAKE_FUNCTION            0\n",
      "            770 STORE_NAME              77 (encode)\n",
      "\n",
      "396         772 LOAD_CONST              55 (<code object <lambda> at 0x000002159E132760, file \"Code Analysis/gpt2sample.py\", line 396>)\n",
      "            774 LOAD_CONST              51 ('<lambda>')\n",
      "            776 MAKE_FUNCTION            0\n",
      "            778 STORE_NAME              78 (decode)\n",
      "\n",
      "399     >>  780 LOAD_NAME               19 (start)\n",
      "            782 LOAD_METHOD             59 (startswith)\n",
      "            784 LOAD_CONST              56 ('FILE:')\n",
      "            786 CALL_METHOD              1\n",
      "            788 EXTENDED_ARG             1\n",
      "            790 POP_JUMP_IF_FALSE      428 (to 856)\n",
      "\n",
      "400         792 LOAD_NAME               71 (open)\n",
      "            794 LOAD_NAME               19 (start)\n",
      "            796 LOAD_CONST              20 (5)\n",
      "            798 LOAD_CONST               2 (None)\n",
      "            800 BUILD_SLICE              2\n",
      "            802 BINARY_SUBSCR\n",
      "            804 LOAD_CONST              57 ('r')\n",
      "            806 LOAD_CONST              58 ('utf-8')\n",
      "            808 LOAD_CONST              59 (('encoding',))\n",
      "            810 CALL_FUNCTION_KW         3\n",
      "            812 SETUP_WITH              12 (to 838)\n",
      "            814 STORE_NAME              72 (f)\n",
      "\n",
      "401         816 LOAD_NAME               72 (f)\n",
      "            818 LOAD_METHOD             82 (read)\n",
      "            820 CALL_METHOD              0\n",
      "            822 STORE_NAME              19 (start)\n",
      "            824 POP_BLOCK\n",
      "\n",
      "400         826 LOAD_CONST               2 (None)\n",
      "            828 DUP_TOP\n",
      "            830 DUP_TOP\n",
      "            832 CALL_FUNCTION            3\n",
      "            834 POP_TOP\n",
      "            836 JUMP_FORWARD             9 (to 856)\n",
      "        >>  838 WITH_EXCEPT_START\n",
      "            840 EXTENDED_ARG             1\n",
      "            842 POP_JUMP_IF_TRUE       423 (to 846)\n",
      "            844 RERAISE                  1\n",
      "        >>  846 POP_TOP\n",
      "            848 POP_TOP\n",
      "            850 POP_TOP\n",
      "            852 POP_EXCEPT\n",
      "            854 POP_TOP\n",
      "\n",
      "402     >>  856 LOAD_NAME               77 (encode)\n",
      "            858 LOAD_NAME               19 (start)\n",
      "            860 CALL_FUNCTION            1\n",
      "            862 STORE_NAME              83 (start_ids)\n",
      "\n",
      "403         864 LOAD_NAME                5 (torch)\n",
      "            866 LOAD_ATTR               84 (tensor)\n",
      "            868 LOAD_NAME               83 (start_ids)\n",
      "            870 LOAD_NAME                5 (torch)\n",
      "            872 LOAD_ATTR               85 (long)\n",
      "            874 LOAD_NAME               25 (device)\n",
      "            876 LOAD_CONST              60 (('dtype', 'device'))\n",
      "            878 CALL_FUNCTION_KW         3\n",
      "            880 LOAD_CONST              61 ((None, Ellipsis))\n",
      "            882 BINARY_SUBSCR\n",
      "            884 STORE_NAME              86 (x)\n",
      "\n",
      "405         886 LOAD_CONST               1 (0)\n",
      "            888 LOAD_CONST               2 (None)\n",
      "            890 IMPORT_NAME             87 (time)\n",
      "            892 STORE_NAME              87 (time)\n",
      "\n",
      "408         894 LOAD_NAME                5 (torch)\n",
      "            896 LOAD_METHOD             88 (no_grad)\n",
      "            898 CALL_METHOD              0\n",
      "            900 SETUP_WITH              53 (to 1008)\n",
      "            902 POP_TOP\n",
      "\n",
      "409         904 LOAD_NAME               44 (ctx)\n",
      "            906 SETUP_WITH              25 (to 958)\n",
      "            908 POP_TOP\n",
      "\n",
      "410         910 LOAD_NAME               89 (range)\n",
      "            912 LOAD_NAME               20 (num_samples)\n",
      "            914 CALL_FUNCTION            1\n",
      "            916 GET_ITER\n",
      "        >>  918 FOR_ITER                12 (to 944)\n",
      "            920 STORE_NAME              57 (k)\n",
      "\n",
      "411         922 LOAD_NAME               52 (model)\n",
      "            924 LOAD_ATTR               90 (generate)\n",
      "            926 LOAD_NAME               86 (x)\n",
      "            928 LOAD_NAME               21 (max_new_tokens)\n",
      "            930 LOAD_NAME               22 (temperature)\n",
      "            932 LOAD_NAME               23 (top_k)\n",
      "            934 LOAD_CONST              62 (('temperature', 'top_k'))\n",
      "            936 CALL_FUNCTION_KW         4\n",
      "            938 STORE_NAME              91 (y)\n",
      "            940 EXTENDED_ARG             1\n",
      "            942 JUMP_ABSOLUTE          459 (to 918)\n",
      "\n",
      "410     >>  944 POP_BLOCK\n",
      "\n",
      "409         946 LOAD_CONST               2 (None)\n",
      "            948 DUP_TOP\n",
      "            950 DUP_TOP\n",
      "            952 CALL_FUNCTION            3\n",
      "            954 POP_TOP\n",
      "            956 JUMP_FORWARD            17 (to 992)\n",
      "        >>  958 WITH_EXCEPT_START\n",
      "            960 EXTENDED_ARG             1\n",
      "            962 POP_JUMP_IF_TRUE       483 (to 966)\n",
      "            964 RERAISE                  1\n",
      "        >>  966 POP_TOP\n",
      "            968 POP_TOP\n",
      "            970 POP_TOP\n",
      "            972 POP_EXCEPT\n",
      "            974 POP_TOP\n",
      "            976 POP_BLOCK\n",
      "\n",
      "408         978 LOAD_CONST               2 (None)\n",
      "            980 DUP_TOP\n",
      "            982 DUP_TOP\n",
      "            984 CALL_FUNCTION            3\n",
      "            986 POP_TOP\n",
      "            988 LOAD_CONST               2 (None)\n",
      "            990 RETURN_VALUE\n",
      "\n",
      "409     >>  992 POP_BLOCK\n",
      "\n",
      "408         994 LOAD_CONST               2 (None)\n",
      "            996 DUP_TOP\n",
      "            998 DUP_TOP\n",
      "           1000 CALL_FUNCTION            3\n",
      "           1002 POP_TOP\n",
      "           1004 LOAD_CONST               2 (None)\n",
      "           1006 RETURN_VALUE\n",
      "        >> 1008 WITH_EXCEPT_START\n",
      "           1010 EXTENDED_ARG             1\n",
      "           1012 POP_JUMP_IF_TRUE       508 (to 1016)\n",
      "           1014 RERAISE                  1\n",
      "        >> 1016 POP_TOP\n",
      "           1018 POP_TOP\n",
      "           1020 POP_TOP\n",
      "           1022 POP_EXCEPT\n",
      "           1024 POP_TOP\n",
      "           1026 LOAD_CONST               2 (None)\n",
      "           1028 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object LayerNorm at 0x000002159E130C90, file \"Code Analysis/gpt2sample.py\", line 18>:\n",
      " 18           0 LOAD_NAME                0 (__name__)\n",
      "              2 STORE_NAME               1 (__module__)\n",
      "              4 LOAD_CONST               0 ('LayerNorm')\n",
      "              6 STORE_NAME               2 (__qualname__)\n",
      "\n",
      " 19           8 LOAD_CONST               1 (\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \")\n",
      "             10 STORE_NAME               3 (__doc__)\n",
      "\n",
      " 21          12 LOAD_CLOSURE             0 (__class__)\n",
      "             14 BUILD_TUPLE              1\n",
      "             16 LOAD_CONST               2 (<code object __init__ at 0x000002159E130190, file \"Code Analysis/gpt2sample.py\", line 21>)\n",
      "             18 LOAD_CONST               3 ('LayerNorm.__init__')\n",
      "             20 MAKE_FUNCTION            8 (closure)\n",
      "             22 STORE_NAME               4 (__init__)\n",
      "\n",
      " 26          24 LOAD_CONST               4 (<code object forward at 0x000002159E1307C0, file \"Code Analysis/gpt2sample.py\", line 26>)\n",
      "             26 LOAD_CONST               5 ('LayerNorm.forward')\n",
      "             28 MAKE_FUNCTION            0\n",
      "             30 STORE_NAME               5 (forward)\n",
      "             32 LOAD_CLOSURE             0 (__class__)\n",
      "             34 DUP_TOP\n",
      "             36 STORE_NAME               6 (__classcell__)\n",
      "             38 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object __init__ at 0x000002159E130190, file \"Code Analysis/gpt2sample.py\", line 21>:\n",
      " 22           0 LOAD_GLOBAL              0 (super)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 LOAD_METHOD              1 (__init__)\n",
      "              6 CALL_METHOD              0\n",
      "              8 POP_TOP\n",
      "\n",
      " 23          10 LOAD_GLOBAL              2 (nn)\n",
      "             12 LOAD_METHOD              3 (Parameter)\n",
      "             14 LOAD_GLOBAL              4 (torch)\n",
      "             16 LOAD_METHOD              5 (ones)\n",
      "             18 LOAD_FAST                1 (ndim)\n",
      "             20 CALL_METHOD              1\n",
      "             22 CALL_METHOD              1\n",
      "             24 LOAD_FAST                0 (self)\n",
      "             26 STORE_ATTR               6 (weight)\n",
      "\n",
      " 24          28 LOAD_FAST                2 (bias)\n",
      "             30 POP_JUMP_IF_FALSE       27 (to 54)\n",
      "             32 LOAD_GLOBAL              2 (nn)\n",
      "             34 LOAD_METHOD              3 (Parameter)\n",
      "             36 LOAD_GLOBAL              4 (torch)\n",
      "             38 LOAD_METHOD              7 (zeros)\n",
      "             40 LOAD_FAST                1 (ndim)\n",
      "             42 CALL_METHOD              1\n",
      "             44 CALL_METHOD              1\n",
      "             46 LOAD_FAST                0 (self)\n",
      "             48 STORE_ATTR               8 (bias)\n",
      "             50 LOAD_CONST               0 (None)\n",
      "             52 RETURN_VALUE\n",
      "        >>   54 LOAD_CONST               0 (None)\n",
      "             56 LOAD_FAST                0 (self)\n",
      "             58 STORE_ATTR               8 (bias)\n",
      "             60 LOAD_CONST               0 (None)\n",
      "             62 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object forward at 0x000002159E1307C0, file \"Code Analysis/gpt2sample.py\", line 26>:\n",
      " 27           0 LOAD_GLOBAL              0 (F)\n",
      "              2 LOAD_METHOD              1 (layer_norm)\n",
      "              4 LOAD_FAST                1 (input)\n",
      "              6 LOAD_FAST                0 (self)\n",
      "              8 LOAD_ATTR                2 (weight)\n",
      "             10 LOAD_ATTR                3 (shape)\n",
      "             12 LOAD_FAST                0 (self)\n",
      "             14 LOAD_ATTR                2 (weight)\n",
      "             16 LOAD_FAST                0 (self)\n",
      "             18 LOAD_ATTR                4 (bias)\n",
      "             20 LOAD_CONST               1 (1e-05)\n",
      "             22 CALL_METHOD              5\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object CausalSelfAttention at 0x000002159E130EA0, file \"Code Analysis/gpt2sample.py\", line 29>:\n",
      " 29           0 LOAD_NAME                0 (__name__)\n",
      "              2 STORE_NAME               1 (__module__)\n",
      "              4 LOAD_CONST               0 ('CausalSelfAttention')\n",
      "              6 STORE_NAME               2 (__qualname__)\n",
      "\n",
      " 31           8 LOAD_CLOSURE             0 (__class__)\n",
      "             10 BUILD_TUPLE              1\n",
      "             12 LOAD_CONST               1 (<code object __init__ at 0x000002159E130D40, file \"Code Analysis/gpt2sample.py\", line 31>)\n",
      "             14 LOAD_CONST               2 ('CausalSelfAttention.__init__')\n",
      "             16 MAKE_FUNCTION            8 (closure)\n",
      "             18 STORE_NAME               3 (__init__)\n",
      "\n",
      " 52          20 LOAD_CONST               3 (<code object forward at 0x000002159E130DF0, file \"Code Analysis/gpt2sample.py\", line 52>)\n",
      "             22 LOAD_CONST               4 ('CausalSelfAttention.forward')\n",
      "             24 MAKE_FUNCTION            0\n",
      "             26 STORE_NAME               4 (forward)\n",
      "             28 LOAD_CLOSURE             0 (__class__)\n",
      "             30 DUP_TOP\n",
      "             32 STORE_NAME               5 (__classcell__)\n",
      "             34 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object __init__ at 0x000002159E130D40, file \"Code Analysis/gpt2sample.py\", line 31>:\n",
      " 32           0 LOAD_GLOBAL              0 (super)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 LOAD_METHOD              1 (__init__)\n",
      "              6 CALL_METHOD              0\n",
      "              8 POP_TOP\n",
      "\n",
      " 33          10 LOAD_FAST                1 (config)\n",
      "             12 LOAD_ATTR                2 (n_embd)\n",
      "             14 LOAD_FAST                1 (config)\n",
      "             16 LOAD_ATTR                3 (n_head)\n",
      "             18 BINARY_MODULO\n",
      "             20 LOAD_CONST               1 (0)\n",
      "             22 COMPARE_OP               2 (==)\n",
      "             24 POP_JUMP_IF_TRUE        15 (to 30)\n",
      "             26 LOAD_ASSERTION_ERROR\n",
      "             28 RAISE_VARARGS            1\n",
      "\n",
      " 35     >>   30 LOAD_GLOBAL              4 (nn)\n",
      "             32 LOAD_ATTR                5 (Linear)\n",
      "             34 LOAD_FAST                1 (config)\n",
      "             36 LOAD_ATTR                2 (n_embd)\n",
      "             38 LOAD_CONST               2 (3)\n",
      "             40 LOAD_FAST                1 (config)\n",
      "             42 LOAD_ATTR                2 (n_embd)\n",
      "             44 BINARY_MULTIPLY\n",
      "             46 LOAD_FAST                1 (config)\n",
      "             48 LOAD_ATTR                6 (bias)\n",
      "             50 LOAD_CONST               3 (('bias',))\n",
      "             52 CALL_FUNCTION_KW         3\n",
      "             54 LOAD_FAST                0 (self)\n",
      "             56 STORE_ATTR               7 (c_attn)\n",
      "\n",
      " 37          58 LOAD_GLOBAL              4 (nn)\n",
      "             60 LOAD_ATTR                5 (Linear)\n",
      "             62 LOAD_FAST                1 (config)\n",
      "             64 LOAD_ATTR                2 (n_embd)\n",
      "             66 LOAD_FAST                1 (config)\n",
      "             68 LOAD_ATTR                2 (n_embd)\n",
      "             70 LOAD_FAST                1 (config)\n",
      "             72 LOAD_ATTR                6 (bias)\n",
      "             74 LOAD_CONST               3 (('bias',))\n",
      "             76 CALL_FUNCTION_KW         3\n",
      "             78 LOAD_FAST                0 (self)\n",
      "             80 STORE_ATTR               8 (c_proj)\n",
      "\n",
      " 39          82 LOAD_GLOBAL              4 (nn)\n",
      "             84 LOAD_METHOD              9 (Dropout)\n",
      "             86 LOAD_FAST                1 (config)\n",
      "             88 LOAD_ATTR               10 (dropout)\n",
      "             90 CALL_METHOD              1\n",
      "             92 LOAD_FAST                0 (self)\n",
      "             94 STORE_ATTR              11 (attn_dropout)\n",
      "\n",
      " 40          96 LOAD_GLOBAL              4 (nn)\n",
      "             98 LOAD_METHOD              9 (Dropout)\n",
      "            100 LOAD_FAST                1 (config)\n",
      "            102 LOAD_ATTR               10 (dropout)\n",
      "            104 CALL_METHOD              1\n",
      "            106 LOAD_FAST                0 (self)\n",
      "            108 STORE_ATTR              12 (resid_dropout)\n",
      "\n",
      " 41         110 LOAD_FAST                1 (config)\n",
      "            112 LOAD_ATTR                3 (n_head)\n",
      "            114 LOAD_FAST                0 (self)\n",
      "            116 STORE_ATTR               3 (n_head)\n",
      "\n",
      " 42         118 LOAD_FAST                1 (config)\n",
      "            120 LOAD_ATTR                2 (n_embd)\n",
      "            122 LOAD_FAST                0 (self)\n",
      "            124 STORE_ATTR               2 (n_embd)\n",
      "\n",
      " 43         126 LOAD_FAST                1 (config)\n",
      "            128 LOAD_ATTR               10 (dropout)\n",
      "            130 LOAD_FAST                0 (self)\n",
      "            132 STORE_ATTR              10 (dropout)\n",
      "\n",
      " 45         134 LOAD_GLOBAL             13 (hasattr)\n",
      "            136 LOAD_GLOBAL             14 (torch)\n",
      "            138 LOAD_ATTR                4 (nn)\n",
      "            140 LOAD_ATTR               15 (functional)\n",
      "            142 LOAD_CONST               4 ('scaled_dot_product_attention')\n",
      "            144 CALL_FUNCTION            2\n",
      "            146 LOAD_FAST                0 (self)\n",
      "            148 STORE_ATTR              16 (flash)\n",
      "\n",
      " 46         150 LOAD_FAST                0 (self)\n",
      "            152 LOAD_ATTR               16 (flash)\n",
      "            154 POP_JUMP_IF_TRUE       107 (to 214)\n",
      "\n",
      " 47         156 LOAD_GLOBAL             17 (print)\n",
      "            158 LOAD_CONST               5 ('WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0')\n",
      "            160 CALL_FUNCTION            1\n",
      "            162 POP_TOP\n",
      "\n",
      " 49         164 LOAD_FAST                0 (self)\n",
      "            166 LOAD_METHOD             18 (register_buffer)\n",
      "            168 LOAD_CONST               6 ('bias')\n",
      "            170 LOAD_GLOBAL             14 (torch)\n",
      "            172 LOAD_METHOD             19 (tril)\n",
      "            174 LOAD_GLOBAL             14 (torch)\n",
      "            176 LOAD_METHOD             20 (ones)\n",
      "            178 LOAD_FAST                1 (config)\n",
      "            180 LOAD_ATTR               21 (block_size)\n",
      "            182 LOAD_FAST                1 (config)\n",
      "            184 LOAD_ATTR               21 (block_size)\n",
      "            186 CALL_METHOD              2\n",
      "            188 CALL_METHOD              1\n",
      "\n",
      " 50         190 LOAD_METHOD             22 (view)\n",
      "            192 LOAD_CONST               7 (1)\n",
      "            194 LOAD_CONST               7 (1)\n",
      "            196 LOAD_FAST                1 (config)\n",
      "            198 LOAD_ATTR               21 (block_size)\n",
      "            200 LOAD_FAST                1 (config)\n",
      "            202 LOAD_ATTR               21 (block_size)\n",
      "            204 CALL_METHOD              4\n",
      "\n",
      " 49         206 CALL_METHOD              2\n",
      "            208 POP_TOP\n",
      "            210 LOAD_CONST               0 (None)\n",
      "            212 RETURN_VALUE\n",
      "\n",
      " 46     >>  214 LOAD_CONST               0 (None)\n",
      "            216 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object forward at 0x000002159E130DF0, file \"Code Analysis/gpt2sample.py\", line 52>:\n",
      " 53           0 LOAD_FAST                1 (x)\n",
      "              2 LOAD_METHOD              0 (size)\n",
      "              4 CALL_METHOD              0\n",
      "              6 UNPACK_SEQUENCE          3\n",
      "              8 STORE_FAST               2 (B)\n",
      "             10 STORE_FAST               3 (T)\n",
      "             12 STORE_FAST               4 (C)\n",
      "\n",
      " 56          14 LOAD_FAST                0 (self)\n",
      "             16 LOAD_METHOD              1 (c_attn)\n",
      "             18 LOAD_FAST                1 (x)\n",
      "             20 CALL_METHOD              1\n",
      "             22 LOAD_ATTR                2 (split)\n",
      "             24 LOAD_FAST                0 (self)\n",
      "             26 LOAD_ATTR                3 (n_embd)\n",
      "             28 LOAD_CONST               1 (2)\n",
      "             30 LOAD_CONST               2 (('dim',))\n",
      "             32 CALL_FUNCTION_KW         2\n",
      "             34 UNPACK_SEQUENCE          3\n",
      "             36 STORE_FAST               5 (q)\n",
      "             38 STORE_FAST               6 (k)\n",
      "             40 STORE_FAST               7 (v)\n",
      "\n",
      " 57          42 LOAD_FAST                6 (k)\n",
      "             44 LOAD_METHOD              4 (view)\n",
      "             46 LOAD_FAST                2 (B)\n",
      "             48 LOAD_FAST                3 (T)\n",
      "             50 LOAD_FAST                0 (self)\n",
      "             52 LOAD_ATTR                5 (n_head)\n",
      "             54 LOAD_FAST                4 (C)\n",
      "             56 LOAD_FAST                0 (self)\n",
      "             58 LOAD_ATTR                5 (n_head)\n",
      "             60 BINARY_FLOOR_DIVIDE\n",
      "             62 CALL_METHOD              4\n",
      "             64 LOAD_METHOD              6 (transpose)\n",
      "             66 LOAD_CONST               3 (1)\n",
      "             68 LOAD_CONST               1 (2)\n",
      "             70 CALL_METHOD              2\n",
      "             72 STORE_FAST               6 (k)\n",
      "\n",
      " 58          74 LOAD_FAST                5 (q)\n",
      "             76 LOAD_METHOD              4 (view)\n",
      "             78 LOAD_FAST                2 (B)\n",
      "             80 LOAD_FAST                3 (T)\n",
      "             82 LOAD_FAST                0 (self)\n",
      "             84 LOAD_ATTR                5 (n_head)\n",
      "             86 LOAD_FAST                4 (C)\n",
      "             88 LOAD_FAST                0 (self)\n",
      "             90 LOAD_ATTR                5 (n_head)\n",
      "             92 BINARY_FLOOR_DIVIDE\n",
      "             94 CALL_METHOD              4\n",
      "             96 LOAD_METHOD              6 (transpose)\n",
      "             98 LOAD_CONST               3 (1)\n",
      "            100 LOAD_CONST               1 (2)\n",
      "            102 CALL_METHOD              2\n",
      "            104 STORE_FAST               5 (q)\n",
      "\n",
      " 59         106 LOAD_FAST                7 (v)\n",
      "            108 LOAD_METHOD              4 (view)\n",
      "            110 LOAD_FAST                2 (B)\n",
      "            112 LOAD_FAST                3 (T)\n",
      "            114 LOAD_FAST                0 (self)\n",
      "            116 LOAD_ATTR                5 (n_head)\n",
      "            118 LOAD_FAST                4 (C)\n",
      "            120 LOAD_FAST                0 (self)\n",
      "            122 LOAD_ATTR                5 (n_head)\n",
      "            124 BINARY_FLOOR_DIVIDE\n",
      "            126 CALL_METHOD              4\n",
      "            128 LOAD_METHOD              6 (transpose)\n",
      "            130 LOAD_CONST               3 (1)\n",
      "            132 LOAD_CONST               1 (2)\n",
      "            134 CALL_METHOD              2\n",
      "            136 STORE_FAST               7 (v)\n",
      "\n",
      " 62         138 LOAD_FAST                0 (self)\n",
      "            140 LOAD_ATTR                7 (flash)\n",
      "            142 POP_JUMP_IF_FALSE       92 (to 184)\n",
      "\n",
      " 64         144 LOAD_GLOBAL              8 (torch)\n",
      "            146 LOAD_ATTR                9 (nn)\n",
      "            148 LOAD_ATTR               10 (functional)\n",
      "            150 LOAD_ATTR               11 (scaled_dot_product_attention)\n",
      "            152 LOAD_FAST                5 (q)\n",
      "            154 LOAD_FAST                6 (k)\n",
      "            156 LOAD_FAST                7 (v)\n",
      "            158 LOAD_CONST               0 (None)\n",
      "            160 LOAD_FAST                0 (self)\n",
      "            162 LOAD_ATTR               12 (training)\n",
      "            164 POP_JUMP_IF_FALSE       86 (to 172)\n",
      "            166 LOAD_FAST                0 (self)\n",
      "            168 LOAD_ATTR               13 (dropout)\n",
      "            170 JUMP_FORWARD             1 (to 174)\n",
      "        >>  172 LOAD_CONST               4 (0)\n",
      "        >>  174 LOAD_CONST               5 (True)\n",
      "            176 LOAD_CONST               6 (('attn_mask', 'dropout_p', 'is_causal'))\n",
      "            178 CALL_FUNCTION_KW         6\n",
      "            180 STORE_FAST               8 (y)\n",
      "            182 JUMP_FORWARD            59 (to 302)\n",
      "\n",
      " 67     >>  184 LOAD_FAST                5 (q)\n",
      "            186 LOAD_FAST                6 (k)\n",
      "            188 LOAD_METHOD              6 (transpose)\n",
      "            190 LOAD_CONST               7 (-2)\n",
      "            192 LOAD_CONST               8 (-1)\n",
      "            194 CALL_METHOD              2\n",
      "            196 BINARY_MATRIX_MULTIPLY\n",
      "            198 LOAD_CONST               9 (1.0)\n",
      "            200 LOAD_GLOBAL             14 (math)\n",
      "            202 LOAD_METHOD             15 (sqrt)\n",
      "            204 LOAD_FAST                6 (k)\n",
      "            206 LOAD_METHOD              0 (size)\n",
      "            208 LOAD_CONST               8 (-1)\n",
      "            210 CALL_METHOD              1\n",
      "            212 CALL_METHOD              1\n",
      "            214 BINARY_TRUE_DIVIDE\n",
      "            216 BINARY_MULTIPLY\n",
      "            218 STORE_FAST               9 (att)\n",
      "\n",
      " 68         220 LOAD_FAST                9 (att)\n",
      "            222 LOAD_METHOD             16 (masked_fill)\n",
      "            224 LOAD_FAST                0 (self)\n",
      "            226 LOAD_ATTR               17 (bias)\n",
      "            228 LOAD_CONST               0 (None)\n",
      "            230 LOAD_CONST               0 (None)\n",
      "            232 BUILD_SLICE              2\n",
      "            234 LOAD_CONST               0 (None)\n",
      "            236 LOAD_CONST               0 (None)\n",
      "            238 BUILD_SLICE              2\n",
      "            240 LOAD_CONST               0 (None)\n",
      "            242 LOAD_FAST                3 (T)\n",
      "            244 BUILD_SLICE              2\n",
      "            246 LOAD_CONST               0 (None)\n",
      "            248 LOAD_FAST                3 (T)\n",
      "            250 BUILD_SLICE              2\n",
      "            252 BUILD_TUPLE              4\n",
      "            254 BINARY_SUBSCR\n",
      "            256 LOAD_CONST               4 (0)\n",
      "            258 COMPARE_OP               2 (==)\n",
      "            260 LOAD_GLOBAL             18 (float)\n",
      "            262 LOAD_CONST              10 ('-inf')\n",
      "            264 CALL_FUNCTION            1\n",
      "            266 CALL_METHOD              2\n",
      "            268 STORE_FAST               9 (att)\n",
      "\n",
      " 69         270 LOAD_GLOBAL             19 (F)\n",
      "            272 LOAD_ATTR               20 (softmax)\n",
      "            274 LOAD_FAST                9 (att)\n",
      "            276 LOAD_CONST               8 (-1)\n",
      "            278 LOAD_CONST               2 (('dim',))\n",
      "            280 CALL_FUNCTION_KW         2\n",
      "            282 STORE_FAST               9 (att)\n",
      "\n",
      " 70         284 LOAD_FAST                0 (self)\n",
      "            286 LOAD_METHOD             21 (attn_dropout)\n",
      "            288 LOAD_FAST                9 (att)\n",
      "            290 CALL_METHOD              1\n",
      "            292 STORE_FAST               9 (att)\n",
      "\n",
      " 71         294 LOAD_FAST                9 (att)\n",
      "            296 LOAD_FAST                7 (v)\n",
      "            298 BINARY_MATRIX_MULTIPLY\n",
      "            300 STORE_FAST               8 (y)\n",
      "\n",
      " 72     >>  302 LOAD_FAST                8 (y)\n",
      "            304 LOAD_METHOD              6 (transpose)\n",
      "            306 LOAD_CONST               3 (1)\n",
      "            308 LOAD_CONST               1 (2)\n",
      "            310 CALL_METHOD              2\n",
      "            312 LOAD_METHOD             22 (contiguous)\n",
      "            314 CALL_METHOD              0\n",
      "            316 LOAD_METHOD              4 (view)\n",
      "            318 LOAD_FAST                2 (B)\n",
      "            320 LOAD_FAST                3 (T)\n",
      "            322 LOAD_FAST                4 (C)\n",
      "            324 CALL_METHOD              3\n",
      "            326 STORE_FAST               8 (y)\n",
      "\n",
      " 75         328 LOAD_FAST                0 (self)\n",
      "            330 LOAD_METHOD             23 (resid_dropout)\n",
      "            332 LOAD_FAST                0 (self)\n",
      "            334 LOAD_METHOD             24 (c_proj)\n",
      "            336 LOAD_FAST                8 (y)\n",
      "            338 CALL_METHOD              1\n",
      "            340 CALL_METHOD              1\n",
      "            342 STORE_FAST               8 (y)\n",
      "\n",
      " 76         344 LOAD_FAST                8 (y)\n",
      "            346 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object MLP at 0x000002159E1310B0, file \"Code Analysis/gpt2sample.py\", line 78>:\n",
      " 78           0 LOAD_NAME                0 (__name__)\n",
      "              2 STORE_NAME               1 (__module__)\n",
      "              4 LOAD_CONST               0 ('MLP')\n",
      "              6 STORE_NAME               2 (__qualname__)\n",
      "\n",
      " 80           8 LOAD_CLOSURE             0 (__class__)\n",
      "             10 BUILD_TUPLE              1\n",
      "             12 LOAD_CONST               1 (<code object __init__ at 0x000002159E130F50, file \"Code Analysis/gpt2sample.py\", line 80>)\n",
      "             14 LOAD_CONST               2 ('MLP.__init__')\n",
      "             16 MAKE_FUNCTION            8 (closure)\n",
      "             18 STORE_NAME               3 (__init__)\n",
      "\n",
      " 87          20 LOAD_CONST               3 (<code object forward at 0x000002159E131000, file \"Code Analysis/gpt2sample.py\", line 87>)\n",
      "             22 LOAD_CONST               4 ('MLP.forward')\n",
      "             24 MAKE_FUNCTION            0\n",
      "             26 STORE_NAME               4 (forward)\n",
      "             28 LOAD_CLOSURE             0 (__class__)\n",
      "             30 DUP_TOP\n",
      "             32 STORE_NAME               5 (__classcell__)\n",
      "             34 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object __init__ at 0x000002159E130F50, file \"Code Analysis/gpt2sample.py\", line 80>:\n",
      " 81           0 LOAD_GLOBAL              0 (super)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 LOAD_METHOD              1 (__init__)\n",
      "              6 CALL_METHOD              0\n",
      "              8 POP_TOP\n",
      "\n",
      " 82          10 LOAD_GLOBAL              2 (nn)\n",
      "             12 LOAD_ATTR                3 (Linear)\n",
      "             14 LOAD_FAST                1 (config)\n",
      "             16 LOAD_ATTR                4 (n_embd)\n",
      "             18 LOAD_CONST               1 (4)\n",
      "             20 LOAD_FAST                1 (config)\n",
      "             22 LOAD_ATTR                4 (n_embd)\n",
      "             24 BINARY_MULTIPLY\n",
      "             26 LOAD_FAST                1 (config)\n",
      "             28 LOAD_ATTR                5 (bias)\n",
      "             30 LOAD_CONST               2 (('bias',))\n",
      "             32 CALL_FUNCTION_KW         3\n",
      "             34 LOAD_FAST                0 (self)\n",
      "             36 STORE_ATTR               6 (c_fc)\n",
      "\n",
      " 83          38 LOAD_GLOBAL              2 (nn)\n",
      "             40 LOAD_METHOD              7 (GELU)\n",
      "             42 CALL_METHOD              0\n",
      "             44 LOAD_FAST                0 (self)\n",
      "             46 STORE_ATTR               8 (gelu)\n",
      "\n",
      " 84          48 LOAD_GLOBAL              2 (nn)\n",
      "             50 LOAD_ATTR                3 (Linear)\n",
      "             52 LOAD_CONST               1 (4)\n",
      "             54 LOAD_FAST                1 (config)\n",
      "             56 LOAD_ATTR                4 (n_embd)\n",
      "             58 BINARY_MULTIPLY\n",
      "             60 LOAD_FAST                1 (config)\n",
      "             62 LOAD_ATTR                4 (n_embd)\n",
      "             64 LOAD_FAST                1 (config)\n",
      "             66 LOAD_ATTR                5 (bias)\n",
      "             68 LOAD_CONST               2 (('bias',))\n",
      "             70 CALL_FUNCTION_KW         3\n",
      "             72 LOAD_FAST                0 (self)\n",
      "             74 STORE_ATTR               9 (c_proj)\n",
      "\n",
      " 85          76 LOAD_GLOBAL              2 (nn)\n",
      "             78 LOAD_METHOD             10 (Dropout)\n",
      "             80 LOAD_FAST                1 (config)\n",
      "             82 LOAD_ATTR               11 (dropout)\n",
      "             84 CALL_METHOD              1\n",
      "             86 LOAD_FAST                0 (self)\n",
      "             88 STORE_ATTR              11 (dropout)\n",
      "             90 LOAD_CONST               0 (None)\n",
      "             92 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object forward at 0x000002159E131000, file \"Code Analysis/gpt2sample.py\", line 87>:\n",
      " 88           0 LOAD_FAST                0 (self)\n",
      "              2 LOAD_METHOD              0 (c_fc)\n",
      "              4 LOAD_FAST                1 (x)\n",
      "              6 CALL_METHOD              1\n",
      "              8 STORE_FAST               1 (x)\n",
      "\n",
      " 89          10 LOAD_FAST                0 (self)\n",
      "             12 LOAD_METHOD              1 (gelu)\n",
      "             14 LOAD_FAST                1 (x)\n",
      "             16 CALL_METHOD              1\n",
      "             18 STORE_FAST               1 (x)\n",
      "\n",
      " 90          20 LOAD_FAST                0 (self)\n",
      "             22 LOAD_METHOD              2 (c_proj)\n",
      "             24 LOAD_FAST                1 (x)\n",
      "             26 CALL_METHOD              1\n",
      "             28 STORE_FAST               1 (x)\n",
      "\n",
      " 91          30 LOAD_FAST                0 (self)\n",
      "             32 LOAD_METHOD              3 (dropout)\n",
      "             34 LOAD_FAST                1 (x)\n",
      "             36 CALL_METHOD              1\n",
      "             38 STORE_FAST               1 (x)\n",
      "\n",
      " 92          40 LOAD_FAST                1 (x)\n",
      "             42 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object Block at 0x000002159E1312C0, file \"Code Analysis/gpt2sample.py\", line 94>:\n",
      " 94           0 LOAD_NAME                0 (__name__)\n",
      "              2 STORE_NAME               1 (__module__)\n",
      "              4 LOAD_CONST               0 ('Block')\n",
      "              6 STORE_NAME               2 (__qualname__)\n",
      "\n",
      " 96           8 LOAD_CLOSURE             0 (__class__)\n",
      "             10 BUILD_TUPLE              1\n",
      "             12 LOAD_CONST               1 (<code object __init__ at 0x000002159E131160, file \"Code Analysis/gpt2sample.py\", line 96>)\n",
      "             14 LOAD_CONST               2 ('Block.__init__')\n",
      "             16 MAKE_FUNCTION            8 (closure)\n",
      "             18 STORE_NAME               3 (__init__)\n",
      "\n",
      "103          20 LOAD_CONST               3 (<code object forward at 0x000002159E131210, file \"Code Analysis/gpt2sample.py\", line 103>)\n",
      "             22 LOAD_CONST               4 ('Block.forward')\n",
      "             24 MAKE_FUNCTION            0\n",
      "             26 STORE_NAME               4 (forward)\n",
      "             28 LOAD_CLOSURE             0 (__class__)\n",
      "             30 DUP_TOP\n",
      "             32 STORE_NAME               5 (__classcell__)\n",
      "             34 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object __init__ at 0x000002159E131160, file \"Code Analysis/gpt2sample.py\", line 96>:\n",
      " 97           0 LOAD_GLOBAL              0 (super)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 LOAD_METHOD              1 (__init__)\n",
      "              6 CALL_METHOD              0\n",
      "              8 POP_TOP\n",
      "\n",
      " 98          10 LOAD_GLOBAL              2 (LayerNorm)\n",
      "             12 LOAD_FAST                1 (config)\n",
      "             14 LOAD_ATTR                3 (n_embd)\n",
      "             16 LOAD_FAST                1 (config)\n",
      "             18 LOAD_ATTR                4 (bias)\n",
      "             20 LOAD_CONST               1 (('bias',))\n",
      "             22 CALL_FUNCTION_KW         2\n",
      "             24 LOAD_FAST                0 (self)\n",
      "             26 STORE_ATTR               5 (ln_1)\n",
      "\n",
      " 99          28 LOAD_GLOBAL              6 (CausalSelfAttention)\n",
      "             30 LOAD_FAST                1 (config)\n",
      "             32 CALL_FUNCTION            1\n",
      "             34 LOAD_FAST                0 (self)\n",
      "             36 STORE_ATTR               7 (attn)\n",
      "\n",
      "100          38 LOAD_GLOBAL              2 (LayerNorm)\n",
      "             40 LOAD_FAST                1 (config)\n",
      "             42 LOAD_ATTR                3 (n_embd)\n",
      "             44 LOAD_FAST                1 (config)\n",
      "             46 LOAD_ATTR                4 (bias)\n",
      "             48 LOAD_CONST               1 (('bias',))\n",
      "             50 CALL_FUNCTION_KW         2\n",
      "             52 LOAD_FAST                0 (self)\n",
      "             54 STORE_ATTR               8 (ln_2)\n",
      "\n",
      "101          56 LOAD_GLOBAL              9 (MLP)\n",
      "             58 LOAD_FAST                1 (config)\n",
      "             60 CALL_FUNCTION            1\n",
      "             62 LOAD_FAST                0 (self)\n",
      "             64 STORE_ATTR              10 (mlp)\n",
      "             66 LOAD_CONST               0 (None)\n",
      "             68 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object forward at 0x000002159E131210, file \"Code Analysis/gpt2sample.py\", line 103>:\n",
      "104           0 LOAD_FAST                1 (x)\n",
      "              2 LOAD_FAST                0 (self)\n",
      "              4 LOAD_METHOD              0 (attn)\n",
      "              6 LOAD_FAST                0 (self)\n",
      "              8 LOAD_METHOD              1 (ln_1)\n",
      "             10 LOAD_FAST                1 (x)\n",
      "             12 CALL_METHOD              1\n",
      "             14 CALL_METHOD              1\n",
      "             16 BINARY_ADD\n",
      "             18 STORE_FAST               1 (x)\n",
      "\n",
      "105          20 LOAD_FAST                1 (x)\n",
      "             22 LOAD_FAST                0 (self)\n",
      "             24 LOAD_METHOD              2 (mlp)\n",
      "             26 LOAD_FAST                0 (self)\n",
      "             28 LOAD_METHOD              3 (ln_2)\n",
      "             30 LOAD_FAST                1 (x)\n",
      "             32 CALL_METHOD              1\n",
      "             34 CALL_METHOD              1\n",
      "             36 BINARY_ADD\n",
      "             38 STORE_FAST               1 (x)\n",
      "\n",
      "106          40 LOAD_FAST                1 (x)\n",
      "             42 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object GPTConfig at 0x000002159E131370, file \"Code Analysis/gpt2sample.py\", line 108>:\n",
      "108           0 LOAD_NAME                0 (__name__)\n",
      "              2 STORE_NAME               1 (__module__)\n",
      "              4 LOAD_CONST               0 ('GPTConfig')\n",
      "              6 STORE_NAME               2 (__qualname__)\n",
      "              8 SETUP_ANNOTATIONS\n",
      "\n",
      "110          10 LOAD_CONST               1 (1024)\n",
      "             12 STORE_NAME               3 (block_size)\n",
      "             14 LOAD_NAME                4 (int)\n",
      "             16 LOAD_NAME                5 (__annotations__)\n",
      "             18 LOAD_CONST               2 ('block_size')\n",
      "             20 STORE_SUBSCR\n",
      "\n",
      "111          22 LOAD_CONST               3 (50304)\n",
      "             24 STORE_NAME               6 (vocab_size)\n",
      "             26 LOAD_NAME                4 (int)\n",
      "             28 LOAD_NAME                5 (__annotations__)\n",
      "             30 LOAD_CONST               4 ('vocab_size')\n",
      "             32 STORE_SUBSCR\n",
      "\n",
      "112          34 LOAD_CONST               5 (12)\n",
      "             36 STORE_NAME               7 (n_layer)\n",
      "             38 LOAD_NAME                4 (int)\n",
      "             40 LOAD_NAME                5 (__annotations__)\n",
      "             42 LOAD_CONST               6 ('n_layer')\n",
      "             44 STORE_SUBSCR\n",
      "\n",
      "113          46 LOAD_CONST               5 (12)\n",
      "             48 STORE_NAME               8 (n_head)\n",
      "             50 LOAD_NAME                4 (int)\n",
      "             52 LOAD_NAME                5 (__annotations__)\n",
      "             54 LOAD_CONST               7 ('n_head')\n",
      "             56 STORE_SUBSCR\n",
      "\n",
      "114          58 LOAD_CONST               8 (768)\n",
      "             60 STORE_NAME               9 (n_embd)\n",
      "             62 LOAD_NAME                4 (int)\n",
      "             64 LOAD_NAME                5 (__annotations__)\n",
      "             66 LOAD_CONST               9 ('n_embd')\n",
      "             68 STORE_SUBSCR\n",
      "\n",
      "115          70 LOAD_CONST              10 (0.0)\n",
      "             72 STORE_NAME              10 (dropout)\n",
      "             74 LOAD_NAME               11 (float)\n",
      "             76 LOAD_NAME                5 (__annotations__)\n",
      "             78 LOAD_CONST              11 ('dropout')\n",
      "             80 STORE_SUBSCR\n",
      "\n",
      "116          82 LOAD_CONST              12 (True)\n",
      "             84 STORE_NAME              12 (bias)\n",
      "             86 LOAD_NAME               13 (bool)\n",
      "             88 LOAD_NAME                5 (__annotations__)\n",
      "             90 LOAD_CONST              13 ('bias')\n",
      "             92 STORE_SUBSCR\n",
      "             94 LOAD_CONST              14 (None)\n",
      "             96 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object GPT at 0x000002159E132340, file \"Code Analysis/gpt2sample.py\", line 118>:\n",
      "118           0 LOAD_NAME                0 (__name__)\n",
      "              2 STORE_NAME               1 (__module__)\n",
      "              4 LOAD_CONST               0 ('GPT')\n",
      "              6 STORE_NAME               2 (__qualname__)\n",
      "\n",
      "120           8 LOAD_CLOSURE             0 (__class__)\n",
      "             10 BUILD_TUPLE              1\n",
      "             12 LOAD_CONST               1 (<code object __init__ at 0x000002159E1314D0, file \"Code Analysis/gpt2sample.py\", line 120>)\n",
      "             14 LOAD_CONST               2 ('GPT.__init__')\n",
      "             16 MAKE_FUNCTION            8 (closure)\n",
      "             18 STORE_NAME               3 (__init__)\n",
      "\n",
      "150          20 LOAD_CONST              22 ((True,))\n",
      "             22 LOAD_CONST               4 (<code object get_num_params at 0x000002159E131630, file \"Code Analysis/gpt2sample.py\", line 150>)\n",
      "             24 LOAD_CONST               5 ('GPT.get_num_params')\n",
      "             26 MAKE_FUNCTION            1 (defaults)\n",
      "             28 STORE_NAME               4 (get_num_params)\n",
      "\n",
      "162          30 LOAD_CONST               6 (<code object _init_weights at 0x000002159E1316E0, file \"Code Analysis/gpt2sample.py\", line 162>)\n",
      "             32 LOAD_CONST               7 ('GPT._init_weights')\n",
      "             34 MAKE_FUNCTION            0\n",
      "             36 STORE_NAME               5 (_init_weights)\n",
      "\n",
      "170          38 LOAD_CONST              23 ((None,))\n",
      "             40 LOAD_CONST               9 (<code object forward at 0x000002159E131790, file \"Code Analysis/gpt2sample.py\", line 170>)\n",
      "             42 LOAD_CONST              10 ('GPT.forward')\n",
      "             44 MAKE_FUNCTION            1 (defaults)\n",
      "             46 STORE_NAME               6 (forward)\n",
      "\n",
      "195          48 LOAD_CONST              11 (<code object crop_block_size at 0x000002159E131840, file \"Code Analysis/gpt2sample.py\", line 195>)\n",
      "             50 LOAD_CONST              12 ('GPT.crop_block_size')\n",
      "             52 MAKE_FUNCTION            0\n",
      "             54 STORE_NAME               7 (crop_block_size)\n",
      "\n",
      "206          56 LOAD_NAME                8 (classmethod)\n",
      "\n",
      "207          58 LOAD_CONST              23 ((None,))\n",
      "             60 LOAD_CONST              13 (<code object from_pretrained at 0x000002159E131C60, file \"Code Analysis/gpt2sample.py\", line 206>)\n",
      "             62 LOAD_CONST              14 ('GPT.from_pretrained')\n",
      "             64 MAKE_FUNCTION            1 (defaults)\n",
      "             66 CALL_FUNCTION            1\n",
      "             68 STORE_NAME               9 (from_pretrained)\n",
      "\n",
      "263          70 LOAD_CONST              15 (<code object configure_optimizers at 0x000002159E132130, file \"Code Analysis/gpt2sample.py\", line 263>)\n",
      "             72 LOAD_CONST              16 ('GPT.configure_optimizers')\n",
      "             74 MAKE_FUNCTION            0\n",
      "             76 STORE_NAME              10 (configure_optimizers)\n",
      "\n",
      "289          78 LOAD_CONST              17 (<code object estimate_mfu at 0x000002159E1321E0, file \"Code Analysis/gpt2sample.py\", line 289>)\n",
      "             80 LOAD_CONST              18 ('GPT.estimate_mfu')\n",
      "             82 MAKE_FUNCTION            0\n",
      "             84 STORE_NAME              11 (estimate_mfu)\n",
      "\n",
      "305          86 LOAD_NAME               12 (torch)\n",
      "             88 LOAD_METHOD             13 (no_grad)\n",
      "             90 CALL_METHOD              0\n",
      "\n",
      "306          92 LOAD_CONST              24 ((1.0, None))\n",
      "             94 LOAD_CONST              20 (<code object generate at 0x000002159E132290, file \"Code Analysis/gpt2sample.py\", line 305>)\n",
      "             96 LOAD_CONST              21 ('GPT.generate')\n",
      "             98 MAKE_FUNCTION            1 (defaults)\n",
      "            100 CALL_FUNCTION            1\n",
      "            102 STORE_NAME              14 (generate)\n",
      "            104 LOAD_CLOSURE             0 (__class__)\n",
      "            106 DUP_TOP\n",
      "            108 STORE_NAME              15 (__classcell__)\n",
      "            110 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object __init__ at 0x000002159E1314D0, file \"Code Analysis/gpt2sample.py\", line 120>:\n",
      "121           0 LOAD_GLOBAL              0 (super)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 LOAD_METHOD              1 (__init__)\n",
      "              6 CALL_METHOD              0\n",
      "              8 POP_TOP\n",
      "\n",
      "122          10 LOAD_DEREF               0 (config)\n",
      "             12 LOAD_ATTR                2 (vocab_size)\n",
      "             14 LOAD_CONST               0 (None)\n",
      "             16 IS_OP                    1\n",
      "             18 POP_JUMP_IF_TRUE        12 (to 24)\n",
      "             20 LOAD_ASSERTION_ERROR\n",
      "             22 RAISE_VARARGS            1\n",
      "\n",
      "123     >>   24 LOAD_DEREF               0 (config)\n",
      "             26 LOAD_ATTR                3 (block_size)\n",
      "             28 LOAD_CONST               0 (None)\n",
      "             30 IS_OP                    1\n",
      "             32 POP_JUMP_IF_TRUE        19 (to 38)\n",
      "             34 LOAD_ASSERTION_ERROR\n",
      "             36 RAISE_VARARGS            1\n",
      "\n",
      "124     >>   38 LOAD_DEREF               0 (config)\n",
      "             40 LOAD_FAST                0 (self)\n",
      "             42 STORE_ATTR               4 (config)\n",
      "\n",
      "126          44 LOAD_GLOBAL              5 (nn)\n",
      "             46 LOAD_METHOD              6 (ModuleDict)\n",
      "             48 LOAD_GLOBAL              7 (dict)\n",
      "\n",
      "127          50 LOAD_GLOBAL              5 (nn)\n",
      "             52 LOAD_METHOD              8 (Embedding)\n",
      "             54 LOAD_DEREF               0 (config)\n",
      "             56 LOAD_ATTR                2 (vocab_size)\n",
      "             58 LOAD_DEREF               0 (config)\n",
      "             60 LOAD_ATTR                9 (n_embd)\n",
      "             62 CALL_METHOD              2\n",
      "\n",
      "128          64 LOAD_GLOBAL              5 (nn)\n",
      "             66 LOAD_METHOD              8 (Embedding)\n",
      "             68 LOAD_DEREF               0 (config)\n",
      "             70 LOAD_ATTR                3 (block_size)\n",
      "             72 LOAD_DEREF               0 (config)\n",
      "             74 LOAD_ATTR                9 (n_embd)\n",
      "             76 CALL_METHOD              2\n",
      "\n",
      "129          78 LOAD_GLOBAL              5 (nn)\n",
      "             80 LOAD_METHOD             10 (Dropout)\n",
      "             82 LOAD_DEREF               0 (config)\n",
      "             84 LOAD_ATTR               11 (dropout)\n",
      "             86 CALL_METHOD              1\n",
      "\n",
      "130          88 LOAD_GLOBAL              5 (nn)\n",
      "             90 LOAD_METHOD             12 (ModuleList)\n",
      "             92 LOAD_CLOSURE             0 (config)\n",
      "             94 BUILD_TUPLE              1\n",
      "             96 LOAD_CONST               1 (<code object <listcomp> at 0x000002159E131420, file \"Code Analysis/gpt2sample.py\", line 130>)\n",
      "             98 LOAD_CONST               2 ('GPT.__init__.<locals>.<listcomp>')\n",
      "            100 MAKE_FUNCTION            8 (closure)\n",
      "            102 LOAD_GLOBAL             13 (range)\n",
      "            104 LOAD_DEREF               0 (config)\n",
      "            106 LOAD_ATTR               14 (n_layer)\n",
      "            108 CALL_FUNCTION            1\n",
      "            110 GET_ITER\n",
      "            112 CALL_FUNCTION            1\n",
      "            114 CALL_METHOD              1\n",
      "\n",
      "131         116 LOAD_GLOBAL             15 (LayerNorm)\n",
      "            118 LOAD_DEREF               0 (config)\n",
      "            120 LOAD_ATTR                9 (n_embd)\n",
      "            122 LOAD_DEREF               0 (config)\n",
      "            124 LOAD_ATTR               16 (bias)\n",
      "            126 LOAD_CONST               3 (('bias',))\n",
      "            128 CALL_FUNCTION_KW         2\n",
      "\n",
      "126         130 LOAD_CONST               4 (('wte', 'wpe', 'drop', 'h', 'ln_f'))\n",
      "            132 CALL_FUNCTION_KW         5\n",
      "            134 CALL_METHOD              1\n",
      "            136 LOAD_FAST                0 (self)\n",
      "            138 STORE_ATTR              17 (transformer)\n",
      "\n",
      "133         140 LOAD_GLOBAL              5 (nn)\n",
      "            142 LOAD_ATTR               18 (Linear)\n",
      "            144 LOAD_DEREF               0 (config)\n",
      "            146 LOAD_ATTR                9 (n_embd)\n",
      "            148 LOAD_DEREF               0 (config)\n",
      "            150 LOAD_ATTR                2 (vocab_size)\n",
      "            152 LOAD_CONST               5 (False)\n",
      "            154 LOAD_CONST               3 (('bias',))\n",
      "            156 CALL_FUNCTION_KW         3\n",
      "            158 LOAD_FAST                0 (self)\n",
      "            160 STORE_ATTR              19 (lm_head)\n",
      "\n",
      "138         162 LOAD_FAST                0 (self)\n",
      "            164 LOAD_ATTR               19 (lm_head)\n",
      "            166 LOAD_ATTR               20 (weight)\n",
      "            168 LOAD_FAST                0 (self)\n",
      "            170 LOAD_ATTR               17 (transformer)\n",
      "            172 LOAD_ATTR               21 (wte)\n",
      "            174 STORE_ATTR              20 (weight)\n",
      "\n",
      "141         176 LOAD_FAST                0 (self)\n",
      "            178 LOAD_METHOD             22 (apply)\n",
      "            180 LOAD_FAST                0 (self)\n",
      "            182 LOAD_ATTR               23 (_init_weights)\n",
      "            184 CALL_METHOD              1\n",
      "            186 POP_TOP\n",
      "\n",
      "143         188 LOAD_FAST                0 (self)\n",
      "            190 LOAD_METHOD             24 (named_parameters)\n",
      "            192 CALL_METHOD              0\n",
      "            194 GET_ITER\n",
      "        >>  196 FOR_ITER                27 (to 252)\n",
      "            198 UNPACK_SEQUENCE          2\n",
      "            200 STORE_FAST               2 (pn)\n",
      "            202 STORE_FAST               3 (p)\n",
      "\n",
      "144         204 LOAD_FAST                2 (pn)\n",
      "            206 LOAD_METHOD             25 (endswith)\n",
      "            208 LOAD_CONST               6 ('c_proj.weight')\n",
      "            210 CALL_METHOD              1\n",
      "            212 POP_JUMP_IF_FALSE      125 (to 250)\n",
      "\n",
      "145         214 LOAD_GLOBAL             26 (torch)\n",
      "            216 LOAD_ATTR                5 (nn)\n",
      "            218 LOAD_ATTR               27 (init)\n",
      "            220 LOAD_ATTR               28 (normal_)\n",
      "            222 LOAD_FAST                3 (p)\n",
      "            224 LOAD_CONST               7 (0.0)\n",
      "            226 LOAD_CONST               8 (0.02)\n",
      "            228 LOAD_GLOBAL             29 (math)\n",
      "            230 LOAD_METHOD             30 (sqrt)\n",
      "            232 LOAD_CONST               9 (2)\n",
      "            234 LOAD_DEREF               0 (config)\n",
      "            236 LOAD_ATTR               14 (n_layer)\n",
      "            238 BINARY_MULTIPLY\n",
      "            240 CALL_METHOD              1\n",
      "            242 BINARY_TRUE_DIVIDE\n",
      "            244 LOAD_CONST              10 (('mean', 'std'))\n",
      "            246 CALL_FUNCTION_KW         3\n",
      "            248 POP_TOP\n",
      "        >>  250 JUMP_ABSOLUTE           98 (to 196)\n",
      "\n",
      "148     >>  252 LOAD_GLOBAL             31 (print)\n",
      "            254 LOAD_CONST              11 ('number of parameters: %.2fM')\n",
      "            256 LOAD_FAST                0 (self)\n",
      "            258 LOAD_METHOD             32 (get_num_params)\n",
      "            260 CALL_METHOD              0\n",
      "            262 LOAD_CONST              12 (1000000.0)\n",
      "            264 BINARY_TRUE_DIVIDE\n",
      "            266 BUILD_TUPLE              1\n",
      "            268 BINARY_MODULO\n",
      "            270 CALL_FUNCTION            1\n",
      "            272 POP_TOP\n",
      "            274 LOAD_CONST               0 (None)\n",
      "            276 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E131420, file \"Code Analysis/gpt2sample.py\", line 130>:\n",
      "130           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 6 (to 18)\n",
      "              6 STORE_FAST               1 (_)\n",
      "              8 LOAD_GLOBAL              0 (Block)\n",
      "             10 LOAD_DEREF               0 (config)\n",
      "             12 CALL_FUNCTION            1\n",
      "             14 LIST_APPEND              2\n",
      "             16 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   18 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object get_num_params at 0x000002159E131630, file \"Code Analysis/gpt2sample.py\", line 150>:\n",
      "157           0 LOAD_GLOBAL              0 (sum)\n",
      "              2 LOAD_CONST               1 (<code object <genexpr> at 0x000002159E131580, file \"Code Analysis/gpt2sample.py\", line 157>)\n",
      "              4 LOAD_CONST               2 ('GPT.get_num_params.<locals>.<genexpr>')\n",
      "              6 MAKE_FUNCTION            0\n",
      "              8 LOAD_FAST                0 (self)\n",
      "             10 LOAD_METHOD              1 (parameters)\n",
      "             12 CALL_METHOD              0\n",
      "             14 GET_ITER\n",
      "             16 CALL_FUNCTION            1\n",
      "             18 CALL_FUNCTION            1\n",
      "             20 STORE_FAST               2 (n_params)\n",
      "\n",
      "158          22 LOAD_FAST                1 (non_embedding)\n",
      "             24 POP_JUMP_IF_FALSE       22 (to 44)\n",
      "\n",
      "159          26 LOAD_FAST                2 (n_params)\n",
      "             28 LOAD_FAST                0 (self)\n",
      "             30 LOAD_ATTR                2 (transformer)\n",
      "             32 LOAD_ATTR                3 (wpe)\n",
      "             34 LOAD_ATTR                4 (weight)\n",
      "             36 LOAD_METHOD              5 (numel)\n",
      "             38 CALL_METHOD              0\n",
      "             40 INPLACE_SUBTRACT\n",
      "             42 STORE_FAST               2 (n_params)\n",
      "\n",
      "160     >>   44 LOAD_FAST                2 (n_params)\n",
      "             46 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <genexpr> at 0x000002159E131580, file \"Code Analysis/gpt2sample.py\", line 157>:\n",
      "              0 GEN_START                0\n",
      "\n",
      "157           2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 7 (to 20)\n",
      "              6 STORE_FAST               1 (p)\n",
      "              8 LOAD_FAST                1 (p)\n",
      "             10 LOAD_METHOD              0 (numel)\n",
      "             12 CALL_METHOD              0\n",
      "             14 YIELD_VALUE\n",
      "             16 POP_TOP\n",
      "             18 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   20 LOAD_CONST               0 (None)\n",
      "             22 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object _init_weights at 0x000002159E1316E0, file \"Code Analysis/gpt2sample.py\", line 162>:\n",
      "163           0 LOAD_GLOBAL              0 (isinstance)\n",
      "              2 LOAD_FAST                1 (module)\n",
      "              4 LOAD_GLOBAL              1 (nn)\n",
      "              6 LOAD_ATTR                2 (Linear)\n",
      "              8 CALL_FUNCTION            2\n",
      "             10 POP_JUMP_IF_FALSE       34 (to 68)\n",
      "\n",
      "164          12 LOAD_GLOBAL              3 (torch)\n",
      "             14 LOAD_ATTR                1 (nn)\n",
      "             16 LOAD_ATTR                4 (init)\n",
      "             18 LOAD_ATTR                5 (normal_)\n",
      "             20 LOAD_FAST                1 (module)\n",
      "             22 LOAD_ATTR                6 (weight)\n",
      "             24 LOAD_CONST               1 (0.0)\n",
      "             26 LOAD_CONST               2 (0.02)\n",
      "             28 LOAD_CONST               3 (('mean', 'std'))\n",
      "             30 CALL_FUNCTION_KW         3\n",
      "             32 POP_TOP\n",
      "\n",
      "165          34 LOAD_FAST                1 (module)\n",
      "             36 LOAD_ATTR                7 (bias)\n",
      "             38 LOAD_CONST               0 (None)\n",
      "             40 IS_OP                    1\n",
      "             42 POP_JUMP_IF_FALSE       32 (to 64)\n",
      "\n",
      "166          44 LOAD_GLOBAL              3 (torch)\n",
      "             46 LOAD_ATTR                1 (nn)\n",
      "             48 LOAD_ATTR                4 (init)\n",
      "             50 LOAD_METHOD              8 (zeros_)\n",
      "             52 LOAD_FAST                1 (module)\n",
      "             54 LOAD_ATTR                7 (bias)\n",
      "             56 CALL_METHOD              1\n",
      "             58 POP_TOP\n",
      "             60 LOAD_CONST               0 (None)\n",
      "             62 RETURN_VALUE\n",
      "\n",
      "165     >>   64 LOAD_CONST               0 (None)\n",
      "             66 RETURN_VALUE\n",
      "\n",
      "167     >>   68 LOAD_GLOBAL              0 (isinstance)\n",
      "             70 LOAD_FAST                1 (module)\n",
      "             72 LOAD_GLOBAL              1 (nn)\n",
      "             74 LOAD_ATTR                9 (Embedding)\n",
      "             76 CALL_FUNCTION            2\n",
      "             78 POP_JUMP_IF_FALSE       53 (to 106)\n",
      "\n",
      "168          80 LOAD_GLOBAL              3 (torch)\n",
      "             82 LOAD_ATTR                1 (nn)\n",
      "             84 LOAD_ATTR                4 (init)\n",
      "             86 LOAD_ATTR                5 (normal_)\n",
      "             88 LOAD_FAST                1 (module)\n",
      "             90 LOAD_ATTR                6 (weight)\n",
      "             92 LOAD_CONST               1 (0.0)\n",
      "             94 LOAD_CONST               2 (0.02)\n",
      "             96 LOAD_CONST               3 (('mean', 'std'))\n",
      "             98 CALL_FUNCTION_KW         3\n",
      "            100 POP_TOP\n",
      "            102 LOAD_CONST               0 (None)\n",
      "            104 RETURN_VALUE\n",
      "\n",
      "167     >>  106 LOAD_CONST               0 (None)\n",
      "            108 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object forward at 0x000002159E131790, file \"Code Analysis/gpt2sample.py\", line 170>:\n",
      "171           0 LOAD_FAST                1 (idx)\n",
      "              2 LOAD_ATTR                0 (device)\n",
      "              4 STORE_FAST               3 (device)\n",
      "\n",
      "172           6 LOAD_FAST                1 (idx)\n",
      "              8 LOAD_METHOD              1 (size)\n",
      "             10 CALL_METHOD              0\n",
      "             12 UNPACK_SEQUENCE          2\n",
      "             14 STORE_FAST               4 (b)\n",
      "             16 STORE_FAST               5 (t)\n",
      "\n",
      "173          18 LOAD_FAST                5 (t)\n",
      "             20 LOAD_FAST                0 (self)\n",
      "             22 LOAD_ATTR                2 (config)\n",
      "             24 LOAD_ATTR                3 (block_size)\n",
      "             26 COMPARE_OP               1 (<=)\n",
      "             28 POP_JUMP_IF_TRUE        27 (to 54)\n",
      "             30 LOAD_ASSERTION_ERROR\n",
      "             32 LOAD_CONST               1 ('Cannot forward sequence of length ')\n",
      "             34 LOAD_FAST                5 (t)\n",
      "             36 FORMAT_VALUE             0\n",
      "             38 LOAD_CONST               2 (', block size is only ')\n",
      "             40 LOAD_FAST                0 (self)\n",
      "             42 LOAD_ATTR                2 (config)\n",
      "             44 LOAD_ATTR                3 (block_size)\n",
      "             46 FORMAT_VALUE             0\n",
      "             48 BUILD_STRING             4\n",
      "             50 CALL_FUNCTION            1\n",
      "             52 RAISE_VARARGS            1\n",
      "\n",
      "174     >>   54 LOAD_GLOBAL              4 (torch)\n",
      "             56 LOAD_ATTR                5 (arange)\n",
      "             58 LOAD_CONST               3 (0)\n",
      "             60 LOAD_FAST                5 (t)\n",
      "             62 LOAD_GLOBAL              4 (torch)\n",
      "             64 LOAD_ATTR                6 (long)\n",
      "             66 LOAD_FAST                3 (device)\n",
      "             68 LOAD_CONST               4 (('dtype', 'device'))\n",
      "             70 CALL_FUNCTION_KW         4\n",
      "             72 STORE_FAST               6 (pos)\n",
      "\n",
      "177          74 LOAD_FAST                0 (self)\n",
      "             76 LOAD_ATTR                7 (transformer)\n",
      "             78 LOAD_METHOD              8 (wte)\n",
      "             80 LOAD_FAST                1 (idx)\n",
      "             82 CALL_METHOD              1\n",
      "             84 STORE_FAST               7 (tok_emb)\n",
      "\n",
      "178          86 LOAD_FAST                0 (self)\n",
      "             88 LOAD_ATTR                7 (transformer)\n",
      "             90 LOAD_METHOD              9 (wpe)\n",
      "             92 LOAD_FAST                6 (pos)\n",
      "             94 CALL_METHOD              1\n",
      "             96 STORE_FAST               8 (pos_emb)\n",
      "\n",
      "179          98 LOAD_FAST                0 (self)\n",
      "            100 LOAD_ATTR                7 (transformer)\n",
      "            102 LOAD_METHOD             10 (drop)\n",
      "            104 LOAD_FAST                7 (tok_emb)\n",
      "            106 LOAD_FAST                8 (pos_emb)\n",
      "            108 BINARY_ADD\n",
      "            110 CALL_METHOD              1\n",
      "            112 STORE_FAST               9 (x)\n",
      "\n",
      "180         114 LOAD_FAST                0 (self)\n",
      "            116 LOAD_ATTR                7 (transformer)\n",
      "            118 LOAD_ATTR               11 (h)\n",
      "            120 GET_ITER\n",
      "        >>  122 FOR_ITER                 6 (to 136)\n",
      "            124 STORE_FAST              10 (block)\n",
      "\n",
      "181         126 LOAD_FAST               10 (block)\n",
      "            128 LOAD_FAST                9 (x)\n",
      "            130 CALL_FUNCTION            1\n",
      "            132 STORE_FAST               9 (x)\n",
      "            134 JUMP_ABSOLUTE           61 (to 122)\n",
      "\n",
      "182     >>  136 LOAD_FAST                0 (self)\n",
      "            138 LOAD_ATTR                7 (transformer)\n",
      "            140 LOAD_METHOD             12 (ln_f)\n",
      "            142 LOAD_FAST                9 (x)\n",
      "            144 CALL_METHOD              1\n",
      "            146 STORE_FAST               9 (x)\n",
      "\n",
      "184         148 LOAD_FAST                2 (targets)\n",
      "            150 LOAD_CONST               0 (None)\n",
      "            152 IS_OP                    1\n",
      "            154 POP_JUMP_IF_FALSE      105 (to 210)\n",
      "\n",
      "186         156 LOAD_FAST                0 (self)\n",
      "            158 LOAD_METHOD             13 (lm_head)\n",
      "            160 LOAD_FAST                9 (x)\n",
      "            162 CALL_METHOD              1\n",
      "            164 STORE_FAST              11 (logits)\n",
      "\n",
      "187         166 LOAD_GLOBAL             14 (F)\n",
      "            168 LOAD_ATTR               15 (cross_entropy)\n",
      "            170 LOAD_FAST               11 (logits)\n",
      "            172 LOAD_METHOD             16 (view)\n",
      "            174 LOAD_CONST               5 (-1)\n",
      "            176 LOAD_FAST               11 (logits)\n",
      "            178 LOAD_METHOD              1 (size)\n",
      "            180 LOAD_CONST               5 (-1)\n",
      "            182 CALL_METHOD              1\n",
      "            184 CALL_METHOD              2\n",
      "            186 LOAD_FAST                2 (targets)\n",
      "            188 LOAD_METHOD             16 (view)\n",
      "            190 LOAD_CONST               5 (-1)\n",
      "            192 CALL_METHOD              1\n",
      "            194 LOAD_CONST               5 (-1)\n",
      "            196 LOAD_CONST               6 (('ignore_index',))\n",
      "            198 CALL_FUNCTION_KW         3\n",
      "            200 STORE_FAST              12 (loss)\n",
      "\n",
      "193         202 LOAD_FAST               11 (logits)\n",
      "            204 LOAD_FAST               12 (loss)\n",
      "            206 BUILD_TUPLE              2\n",
      "            208 RETURN_VALUE\n",
      "\n",
      "190     >>  210 LOAD_FAST                0 (self)\n",
      "            212 LOAD_METHOD             13 (lm_head)\n",
      "            214 LOAD_FAST                9 (x)\n",
      "            216 LOAD_CONST               0 (None)\n",
      "            218 LOAD_CONST               0 (None)\n",
      "            220 BUILD_SLICE              2\n",
      "            222 LOAD_CONST               5 (-1)\n",
      "            224 BUILD_LIST               1\n",
      "            226 LOAD_CONST               0 (None)\n",
      "            228 LOAD_CONST               0 (None)\n",
      "            230 BUILD_SLICE              2\n",
      "            232 BUILD_TUPLE              3\n",
      "            234 BINARY_SUBSCR\n",
      "            236 CALL_METHOD              1\n",
      "            238 STORE_FAST              11 (logits)\n",
      "\n",
      "191         240 LOAD_CONST               0 (None)\n",
      "            242 STORE_FAST              12 (loss)\n",
      "\n",
      "193         244 LOAD_FAST               11 (logits)\n",
      "            246 LOAD_FAST               12 (loss)\n",
      "            248 BUILD_TUPLE              2\n",
      "            250 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object crop_block_size at 0x000002159E131840, file \"Code Analysis/gpt2sample.py\", line 195>:\n",
      "199           0 LOAD_FAST                1 (block_size)\n",
      "              2 LOAD_FAST                0 (self)\n",
      "              4 LOAD_ATTR                0 (config)\n",
      "              6 LOAD_ATTR                1 (block_size)\n",
      "              8 COMPARE_OP               1 (<=)\n",
      "             10 POP_JUMP_IF_TRUE         8 (to 16)\n",
      "             12 LOAD_ASSERTION_ERROR\n",
      "             14 RAISE_VARARGS            1\n",
      "\n",
      "200     >>   16 LOAD_FAST                1 (block_size)\n",
      "             18 LOAD_FAST                0 (self)\n",
      "             20 LOAD_ATTR                0 (config)\n",
      "             22 STORE_ATTR               1 (block_size)\n",
      "\n",
      "201          24 LOAD_GLOBAL              2 (nn)\n",
      "             26 LOAD_METHOD              3 (Parameter)\n",
      "             28 LOAD_FAST                0 (self)\n",
      "             30 LOAD_ATTR                4 (transformer)\n",
      "             32 LOAD_ATTR                5 (wpe)\n",
      "             34 LOAD_ATTR                6 (weight)\n",
      "             36 LOAD_CONST               0 (None)\n",
      "             38 LOAD_FAST                1 (block_size)\n",
      "             40 BUILD_SLICE              2\n",
      "             42 BINARY_SUBSCR\n",
      "             44 CALL_METHOD              1\n",
      "             46 LOAD_FAST                0 (self)\n",
      "             48 LOAD_ATTR                4 (transformer)\n",
      "             50 LOAD_ATTR                5 (wpe)\n",
      "             52 STORE_ATTR               6 (weight)\n",
      "\n",
      "202          54 LOAD_FAST                0 (self)\n",
      "             56 LOAD_ATTR                4 (transformer)\n",
      "             58 LOAD_ATTR                7 (h)\n",
      "             60 GET_ITER\n",
      "        >>   62 FOR_ITER                28 (to 120)\n",
      "             64 STORE_FAST               2 (block)\n",
      "\n",
      "203          66 LOAD_GLOBAL              8 (hasattr)\n",
      "             68 LOAD_FAST                2 (block)\n",
      "             70 LOAD_ATTR                9 (attn)\n",
      "             72 LOAD_CONST               1 ('bias')\n",
      "             74 CALL_FUNCTION            2\n",
      "             76 POP_JUMP_IF_FALSE       59 (to 118)\n",
      "\n",
      "204          78 LOAD_FAST                2 (block)\n",
      "             80 LOAD_ATTR                9 (attn)\n",
      "             82 LOAD_ATTR               10 (bias)\n",
      "             84 LOAD_CONST               0 (None)\n",
      "             86 LOAD_CONST               0 (None)\n",
      "             88 BUILD_SLICE              2\n",
      "             90 LOAD_CONST               0 (None)\n",
      "             92 LOAD_CONST               0 (None)\n",
      "             94 BUILD_SLICE              2\n",
      "             96 LOAD_CONST               0 (None)\n",
      "             98 LOAD_FAST                1 (block_size)\n",
      "            100 BUILD_SLICE              2\n",
      "            102 LOAD_CONST               0 (None)\n",
      "            104 LOAD_FAST                1 (block_size)\n",
      "            106 BUILD_SLICE              2\n",
      "            108 BUILD_TUPLE              4\n",
      "            110 BINARY_SUBSCR\n",
      "            112 LOAD_FAST                2 (block)\n",
      "            114 LOAD_ATTR                9 (attn)\n",
      "            116 STORE_ATTR              10 (bias)\n",
      "        >>  118 JUMP_ABSOLUTE           31 (to 62)\n",
      "\n",
      "202     >>  120 LOAD_CONST               0 (None)\n",
      "            122 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object from_pretrained at 0x000002159E131C60, file \"Code Analysis/gpt2sample.py\", line 206>:\n",
      "208           0 LOAD_FAST                1 (model_type)\n",
      "              2 LOAD_CONST               1 (frozenset({'gpt2-medium', 'gpt2-xl', 'gpt2-large', 'gpt2'}))\n",
      "              4 CONTAINS_OP              0\n",
      "              6 POP_JUMP_IF_TRUE         6 (to 12)\n",
      "              8 LOAD_ASSERTION_ERROR\n",
      "             10 RAISE_VARARGS            1\n",
      "\n",
      "209     >>   12 LOAD_FAST                2 (override_args)\n",
      "             14 JUMP_IF_TRUE_OR_POP      9 (to 18)\n",
      "             16 BUILD_MAP                0\n",
      "        >>   18 STORE_FAST               2 (override_args)\n",
      "\n",
      "211          20 LOAD_GLOBAL              0 (all)\n",
      "             22 LOAD_CONST               2 (<code object <genexpr> at 0x000002159E1318F0, file \"Code Analysis/gpt2sample.py\", line 211>)\n",
      "             24 LOAD_CONST               3 ('GPT.from_pretrained.<locals>.<genexpr>')\n",
      "             26 MAKE_FUNCTION            0\n",
      "             28 LOAD_FAST                2 (override_args)\n",
      "             30 GET_ITER\n",
      "             32 CALL_FUNCTION            1\n",
      "             34 CALL_FUNCTION            1\n",
      "             36 POP_JUMP_IF_TRUE        21 (to 42)\n",
      "             38 LOAD_ASSERTION_ERROR\n",
      "             40 RAISE_VARARGS            1\n",
      "\n",
      "212     >>   42 LOAD_CONST               4 (0)\n",
      "             44 LOAD_CONST               5 (('GPT2LMHeadModel',))\n",
      "             46 IMPORT_NAME              1 (transformers)\n",
      "             48 IMPORT_FROM              2 (GPT2LMHeadModel)\n",
      "             50 STORE_FAST               3 (GPT2LMHeadModel)\n",
      "             52 POP_TOP\n",
      "\n",
      "213          54 LOAD_GLOBAL              3 (print)\n",
      "             56 LOAD_CONST               6 ('loading weights from pretrained gpt: %s')\n",
      "             58 LOAD_FAST                1 (model_type)\n",
      "             60 BINARY_MODULO\n",
      "             62 CALL_FUNCTION            1\n",
      "             64 POP_TOP\n",
      "\n",
      "217          66 LOAD_GLOBAL              4 (dict)\n",
      "             68 LOAD_CONST               7 (12)\n",
      "             70 LOAD_CONST               7 (12)\n",
      "             72 LOAD_CONST               8 (768)\n",
      "             74 LOAD_CONST               9 (('n_layer', 'n_head', 'n_embd'))\n",
      "             76 CALL_FUNCTION_KW         3\n",
      "\n",
      "218          78 LOAD_GLOBAL              4 (dict)\n",
      "             80 LOAD_CONST              10 (24)\n",
      "             82 LOAD_CONST              11 (16)\n",
      "             84 LOAD_CONST              12 (1024)\n",
      "             86 LOAD_CONST               9 (('n_layer', 'n_head', 'n_embd'))\n",
      "             88 CALL_FUNCTION_KW         3\n",
      "\n",
      "219          90 LOAD_GLOBAL              4 (dict)\n",
      "             92 LOAD_CONST              13 (36)\n",
      "             94 LOAD_CONST              14 (20)\n",
      "             96 LOAD_CONST              15 (1280)\n",
      "             98 LOAD_CONST               9 (('n_layer', 'n_head', 'n_embd'))\n",
      "            100 CALL_FUNCTION_KW         3\n",
      "\n",
      "220         102 LOAD_GLOBAL              4 (dict)\n",
      "            104 LOAD_CONST              16 (48)\n",
      "            106 LOAD_CONST              17 (25)\n",
      "            108 LOAD_CONST              18 (1600)\n",
      "            110 LOAD_CONST               9 (('n_layer', 'n_head', 'n_embd'))\n",
      "            112 CALL_FUNCTION_KW         3\n",
      "\n",
      "216         114 LOAD_CONST              19 (('gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'))\n",
      "            116 BUILD_CONST_KEY_MAP      4\n",
      "\n",
      "221         118 LOAD_FAST                1 (model_type)\n",
      "\n",
      "216         120 BINARY_SUBSCR\n",
      "            122 STORE_FAST               4 (config_args)\n",
      "\n",
      "222         124 LOAD_GLOBAL              3 (print)\n",
      "            126 LOAD_CONST              20 ('forcing vocab_size=50257, block_size=1024, bias=True')\n",
      "            128 CALL_FUNCTION            1\n",
      "            130 POP_TOP\n",
      "\n",
      "223         132 LOAD_CONST              21 (50257)\n",
      "            134 LOAD_FAST                4 (config_args)\n",
      "            136 LOAD_CONST              22 ('vocab_size')\n",
      "            138 STORE_SUBSCR\n",
      "\n",
      "224         140 LOAD_CONST              12 (1024)\n",
      "            142 LOAD_FAST                4 (config_args)\n",
      "            144 LOAD_CONST              23 ('block_size')\n",
      "            146 STORE_SUBSCR\n",
      "\n",
      "225         148 LOAD_CONST              24 (True)\n",
      "            150 LOAD_FAST                4 (config_args)\n",
      "            152 LOAD_CONST              25 ('bias')\n",
      "            154 STORE_SUBSCR\n",
      "\n",
      "227         156 LOAD_CONST              26 ('dropout')\n",
      "            158 LOAD_FAST                2 (override_args)\n",
      "            160 CONTAINS_OP              0\n",
      "            162 POP_JUMP_IF_FALSE       97 (to 194)\n",
      "\n",
      "228         164 LOAD_GLOBAL              3 (print)\n",
      "            166 LOAD_CONST              27 ('overriding dropout rate to ')\n",
      "            168 LOAD_FAST                2 (override_args)\n",
      "            170 LOAD_CONST              26 ('dropout')\n",
      "            172 BINARY_SUBSCR\n",
      "            174 FORMAT_VALUE             0\n",
      "            176 BUILD_STRING             2\n",
      "            178 CALL_FUNCTION            1\n",
      "            180 POP_TOP\n",
      "\n",
      "229         182 LOAD_FAST                2 (override_args)\n",
      "            184 LOAD_CONST              26 ('dropout')\n",
      "            186 BINARY_SUBSCR\n",
      "            188 LOAD_FAST                4 (config_args)\n",
      "            190 LOAD_CONST              26 ('dropout')\n",
      "            192 STORE_SUBSCR\n",
      "\n",
      "231     >>  194 LOAD_GLOBAL              5 (GPTConfig)\n",
      "            196 LOAD_CONST              37 (())\n",
      "            198 BUILD_MAP                0\n",
      "            200 LOAD_FAST                4 (config_args)\n",
      "            202 DICT_MERGE               1\n",
      "            204 CALL_FUNCTION_EX         1\n",
      "            206 STORE_FAST               5 (config)\n",
      "\n",
      "232         208 LOAD_GLOBAL              6 (GPT)\n",
      "            210 LOAD_FAST                5 (config)\n",
      "            212 CALL_FUNCTION            1\n",
      "            214 STORE_FAST               6 (model)\n",
      "\n",
      "233         216 LOAD_FAST                6 (model)\n",
      "            218 LOAD_METHOD              7 (state_dict)\n",
      "            220 CALL_METHOD              0\n",
      "            222 STORE_FAST               7 (sd)\n",
      "\n",
      "234         224 LOAD_FAST                7 (sd)\n",
      "            226 LOAD_METHOD              8 (keys)\n",
      "            228 CALL_METHOD              0\n",
      "            230 STORE_FAST               8 (sd_keys)\n",
      "\n",
      "235         232 LOAD_CONST              28 (<code object <listcomp> at 0x000002159E1319A0, file \"Code Analysis/gpt2sample.py\", line 235>)\n",
      "            234 LOAD_CONST              29 ('GPT.from_pretrained.<locals>.<listcomp>')\n",
      "            236 MAKE_FUNCTION            0\n",
      "            238 LOAD_FAST                8 (sd_keys)\n",
      "            240 GET_ITER\n",
      "            242 CALL_FUNCTION            1\n",
      "            244 STORE_FAST               8 (sd_keys)\n",
      "\n",
      "238         246 LOAD_FAST                3 (GPT2LMHeadModel)\n",
      "            248 LOAD_METHOD              9 (from_pretrained)\n",
      "            250 LOAD_FAST                1 (model_type)\n",
      "            252 CALL_METHOD              1\n",
      "            254 STORE_FAST               9 (model_hf)\n",
      "\n",
      "239         256 LOAD_FAST                9 (model_hf)\n",
      "            258 LOAD_METHOD              7 (state_dict)\n",
      "            260 CALL_METHOD              0\n",
      "            262 STORE_FAST              10 (sd_hf)\n",
      "\n",
      "242         264 LOAD_FAST               10 (sd_hf)\n",
      "            266 LOAD_METHOD              8 (keys)\n",
      "            268 CALL_METHOD              0\n",
      "            270 STORE_FAST              11 (sd_keys_hf)\n",
      "\n",
      "243         272 LOAD_CONST              30 (<code object <listcomp> at 0x000002159E131A50, file \"Code Analysis/gpt2sample.py\", line 243>)\n",
      "            274 LOAD_CONST              29 ('GPT.from_pretrained.<locals>.<listcomp>')\n",
      "            276 MAKE_FUNCTION            0\n",
      "            278 LOAD_FAST               11 (sd_keys_hf)\n",
      "            280 GET_ITER\n",
      "            282 CALL_FUNCTION            1\n",
      "            284 STORE_FAST              11 (sd_keys_hf)\n",
      "\n",
      "244         286 LOAD_CONST              31 (<code object <listcomp> at 0x000002159E131B00, file \"Code Analysis/gpt2sample.py\", line 244>)\n",
      "            288 LOAD_CONST              29 ('GPT.from_pretrained.<locals>.<listcomp>')\n",
      "            290 MAKE_FUNCTION            0\n",
      "            292 LOAD_FAST               11 (sd_keys_hf)\n",
      "            294 GET_ITER\n",
      "            296 CALL_FUNCTION            1\n",
      "            298 STORE_FAST              11 (sd_keys_hf)\n",
      "\n",
      "245         300 BUILD_LIST               0\n",
      "            302 LOAD_CONST              32 (('attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight'))\n",
      "            304 LIST_EXTEND              1\n",
      "            306 STORE_FAST              12 (transposed)\n",
      "\n",
      "248         308 LOAD_GLOBAL             10 (len)\n",
      "            310 LOAD_FAST               11 (sd_keys_hf)\n",
      "            312 CALL_FUNCTION            1\n",
      "            314 LOAD_GLOBAL             10 (len)\n",
      "            316 LOAD_FAST                8 (sd_keys)\n",
      "            318 CALL_FUNCTION            1\n",
      "            320 COMPARE_OP               2 (==)\n",
      "            322 POP_JUMP_IF_TRUE       176 (to 352)\n",
      "            324 LOAD_ASSERTION_ERROR\n",
      "            326 LOAD_CONST              33 ('mismatched keys: ')\n",
      "            328 LOAD_GLOBAL             10 (len)\n",
      "            330 LOAD_FAST               11 (sd_keys_hf)\n",
      "            332 CALL_FUNCTION            1\n",
      "            334 FORMAT_VALUE             0\n",
      "            336 LOAD_CONST              34 (' != ')\n",
      "            338 LOAD_GLOBAL             10 (len)\n",
      "            340 LOAD_FAST                8 (sd_keys)\n",
      "            342 CALL_FUNCTION            1\n",
      "            344 FORMAT_VALUE             0\n",
      "            346 BUILD_STRING             4\n",
      "            348 CALL_FUNCTION            1\n",
      "            350 RAISE_VARARGS            1\n",
      "\n",
      "249     >>  352 LOAD_FAST               11 (sd_keys_hf)\n",
      "            354 GET_ITER\n",
      "        >>  356 FOR_ITER               104 (to 566)\n",
      "            358 STORE_DEREF              0 (k)\n",
      "\n",
      "250         360 LOAD_GLOBAL             11 (any)\n",
      "            362 LOAD_CLOSURE             0 (k)\n",
      "            364 BUILD_TUPLE              1\n",
      "            366 LOAD_CONST              35 (<code object <genexpr> at 0x000002159E131BB0, file \"Code Analysis/gpt2sample.py\", line 250>)\n",
      "            368 LOAD_CONST               3 ('GPT.from_pretrained.<locals>.<genexpr>')\n",
      "            370 MAKE_FUNCTION            8 (closure)\n",
      "            372 LOAD_FAST               12 (transposed)\n",
      "            374 GET_ITER\n",
      "            376 CALL_FUNCTION            1\n",
      "            378 CALL_FUNCTION            1\n",
      "            380 POP_JUMP_IF_FALSE      240 (to 480)\n",
      "\n",
      "252         382 LOAD_FAST               10 (sd_hf)\n",
      "            384 LOAD_DEREF               0 (k)\n",
      "            386 BINARY_SUBSCR\n",
      "            388 LOAD_ATTR               12 (shape)\n",
      "            390 LOAD_CONST               0 (None)\n",
      "            392 LOAD_CONST               0 (None)\n",
      "            394 LOAD_CONST              36 (-1)\n",
      "            396 BUILD_SLICE              3\n",
      "            398 BINARY_SUBSCR\n",
      "            400 LOAD_FAST                7 (sd)\n",
      "            402 LOAD_DEREF               0 (k)\n",
      "            404 BINARY_SUBSCR\n",
      "            406 LOAD_ATTR               12 (shape)\n",
      "            408 COMPARE_OP               2 (==)\n",
      "            410 POP_JUMP_IF_TRUE       208 (to 416)\n",
      "            412 LOAD_ASSERTION_ERROR\n",
      "            414 RAISE_VARARGS            1\n",
      "\n",
      "253     >>  416 LOAD_GLOBAL             13 (torch)\n",
      "            418 LOAD_METHOD             14 (no_grad)\n",
      "            420 CALL_METHOD              0\n",
      "            422 SETUP_WITH              19 (to 462)\n",
      "            424 POP_TOP\n",
      "\n",
      "254         426 LOAD_FAST                7 (sd)\n",
      "            428 LOAD_DEREF               0 (k)\n",
      "            430 BINARY_SUBSCR\n",
      "            432 LOAD_METHOD             15 (copy_)\n",
      "            434 LOAD_FAST               10 (sd_hf)\n",
      "            436 LOAD_DEREF               0 (k)\n",
      "            438 BINARY_SUBSCR\n",
      "            440 LOAD_METHOD             16 (t)\n",
      "            442 CALL_METHOD              0\n",
      "            444 CALL_METHOD              1\n",
      "            446 POP_TOP\n",
      "            448 POP_BLOCK\n",
      "\n",
      "253         450 LOAD_CONST               0 (None)\n",
      "            452 DUP_TOP\n",
      "            454 DUP_TOP\n",
      "            456 CALL_FUNCTION            3\n",
      "            458 POP_TOP\n",
      "            460 JUMP_FORWARD             8 (to 478)\n",
      "        >>  462 WITH_EXCEPT_START\n",
      "            464 POP_JUMP_IF_TRUE       234 (to 468)\n",
      "            466 RERAISE                  1\n",
      "        >>  468 POP_TOP\n",
      "            470 POP_TOP\n",
      "            472 POP_TOP\n",
      "            474 POP_EXCEPT\n",
      "            476 POP_TOP\n",
      "        >>  478 JUMP_ABSOLUTE          178 (to 356)\n",
      "\n",
      "257     >>  480 LOAD_FAST               10 (sd_hf)\n",
      "            482 LOAD_DEREF               0 (k)\n",
      "            484 BINARY_SUBSCR\n",
      "            486 LOAD_ATTR               12 (shape)\n",
      "            488 LOAD_FAST                7 (sd)\n",
      "            490 LOAD_DEREF               0 (k)\n",
      "            492 BINARY_SUBSCR\n",
      "            494 LOAD_ATTR               12 (shape)\n",
      "            496 COMPARE_OP               2 (==)\n",
      "            498 POP_JUMP_IF_TRUE       252 (to 504)\n",
      "            500 LOAD_ASSERTION_ERROR\n",
      "            502 RAISE_VARARGS            1\n",
      "\n",
      "258     >>  504 LOAD_GLOBAL             13 (torch)\n",
      "            506 LOAD_METHOD             14 (no_grad)\n",
      "            508 CALL_METHOD              0\n",
      "            510 SETUP_WITH              17 (to 546)\n",
      "            512 POP_TOP\n",
      "\n",
      "259         514 LOAD_FAST                7 (sd)\n",
      "            516 LOAD_DEREF               0 (k)\n",
      "            518 BINARY_SUBSCR\n",
      "            520 LOAD_METHOD             15 (copy_)\n",
      "            522 LOAD_FAST               10 (sd_hf)\n",
      "            524 LOAD_DEREF               0 (k)\n",
      "            526 BINARY_SUBSCR\n",
      "            528 CALL_METHOD              1\n",
      "            530 POP_TOP\n",
      "            532 POP_BLOCK\n",
      "\n",
      "258         534 LOAD_CONST               0 (None)\n",
      "            536 DUP_TOP\n",
      "            538 DUP_TOP\n",
      "            540 CALL_FUNCTION            3\n",
      "            542 POP_TOP\n",
      "            544 JUMP_FORWARD             9 (to 564)\n",
      "        >>  546 WITH_EXCEPT_START\n",
      "            548 EXTENDED_ARG             1\n",
      "            550 POP_JUMP_IF_TRUE       277 (to 554)\n",
      "            552 RERAISE                  1\n",
      "        >>  554 POP_TOP\n",
      "            556 POP_TOP\n",
      "            558 POP_TOP\n",
      "            560 POP_EXCEPT\n",
      "            562 POP_TOP\n",
      "        >>  564 JUMP_ABSOLUTE          178 (to 356)\n",
      "\n",
      "261     >>  566 LOAD_FAST                6 (model)\n",
      "            568 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <genexpr> at 0x000002159E1318F0, file \"Code Analysis/gpt2sample.py\", line 211>:\n",
      "              0 GEN_START                0\n",
      "\n",
      "211           2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 7 (to 20)\n",
      "              6 STORE_FAST               1 (k)\n",
      "              8 LOAD_FAST                1 (k)\n",
      "             10 LOAD_CONST               0 ('dropout')\n",
      "             12 COMPARE_OP               2 (==)\n",
      "             14 YIELD_VALUE\n",
      "             16 POP_TOP\n",
      "             18 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   20 LOAD_CONST               1 (None)\n",
      "             22 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E1319A0, file \"Code Analysis/gpt2sample.py\", line 235>:\n",
      "235           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 9 (to 24)\n",
      "              6 STORE_FAST               1 (k)\n",
      "              8 LOAD_FAST                1 (k)\n",
      "             10 LOAD_METHOD              0 (endswith)\n",
      "             12 LOAD_CONST               0 ('.attn.bias')\n",
      "             14 CALL_METHOD              1\n",
      "             16 POP_JUMP_IF_TRUE         2 (to 4)\n",
      "             18 LOAD_FAST                1 (k)\n",
      "             20 LIST_APPEND              2\n",
      "             22 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   24 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E131A50, file \"Code Analysis/gpt2sample.py\", line 243>:\n",
      "243           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 9 (to 24)\n",
      "              6 STORE_FAST               1 (k)\n",
      "              8 LOAD_FAST                1 (k)\n",
      "             10 LOAD_METHOD              0 (endswith)\n",
      "             12 LOAD_CONST               0 ('.attn.masked_bias')\n",
      "             14 CALL_METHOD              1\n",
      "             16 POP_JUMP_IF_TRUE         2 (to 4)\n",
      "             18 LOAD_FAST                1 (k)\n",
      "             20 LIST_APPEND              2\n",
      "             22 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   24 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E131B00, file \"Code Analysis/gpt2sample.py\", line 244>:\n",
      "244           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 9 (to 24)\n",
      "              6 STORE_FAST               1 (k)\n",
      "              8 LOAD_FAST                1 (k)\n",
      "             10 LOAD_METHOD              0 (endswith)\n",
      "             12 LOAD_CONST               0 ('.attn.bias')\n",
      "             14 CALL_METHOD              1\n",
      "             16 POP_JUMP_IF_TRUE         2 (to 4)\n",
      "             18 LOAD_FAST                1 (k)\n",
      "             20 LIST_APPEND              2\n",
      "             22 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   24 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <genexpr> at 0x000002159E131BB0, file \"Code Analysis/gpt2sample.py\", line 250>:\n",
      "              0 GEN_START                0\n",
      "\n",
      "250           2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 8 (to 22)\n",
      "              6 STORE_FAST               1 (w)\n",
      "              8 LOAD_DEREF               0 (k)\n",
      "             10 LOAD_METHOD              0 (endswith)\n",
      "             12 LOAD_FAST                1 (w)\n",
      "             14 CALL_METHOD              1\n",
      "             16 YIELD_VALUE\n",
      "             18 POP_TOP\n",
      "             20 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   22 LOAD_CONST               0 (None)\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object configure_optimizers at 0x000002159E132130, file \"Code Analysis/gpt2sample.py\", line 263>:\n",
      "265           0 LOAD_CONST               1 (<code object <dictcomp> at 0x000002159E131D10, file \"Code Analysis/gpt2sample.py\", line 265>)\n",
      "              2 LOAD_CONST               2 ('GPT.configure_optimizers.<locals>.<dictcomp>')\n",
      "              4 MAKE_FUNCTION            0\n",
      "              6 LOAD_FAST                0 (self)\n",
      "              8 LOAD_METHOD              0 (named_parameters)\n",
      "             10 CALL_METHOD              0\n",
      "             12 GET_ITER\n",
      "             14 CALL_FUNCTION            1\n",
      "             16 STORE_FAST               5 (param_dict)\n",
      "\n",
      "267          18 LOAD_CONST               3 (<code object <dictcomp> at 0x000002159E131DC0, file \"Code Analysis/gpt2sample.py\", line 267>)\n",
      "             20 LOAD_CONST               2 ('GPT.configure_optimizers.<locals>.<dictcomp>')\n",
      "             22 MAKE_FUNCTION            0\n",
      "             24 LOAD_FAST                5 (param_dict)\n",
      "             26 LOAD_METHOD              1 (items)\n",
      "             28 CALL_METHOD              0\n",
      "             30 GET_ITER\n",
      "             32 CALL_FUNCTION            1\n",
      "             34 STORE_FAST               5 (param_dict)\n",
      "\n",
      "270          36 LOAD_CONST               4 (<code object <listcomp> at 0x000002159E131E70, file \"Code Analysis/gpt2sample.py\", line 270>)\n",
      "             38 LOAD_CONST               5 ('GPT.configure_optimizers.<locals>.<listcomp>')\n",
      "             40 MAKE_FUNCTION            0\n",
      "             42 LOAD_FAST                5 (param_dict)\n",
      "             44 LOAD_METHOD              1 (items)\n",
      "             46 CALL_METHOD              0\n",
      "             48 GET_ITER\n",
      "             50 CALL_FUNCTION            1\n",
      "             52 STORE_FAST               6 (decay_params)\n",
      "\n",
      "271          54 LOAD_CONST               6 (<code object <listcomp> at 0x000002159E131F20, file \"Code Analysis/gpt2sample.py\", line 271>)\n",
      "             56 LOAD_CONST               5 ('GPT.configure_optimizers.<locals>.<listcomp>')\n",
      "             58 MAKE_FUNCTION            0\n",
      "             60 LOAD_FAST                5 (param_dict)\n",
      "             62 LOAD_METHOD              1 (items)\n",
      "             64 CALL_METHOD              0\n",
      "             66 GET_ITER\n",
      "             68 CALL_FUNCTION            1\n",
      "             70 STORE_FAST               7 (nodecay_params)\n",
      "\n",
      "273          72 LOAD_FAST                6 (decay_params)\n",
      "             74 LOAD_FAST                1 (weight_decay)\n",
      "             76 LOAD_CONST               7 (('params', 'weight_decay'))\n",
      "             78 BUILD_CONST_KEY_MAP      2\n",
      "\n",
      "274          80 LOAD_FAST                7 (nodecay_params)\n",
      "             82 LOAD_CONST               8 (0.0)\n",
      "             84 LOAD_CONST               7 (('params', 'weight_decay'))\n",
      "             86 BUILD_CONST_KEY_MAP      2\n",
      "\n",
      "272          88 BUILD_LIST               2\n",
      "             90 STORE_FAST               8 (optim_groups)\n",
      "\n",
      "276          92 LOAD_GLOBAL              2 (sum)\n",
      "             94 LOAD_CONST               9 (<code object <genexpr> at 0x000002159E131FD0, file \"Code Analysis/gpt2sample.py\", line 276>)\n",
      "             96 LOAD_CONST              10 ('GPT.configure_optimizers.<locals>.<genexpr>')\n",
      "             98 MAKE_FUNCTION            0\n",
      "            100 LOAD_FAST                6 (decay_params)\n",
      "            102 GET_ITER\n",
      "            104 CALL_FUNCTION            1\n",
      "            106 CALL_FUNCTION            1\n",
      "            108 STORE_FAST               9 (num_decay_params)\n",
      "\n",
      "277         110 LOAD_GLOBAL              2 (sum)\n",
      "            112 LOAD_CONST              11 (<code object <genexpr> at 0x000002159E132080, file \"Code Analysis/gpt2sample.py\", line 277>)\n",
      "            114 LOAD_CONST              10 ('GPT.configure_optimizers.<locals>.<genexpr>')\n",
      "            116 MAKE_FUNCTION            0\n",
      "            118 LOAD_FAST                7 (nodecay_params)\n",
      "            120 GET_ITER\n",
      "            122 CALL_FUNCTION            1\n",
      "            124 CALL_FUNCTION            1\n",
      "            126 STORE_FAST              10 (num_nodecay_params)\n",
      "\n",
      "278         128 LOAD_GLOBAL              3 (print)\n",
      "            130 LOAD_CONST              12 ('num decayed parameter tensors: ')\n",
      "            132 LOAD_GLOBAL              4 (len)\n",
      "            134 LOAD_FAST                6 (decay_params)\n",
      "            136 CALL_FUNCTION            1\n",
      "            138 FORMAT_VALUE             0\n",
      "            140 LOAD_CONST              13 (', with ')\n",
      "            142 LOAD_FAST                9 (num_decay_params)\n",
      "            144 LOAD_CONST              14 (',')\n",
      "            146 FORMAT_VALUE             4 (with format)\n",
      "            148 LOAD_CONST              15 (' parameters')\n",
      "            150 BUILD_STRING             5\n",
      "            152 CALL_FUNCTION            1\n",
      "            154 POP_TOP\n",
      "\n",
      "279         156 LOAD_GLOBAL              3 (print)\n",
      "            158 LOAD_CONST              16 ('num non-decayed parameter tensors: ')\n",
      "            160 LOAD_GLOBAL              4 (len)\n",
      "            162 LOAD_FAST                7 (nodecay_params)\n",
      "            164 CALL_FUNCTION            1\n",
      "            166 FORMAT_VALUE             0\n",
      "            168 LOAD_CONST              13 (', with ')\n",
      "            170 LOAD_FAST               10 (num_nodecay_params)\n",
      "            172 LOAD_CONST              14 (',')\n",
      "            174 FORMAT_VALUE             4 (with format)\n",
      "            176 LOAD_CONST              15 (' parameters')\n",
      "            178 BUILD_STRING             5\n",
      "            180 CALL_FUNCTION            1\n",
      "            182 POP_TOP\n",
      "\n",
      "281         184 LOAD_CONST              17 ('fused')\n",
      "            186 LOAD_GLOBAL              5 (inspect)\n",
      "            188 LOAD_METHOD              6 (signature)\n",
      "            190 LOAD_GLOBAL              7 (torch)\n",
      "            192 LOAD_ATTR                8 (optim)\n",
      "            194 LOAD_ATTR                9 (AdamW)\n",
      "            196 CALL_METHOD              1\n",
      "            198 LOAD_ATTR               10 (parameters)\n",
      "            200 CONTAINS_OP              0\n",
      "            202 STORE_FAST              11 (fused_available)\n",
      "\n",
      "282         204 LOAD_FAST               11 (fused_available)\n",
      "            206 JUMP_IF_FALSE_OR_POP   107 (to 214)\n",
      "            208 LOAD_FAST                4 (device_type)\n",
      "            210 LOAD_CONST              18 ('cuda')\n",
      "            212 COMPARE_OP               2 (==)\n",
      "        >>  214 STORE_FAST              12 (use_fused)\n",
      "\n",
      "283         216 LOAD_FAST               12 (use_fused)\n",
      "            218 POP_JUMP_IF_FALSE      115 (to 230)\n",
      "            220 LOAD_GLOBAL             11 (dict)\n",
      "            222 LOAD_CONST              19 (True)\n",
      "            224 LOAD_CONST              20 (('fused',))\n",
      "            226 CALL_FUNCTION_KW         1\n",
      "            228 JUMP_FORWARD             2 (to 234)\n",
      "        >>  230 LOAD_GLOBAL             11 (dict)\n",
      "            232 CALL_FUNCTION            0\n",
      "        >>  234 STORE_FAST              13 (extra_args)\n",
      "\n",
      "284         236 LOAD_GLOBAL              7 (torch)\n",
      "            238 LOAD_ATTR                8 (optim)\n",
      "            240 LOAD_ATTR                9 (AdamW)\n",
      "            242 LOAD_FAST                8 (optim_groups)\n",
      "            244 BUILD_TUPLE              1\n",
      "            246 LOAD_FAST                2 (learning_rate)\n",
      "            248 LOAD_FAST                3 (betas)\n",
      "            250 LOAD_CONST              21 (('lr', 'betas'))\n",
      "            252 BUILD_CONST_KEY_MAP      2\n",
      "            254 LOAD_FAST               13 (extra_args)\n",
      "            256 DICT_MERGE               1\n",
      "            258 CALL_FUNCTION_EX         1\n",
      "            260 STORE_FAST              14 (optimizer)\n",
      "\n",
      "285         262 LOAD_GLOBAL              3 (print)\n",
      "            264 LOAD_CONST              22 ('using fused AdamW: ')\n",
      "            266 LOAD_FAST               12 (use_fused)\n",
      "            268 FORMAT_VALUE             0\n",
      "            270 BUILD_STRING             2\n",
      "            272 CALL_FUNCTION            1\n",
      "            274 POP_TOP\n",
      "\n",
      "287         276 LOAD_FAST               14 (optimizer)\n",
      "            278 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <dictcomp> at 0x000002159E131D10, file \"Code Analysis/gpt2sample.py\", line 265>:\n",
      "265           0 BUILD_MAP                0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 7 (to 20)\n",
      "              6 UNPACK_SEQUENCE          2\n",
      "              8 STORE_FAST               1 (pn)\n",
      "             10 STORE_FAST               2 (p)\n",
      "             12 LOAD_FAST                1 (pn)\n",
      "             14 LOAD_FAST                2 (p)\n",
      "             16 MAP_ADD                  2\n",
      "             18 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   20 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <dictcomp> at 0x000002159E131DC0, file \"Code Analysis/gpt2sample.py\", line 267>:\n",
      "267           0 BUILD_MAP                0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                10 (to 26)\n",
      "              6 UNPACK_SEQUENCE          2\n",
      "              8 STORE_FAST               1 (pn)\n",
      "             10 STORE_FAST               2 (p)\n",
      "             12 LOAD_FAST                2 (p)\n",
      "             14 LOAD_ATTR                0 (requires_grad)\n",
      "             16 POP_JUMP_IF_FALSE        2 (to 4)\n",
      "             18 LOAD_FAST                1 (pn)\n",
      "             20 LOAD_FAST                2 (p)\n",
      "             22 MAP_ADD                  2\n",
      "             24 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   26 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E131E70, file \"Code Analysis/gpt2sample.py\", line 270>:\n",
      "270           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                12 (to 30)\n",
      "              6 UNPACK_SEQUENCE          2\n",
      "              8 STORE_FAST               1 (n)\n",
      "             10 STORE_FAST               2 (p)\n",
      "             12 LOAD_FAST                2 (p)\n",
      "             14 LOAD_METHOD              0 (dim)\n",
      "             16 CALL_METHOD              0\n",
      "             18 LOAD_CONST               0 (2)\n",
      "             20 COMPARE_OP               5 (>=)\n",
      "             22 POP_JUMP_IF_FALSE        2 (to 4)\n",
      "             24 LOAD_FAST                2 (p)\n",
      "             26 LIST_APPEND              2\n",
      "             28 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   30 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E131F20, file \"Code Analysis/gpt2sample.py\", line 271>:\n",
      "271           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                12 (to 30)\n",
      "              6 UNPACK_SEQUENCE          2\n",
      "              8 STORE_FAST               1 (n)\n",
      "             10 STORE_FAST               2 (p)\n",
      "             12 LOAD_FAST                2 (p)\n",
      "             14 LOAD_METHOD              0 (dim)\n",
      "             16 CALL_METHOD              0\n",
      "             18 LOAD_CONST               0 (2)\n",
      "             20 COMPARE_OP               0 (<)\n",
      "             22 POP_JUMP_IF_FALSE        2 (to 4)\n",
      "             24 LOAD_FAST                2 (p)\n",
      "             26 LIST_APPEND              2\n",
      "             28 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   30 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <genexpr> at 0x000002159E131FD0, file \"Code Analysis/gpt2sample.py\", line 276>:\n",
      "              0 GEN_START                0\n",
      "\n",
      "276           2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 7 (to 20)\n",
      "              6 STORE_FAST               1 (p)\n",
      "              8 LOAD_FAST                1 (p)\n",
      "             10 LOAD_METHOD              0 (numel)\n",
      "             12 CALL_METHOD              0\n",
      "             14 YIELD_VALUE\n",
      "             16 POP_TOP\n",
      "             18 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   20 LOAD_CONST               0 (None)\n",
      "             22 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <genexpr> at 0x000002159E132080, file \"Code Analysis/gpt2sample.py\", line 277>:\n",
      "              0 GEN_START                0\n",
      "\n",
      "277           2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 7 (to 20)\n",
      "              6 STORE_FAST               1 (p)\n",
      "              8 LOAD_FAST                1 (p)\n",
      "             10 LOAD_METHOD              0 (numel)\n",
      "             12 CALL_METHOD              0\n",
      "             14 YIELD_VALUE\n",
      "             16 POP_TOP\n",
      "             18 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   20 LOAD_CONST               0 (None)\n",
      "             22 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object estimate_mfu at 0x000002159E1321E0, file \"Code Analysis/gpt2sample.py\", line 289>:\n",
      "293           0 LOAD_FAST                0 (self)\n",
      "              2 LOAD_METHOD              0 (get_num_params)\n",
      "              4 CALL_METHOD              0\n",
      "              6 STORE_FAST               3 (N)\n",
      "\n",
      "294           8 LOAD_FAST                0 (self)\n",
      "             10 LOAD_ATTR                1 (config)\n",
      "             12 STORE_FAST               4 (cfg)\n",
      "\n",
      "295          14 LOAD_FAST                4 (cfg)\n",
      "             16 LOAD_ATTR                2 (n_layer)\n",
      "             18 LOAD_FAST                4 (cfg)\n",
      "             20 LOAD_ATTR                3 (n_head)\n",
      "             22 LOAD_FAST                4 (cfg)\n",
      "             24 LOAD_ATTR                4 (n_embd)\n",
      "             26 LOAD_FAST                4 (cfg)\n",
      "             28 LOAD_ATTR                3 (n_head)\n",
      "             30 BINARY_FLOOR_DIVIDE\n",
      "             32 LOAD_FAST                4 (cfg)\n",
      "             34 LOAD_ATTR                5 (block_size)\n",
      "             36 BUILD_TUPLE              4\n",
      "             38 UNPACK_SEQUENCE          4\n",
      "             40 STORE_FAST               5 (L)\n",
      "             42 STORE_FAST               6 (H)\n",
      "             44 STORE_FAST               7 (Q)\n",
      "             46 STORE_FAST               8 (T)\n",
      "\n",
      "296          48 LOAD_CONST               1 (6)\n",
      "             50 LOAD_FAST                3 (N)\n",
      "             52 BINARY_MULTIPLY\n",
      "             54 LOAD_CONST               2 (12)\n",
      "             56 LOAD_FAST                5 (L)\n",
      "             58 BINARY_MULTIPLY\n",
      "             60 LOAD_FAST                6 (H)\n",
      "             62 BINARY_MULTIPLY\n",
      "             64 LOAD_FAST                7 (Q)\n",
      "             66 BINARY_MULTIPLY\n",
      "             68 LOAD_FAST                8 (T)\n",
      "             70 BINARY_MULTIPLY\n",
      "             72 BINARY_ADD\n",
      "             74 STORE_FAST               9 (flops_per_token)\n",
      "\n",
      "297          76 LOAD_FAST                9 (flops_per_token)\n",
      "             78 LOAD_FAST                8 (T)\n",
      "             80 BINARY_MULTIPLY\n",
      "             82 STORE_FAST              10 (flops_per_fwdbwd)\n",
      "\n",
      "298          84 LOAD_FAST               10 (flops_per_fwdbwd)\n",
      "             86 LOAD_FAST                1 (fwdbwd_per_iter)\n",
      "             88 BINARY_MULTIPLY\n",
      "             90 STORE_FAST              11 (flops_per_iter)\n",
      "\n",
      "300          92 LOAD_FAST               11 (flops_per_iter)\n",
      "             94 LOAD_CONST               3 (1.0)\n",
      "             96 LOAD_FAST                2 (dt)\n",
      "             98 BINARY_TRUE_DIVIDE\n",
      "            100 BINARY_MULTIPLY\n",
      "            102 STORE_FAST              12 (flops_achieved)\n",
      "\n",
      "301         104 LOAD_CONST               4 (312000000000000.0)\n",
      "            106 STORE_FAST              13 (flops_promised)\n",
      "\n",
      "302         108 LOAD_FAST               12 (flops_achieved)\n",
      "            110 LOAD_FAST               13 (flops_promised)\n",
      "            112 BINARY_TRUE_DIVIDE\n",
      "            114 STORE_FAST              14 (mfu)\n",
      "\n",
      "303         116 LOAD_FAST               14 (mfu)\n",
      "            118 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object generate at 0x000002159E132290, file \"Code Analysis/gpt2sample.py\", line 305>:\n",
      "312           0 LOAD_GLOBAL              0 (range)\n",
      "              2 LOAD_FAST                2 (max_new_tokens)\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 GET_ITER\n",
      "        >>    8 FOR_ITER               102 (to 214)\n",
      "             10 STORE_FAST               5 (_)\n",
      "\n",
      "314          12 LOAD_FAST                1 (idx)\n",
      "             14 LOAD_METHOD              1 (size)\n",
      "             16 LOAD_CONST               1 (1)\n",
      "             18 CALL_METHOD              1\n",
      "             20 LOAD_FAST                0 (self)\n",
      "             22 LOAD_ATTR                2 (config)\n",
      "             24 LOAD_ATTR                3 (block_size)\n",
      "             26 COMPARE_OP               1 (<=)\n",
      "             28 POP_JUMP_IF_FALSE       17 (to 34)\n",
      "             30 LOAD_FAST                1 (idx)\n",
      "             32 JUMP_FORWARD            12 (to 58)\n",
      "        >>   34 LOAD_FAST                1 (idx)\n",
      "             36 LOAD_CONST               2 (None)\n",
      "             38 LOAD_CONST               2 (None)\n",
      "             40 BUILD_SLICE              2\n",
      "             42 LOAD_FAST                0 (self)\n",
      "             44 LOAD_ATTR                2 (config)\n",
      "             46 LOAD_ATTR                3 (block_size)\n",
      "             48 UNARY_NEGATIVE\n",
      "             50 LOAD_CONST               2 (None)\n",
      "             52 BUILD_SLICE              2\n",
      "             54 BUILD_TUPLE              2\n",
      "             56 BINARY_SUBSCR\n",
      "        >>   58 STORE_FAST               6 (idx_cond)\n",
      "\n",
      "316          60 LOAD_FAST                0 (self)\n",
      "             62 LOAD_FAST                6 (idx_cond)\n",
      "             64 CALL_FUNCTION            1\n",
      "             66 UNPACK_SEQUENCE          2\n",
      "             68 STORE_FAST               7 (logits)\n",
      "             70 STORE_FAST               5 (_)\n",
      "\n",
      "318          72 LOAD_FAST                7 (logits)\n",
      "             74 LOAD_CONST               2 (None)\n",
      "             76 LOAD_CONST               2 (None)\n",
      "             78 BUILD_SLICE              2\n",
      "             80 LOAD_CONST               3 (-1)\n",
      "             82 LOAD_CONST               2 (None)\n",
      "             84 LOAD_CONST               2 (None)\n",
      "             86 BUILD_SLICE              2\n",
      "             88 BUILD_TUPLE              3\n",
      "             90 BINARY_SUBSCR\n",
      "             92 LOAD_FAST                3 (temperature)\n",
      "             94 BINARY_TRUE_DIVIDE\n",
      "             96 STORE_FAST               7 (logits)\n",
      "\n",
      "320          98 LOAD_FAST                4 (top_k)\n",
      "            100 LOAD_CONST               2 (None)\n",
      "            102 IS_OP                    1\n",
      "            104 POP_JUMP_IF_FALSE       83 (to 166)\n",
      "\n",
      "321         106 LOAD_GLOBAL              4 (torch)\n",
      "            108 LOAD_METHOD              5 (topk)\n",
      "            110 LOAD_FAST                7 (logits)\n",
      "            112 LOAD_GLOBAL              6 (min)\n",
      "            114 LOAD_FAST                4 (top_k)\n",
      "            116 LOAD_FAST                7 (logits)\n",
      "            118 LOAD_METHOD              1 (size)\n",
      "            120 LOAD_CONST               3 (-1)\n",
      "            122 CALL_METHOD              1\n",
      "            124 CALL_FUNCTION            2\n",
      "            126 CALL_METHOD              2\n",
      "            128 UNPACK_SEQUENCE          2\n",
      "            130 STORE_FAST               8 (v)\n",
      "            132 STORE_FAST               5 (_)\n",
      "\n",
      "322         134 LOAD_GLOBAL              7 (float)\n",
      "            136 LOAD_CONST               4 ('Inf')\n",
      "            138 CALL_FUNCTION            1\n",
      "            140 UNARY_NEGATIVE\n",
      "            142 LOAD_FAST                7 (logits)\n",
      "            144 LOAD_FAST                7 (logits)\n",
      "            146 LOAD_FAST                8 (v)\n",
      "            148 LOAD_CONST               2 (None)\n",
      "            150 LOAD_CONST               2 (None)\n",
      "            152 BUILD_SLICE              2\n",
      "            154 LOAD_CONST               3 (-1)\n",
      "            156 BUILD_LIST               1\n",
      "            158 BUILD_TUPLE              2\n",
      "            160 BINARY_SUBSCR\n",
      "            162 COMPARE_OP               0 (<)\n",
      "            164 STORE_SUBSCR\n",
      "\n",
      "324     >>  166 LOAD_GLOBAL              8 (F)\n",
      "            168 LOAD_ATTR                9 (softmax)\n",
      "            170 LOAD_FAST                7 (logits)\n",
      "            172 LOAD_CONST               3 (-1)\n",
      "            174 LOAD_CONST               5 (('dim',))\n",
      "            176 CALL_FUNCTION_KW         2\n",
      "            178 STORE_FAST               9 (probs)\n",
      "\n",
      "326         180 LOAD_GLOBAL              4 (torch)\n",
      "            182 LOAD_ATTR               10 (multinomial)\n",
      "            184 LOAD_FAST                9 (probs)\n",
      "            186 LOAD_CONST               1 (1)\n",
      "            188 LOAD_CONST               6 (('num_samples',))\n",
      "            190 CALL_FUNCTION_KW         2\n",
      "            192 STORE_FAST              10 (idx_next)\n",
      "\n",
      "328         194 LOAD_GLOBAL              4 (torch)\n",
      "            196 LOAD_ATTR               11 (cat)\n",
      "            198 LOAD_FAST                1 (idx)\n",
      "            200 LOAD_FAST               10 (idx_next)\n",
      "            202 BUILD_TUPLE              2\n",
      "            204 LOAD_CONST               1 (1)\n",
      "            206 LOAD_CONST               5 (('dim',))\n",
      "            208 CALL_FUNCTION_KW         2\n",
      "            210 STORE_FAST               1 (idx)\n",
      "            212 JUMP_ABSOLUTE            4 (to 8)\n",
      "\n",
      "330     >>  214 LOAD_FAST                1 (idx)\n",
      "            216 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <lambda> at 0x000002159E1324A0, file \"Code Analysis/gpt2sample.py\", line 389>:\n",
      "389           0 LOAD_CONST               1 (<code object <listcomp> at 0x000002159E1323F0, file \"Code Analysis/gpt2sample.py\", line 389>)\n",
      "              2 LOAD_CONST               2 ('<lambda>.<locals>.<listcomp>')\n",
      "              4 MAKE_FUNCTION            0\n",
      "              6 LOAD_FAST                0 (s)\n",
      "              8 GET_ITER\n",
      "             10 CALL_FUNCTION            1\n",
      "             12 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E1323F0, file \"Code Analysis/gpt2sample.py\", line 389>:\n",
      "389           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 6 (to 18)\n",
      "              6 STORE_FAST               1 (c)\n",
      "              8 LOAD_GLOBAL              0 (stoi)\n",
      "             10 LOAD_FAST                1 (c)\n",
      "             12 BINARY_SUBSCR\n",
      "             14 LIST_APPEND              2\n",
      "             16 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   18 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <lambda> at 0x000002159E132600, file \"Code Analysis/gpt2sample.py\", line 390>:\n",
      "390           0 LOAD_CONST               1 ('')\n",
      "              2 LOAD_METHOD              0 (join)\n",
      "              4 LOAD_CONST               2 (<code object <listcomp> at 0x000002159E132550, file \"Code Analysis/gpt2sample.py\", line 390>)\n",
      "              6 LOAD_CONST               3 ('<lambda>.<locals>.<listcomp>')\n",
      "              8 MAKE_FUNCTION            0\n",
      "             10 LOAD_FAST                0 (l)\n",
      "             12 GET_ITER\n",
      "             14 CALL_FUNCTION            1\n",
      "             16 CALL_METHOD              1\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <listcomp> at 0x000002159E132550, file \"Code Analysis/gpt2sample.py\", line 390>:\n",
      "390           0 BUILD_LIST               0\n",
      "              2 LOAD_FAST                0 (.0)\n",
      "        >>    4 FOR_ITER                 6 (to 18)\n",
      "              6 STORE_FAST               1 (i)\n",
      "              8 LOAD_GLOBAL              0 (itos)\n",
      "             10 LOAD_FAST                1 (i)\n",
      "             12 BINARY_SUBSCR\n",
      "             14 LIST_APPEND              2\n",
      "             16 JUMP_ABSOLUTE            2 (to 4)\n",
      "        >>   18 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <lambda> at 0x000002159E1326B0, file \"Code Analysis/gpt2sample.py\", line 395>:\n",
      "395           0 LOAD_GLOBAL              0 (enc)\n",
      "              2 LOAD_ATTR                1 (encode)\n",
      "              4 LOAD_FAST                0 (s)\n",
      "              6 LOAD_CONST               1 ('<|endoftext|>')\n",
      "              8 BUILD_SET                1\n",
      "             10 LOAD_CONST               2 (('allowed_special',))\n",
      "             12 CALL_FUNCTION_KW         2\n",
      "             14 RETURN_VALUE\n",
      "\n",
      "Disassembly of <code object <lambda> at 0x000002159E132760, file \"Code Analysis/gpt2sample.py\", line 396>:\n",
      "396           0 LOAD_GLOBAL              0 (enc)\n",
      "              2 LOAD_METHOD              1 (decode)\n",
      "              4 LOAD_FAST                0 (l)\n",
      "              6 CALL_METHOD              1\n",
      "              8 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import dis\n",
    "import marshal\n",
    "\n",
    "with open('Code Analysis/__pycache__/gpt2sample.cpython-310.pyc', 'rb') as f:\n",
    "\n",
    "    f.read(16)  # skip the .pyc header (magic number, timestamp, etc.)\n",
    "    code = marshal.load(f)\n",
    "\n",
    "dis.dis(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f505ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD_CONST          : 105\n",
      "LOAD_NAME           : 94\n",
      "STORE_NAME          : 56\n",
      "POP_TOP             : 33\n",
      "LOAD_ATTR           : 25\n",
      "CALL_FUNCTION       : 21\n",
      "LOAD_METHOD         : 21\n",
      "CALL_METHOD         : 21\n",
      "POP_JUMP_IF_FALSE   : 13\n",
      "EXTENDED_ARG        : 12\n",
      "BINARY_SUBSCR       : 11\n",
      "MAKE_FUNCTION       : 10\n",
      "DUP_TOP             : 10\n",
      "JUMP_FORWARD        : 8\n",
      "IMPORT_NAME         : 7\n",
      "LOAD_BUILD_CLASS    : 6\n",
      "CALL_FUNCTION_KW    : 6\n",
      "POP_BLOCK           : 5\n",
      "SETUP_WITH          : 4\n",
      "WITH_EXCEPT_START   : 4\n",
      "POP_JUMP_IF_TRUE    : 4\n",
      "RERAISE             : 4\n",
      "POP_EXCEPT          : 4\n",
      "IMPORT_FROM         : 3\n",
      "CONTAINS_OP         : 3\n",
      "COMPARE_OP          : 3\n",
      "RETURN_VALUE        : 3\n",
      "STORE_ATTR          : 2\n",
      "GET_ITER            : 2\n",
      "FOR_ITER            : 2\n",
      "BUILD_SLICE         : 2\n",
      "JUMP_ABSOLUTE       : 2\n",
      "BUILD_CONST_KEY_MAP : 1\n",
      "BUILD_MAP           : 1\n",
      "DICT_MERGE          : 1\n",
      "CALL_FUNCTION_EX    : 1\n",
      "UNPACK_SEQUENCE     : 1\n",
      "STORE_SUBSCR        : 1\n",
      "FORMAT_VALUE        : 1\n",
      "BUILD_STRING        : 1\n",
      "ROT_TWO             : 1\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# Count instructions\n",
    "counter = collections.Counter()\n",
    "for instr in dis.get_instructions(code):\n",
    "    counter[instr.opname] += 1\n",
    "\n",
    "# Print results\n",
    "for opname, count in counter.most_common():\n",
    "    print(f\"{opname:20s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "855d698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"snakeviz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55d67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15360\\anaconda3\\envs\\nanoGPT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cProfile\n",
    "from Code_Analysis import gpt2sample\n",
    "\n",
    "cProfile.run('gpt2sample.main()', filename='profile_output.prof')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250b07ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 19 21:39:35 2025    profile_output.prof\n",
      "\n",
      "         5139763 function calls (4839702 primitive calls) in 18.753 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 8160 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   3695/1    0.144    0.000   18.757   18.757 {built-in method builtins.exec}\n",
      "        1    0.047    0.047   18.757   18.757 gpt2sample.py:331(main)\n",
      "      457    0.012    0.000   12.949    0.028 __init__.py:1(<module>)\n",
      "        6    0.000    0.000    9.478    1.580 _contextlib.py:113(decorate_context)\n",
      "        5    0.290    0.058    9.458    1.892 gpt2sample.py:305(generate)\n",
      "75000/500    0.091    0.000    8.868    0.018 module.py:1732(_wrapped_call_impl)\n",
      "75000/500    0.148    0.000    8.866    0.018 module.py:1740(_call_impl)\n",
      "      500    3.362    0.007    8.860    0.018 gpt2sample.py:170(forward)\n",
      "        1    0.002    0.002    7.903    7.903 gpt2sample.py:206(from_pretrained)\n",
      "   4509/6    0.025    0.000    5.879    0.980 <frozen importlib._bootstrap>:1022(_find_and_load)\n",
      "   1800/6    0.009    0.000    5.878    0.980 <frozen importlib._bootstrap>:987(_find_and_load_unlocked)\n",
      "   1739/6    0.009    0.000    5.876    0.979 <frozen importlib._bootstrap>:664(_load_unlocked)\n",
      "   1715/6    0.006    0.000    5.875    0.979 <frozen importlib._bootstrap_external>:877(exec_module)\n",
      "   2345/6    0.002    0.000    5.869    0.978 <frozen importlib._bootstrap>:233(_call_with_frames_removed)\n",
      "  1903/20    0.005    0.000    5.761    0.288 <frozen importlib._bootstrap>:1053(_handle_fromlist)\n",
      "45722/7562    0.018    0.000    5.480    0.001 {built-in method builtins.hasattr}\n",
      "  2734/27    0.002    0.000    5.478    0.203 __init__.py:108(import_module)\n",
      "  2736/27    0.002    0.000    5.478    0.203 <frozen importlib._bootstrap>:1038(_gcd_import)\n",
      "     6000    0.273    0.000    5.270    0.001 gpt2sample.py:103(forward)\n",
      "     28/1    0.000    0.000    4.735    4.735 import_utils.py:1938(__getattr__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1bb72f0ae60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "p = pstats.Stats('profile_output.prof')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac54ab",
   "metadata": {},
   "source": [
    "from cprofile output with pstats, definately I found function main, generate, forward, from_pretrained, cost most of the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c18ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# Step 2: Launch Snakeviz using subprocess\n",
    "subprocess.run([\"snakeviz\", \"profile_output.prof\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738636eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABpYAAALECAYAAAACbQN0AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAKEASURBVHhe7N1tbFzXfe/7H5+kIWmKQ4mySJl6oCWromwnppwCkZq+CI1eoBR8gEjoBSIiQVqqBXqlUyClzjVOrfbFuXQP3Ipp0YoN0IjHBwmkAg3EAtcQ86IQ0zYXUoDaoh3bGkWWRD3QIilR5NAUyZH4MHet/UAOhzPkkOLDkPp+Blt77bUf1tprryHi+WetnTH++EFUAAAAAAAAAAAAwCwyvTUAAAAAAAAAAAAwIwJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBKCCwBAAAAAAAAAAAgJQSWAAAAAAAAAAAAkJKMrjsfRb00AAAAAAAAAAAAkFRG1PDSAAAAAAAAAAAAQFJMhQcAAAAAAAAAAICUEFgCAAAAAAAAAABASggsAQAAAAAAAAAAICUElgAAAAAAAAAAAJASAksAAAAAAAAAAABICYElAAAAAAAAAAAApITAEgAAAAAAAAAAAFJCYAkAAAAAAAAAAAApIbAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBKCCwBAAAAAAAAAAAgJQSWAAAAAAAAAAAAkBICSwAAAAAAAAAAAEgJgSUAAAAAAAAAAACkhMASAAAAAAAAAAAAUpIRNbw0FsWI+u/d1t3ufj0ObNLrFWVePgAAAAAAAAAAwMqS1oGlkd4O3e7q1UBkROMxtczMzNGaos3atWFEd+5268m615R28ZqH1/XRrX6NeZuOvKUOLHUo9GG3hrytWFmF5Xpt53pvS+q9/pHa+6fU1sjTptcrlLjGY+ppD+lu72ON282cfJXu2K3N+c5OAAAAAAAAAACwCqVpYCkmaJEZUPHWHSrbEFDWWEThnm513e/V4BMnnOHICe7QV3YEva30MvJFSJ92DbnBlyUPLFljGrhzVdcfRNw6KEuF21/Tzg3OxhRjfTf02c2wRkw6p3CbXt5ZbI5OplfXP25X/6i3aeQU7dRXXiz0tgAAAAAAAAAAwGqTlu9YinRc80bCZKlw625ts0EluyMroOCmbdr96i5typus+nh0MsiUbnICOcrw0ssjSwUvFGiNt2UfeUaSp54VyHY7RGa+istnCipZhQrm53hpI2OtCjYQVAIAAAAAAAAAYDVLw8DSA93p8Ub4BNarbEOi8Ea+ynZtVoG3a2wk0WRvmJBVpLyJyNKIIo/ip7xzjT0c0mObyMnV+pmjSkaWine+rJd37dKulyr0lb2vqJy4EgAAAAAAAAAAq1r6BZYe9mvIi3uszStSwE1Ol7VJW4vz3BuIjk99lxHiFChv7eSjfjJ430tN1TMUcdZr89cnb/cpshQoKFDBujzFjF0CAAAAAAAAAACrVPoFloZHJoJEGdl5XiqxQNkmrX+uQMHCPOe9QEhuU95kqGj88aB6vfSkXg0M23FiWQqsK3CzAAAAAAAAAAAAYqRfYCkrY6JST4Z7vFQy67XtN3ZpxwvFKY6weYZtyNNaL6nRiAaGvbRveECRUbPODCh/g5sFAAAAAAAAAAAQK/0CS8E8+a8DGh/o1vWHTHK3IHILFMj20hrRcNhLeiberxR4Ts87ObN4MqSBnk7dCP1KH374oT66Pn0MFAAAAAAAAAAAWF0yooaXThNj6r72iToG/IBSptYWlmhbeakKsrysGUXU88V9DY1H9eTRgB4NP9ZY7ia9viNPd9rvqXfQbPt3nJnlvMdpU9k2bcz38qYZUW/HbXX3PVJkZEzmshMyzfk5gQJtKNuu0mSVe3hdH93qd6f3yzP1qChzsqfkT5Glwu2vaWeiUUNjA+q83aGHXw7p8cR8gZnKygnouaJN2la2fsZ3HXVebdO9QTvdnSmlcJte21nspC1/39ril/XKtiTjv/w6mzIzM8x2dHyiPbIKy8311rsbSXXrWluHBtwqzC67UOVf3anZrgoAAAAAAAAAAJZGGgaWjLEB3bt+Q52PYsIuGTnKW1+qsi0bZwkwDajjeocGIo815EVfsgqCWjsY1tC4SedtVNkLz2msp1P3+iJyYhxZBSp7dZc2xV93sFvXbpprPTHpzLUq3FSmsk1BBbLGFAn36Isv7ikcsVfIUeH2l7VzQ4KKJQssOXp0/ePb6vemoCveukNlGwJKeHuD93T1804N2gutKdDm7TtUmjeicHeHOrr79dhWIztfpTt3a3OSINnY3c/00f2Iu7FmvXa9Wi73bUq9ph7tph5rtf6lV1S+zsmcVeT2p/qsxxnnlGJgqUOhD7s1ZJOZedq4c5e2xj7MMdMen3rtYaxdv1OvlBe6GwAAAAAAAAAAYNml31R4VlaBNv/Gq9r1QnBy+rboiIYe3tG1jz/SZzfvqd8GexIqUNnOClWUPjcRoBkbsEGlHBWU7dZrFVtVvG69Nr34srYVekeMDajnCy/gMiGiO7e8oJJppvxNFdq52QaV7HaWAsFN2vHieu+9RSPqv3dHA046dWPdvXrkBJXytGmXqU+yoJINuNzwgkrZhdq2Z5c7QioroODmnarYWuiOVBodVOeNG0o2KV1WYcx7lp5E1O8l9WW/936lNcpLMahkBbIS1ja5vsempSzTniVxQSU7Uu1Gx0RQSWuLVU5QCQAAAAAAAACAtJKegSVHlgpKdujlr35FO0sLvYCOER1TpK9T1z/9SFe/GEgwlVwi7oiiXZumDuVZXzQZfHoy1OelfEN64gc5NK7oeIIgSux7i2IDNakY7NC1e7b+pm5bd6ks6VR8Um97h/qdiIxtk3IVx1Ula0O5Nj3nPcqRsDrvxAfJPOvytGbiiUc01O2mxvrd9ytlmvvZ5GYtjidj7vNas06lpVNvYqzzc93zpz/MDGhj+TbN0CQAAAAAAAAAAGAZpHFgyZejws079fJrr2nXlmLlTYxgGtNg1zV9dq1bg15WUnnrE09TF2PcXG+q9SrdWKi8vDzlrStWcZGXvSD61X6zW0PjmcrbtGPmuo3d0/2JgEueCqfN12dladO63ImHGRnsSRJwM+038fqkcT0ZcsdY3R90h38FnnveWS+ajBytWZOltflFmjIWabhD17oG3WkJzV3kbdyurUSVAAAAAAAAAABIOysgsOTLUsHz21Tx1a+ofENgouIjAx26cSPZ5G9PJ99OM1dRoYqXtmmjF+gYiwyot7tDd25d19VQp/PeprkZU8/12+p9Yho/f5N2zjRUyep5pGG/jEBe8hFFa7KV4SXt6Kn48VeuLBUGJibD0+OIffdTtwacAtYqsG7m4NtTe75cL7/6ml55MfZdTBF13How0Y6ZeRu1dbY2AQAAAAAAAAAAy2IFBZZ8OVrvTGuXNxlcCnfqzrC3sQhGejt149ef6qO2D/XRZ9fU/sUDhQdHlZ2b5J1IM4h0XNNdd147jQ/26M5DJ5nU2JMRbySPlJUzMdxoZqMjSjIZngrWxdQ5MqSeL4f0xBaQHVDhHN6vtFAGO9r1YCKqlK+SXWVMgQcAAAAAAAAAQJpagYElV37ZLm3K96sf0Zf3k4VSnsJYWLevtOlX7fcUfvRYY5l5Ki7bqa/srdRXXt6tnduLtHYuLTg2oPaecW3Yul7uuKERhb+4oZnGW/U/doNQ1lj/bX344YeJl1t29FEKNuQr4Nd5PKL+7sGJ9yvFjiNaEoMduvNgyAucZalg80uKe/USAAAAAAAAAABII2kXWIo8vKcbn1/V7VlG8thAxObnJkfwjI4MeamF0qsbn91QjzcPXU5BmXZ/tULbNhUqx8mZh8cRqXi7tm7cqpJCL4IyEta99n43nUDh2snSsgq36fXXX09hqVCZd850RcqduOSIBh/ZwFWmcguSTrK3SAZ15/bkFHhZhWXakfD9UQAAAAAAAAAAIF2kXWBp5Mtehb8cVO/Dbi9neUTudCrsDxbKLlTZrk1PP0Xb2vUqd94flKXicnM9r/Uf997V7UE3HS8rO2viIY2NLMSorIAKJyNLGh+3kZ0c5Qbd7aUyePeWHvovj8ox7VteHDetYEQ9X3SqJ5LSOCwAAAAAAAAAALAE0i6wVBDIdio1Ptine7PEFCJjkwdk5+R5qYXRMzgZxMnKX78w08RlZWlijFVWqbYUB7wH8Fi9t+8oYWwpmKc1XlJPIurxksmMRSKanDwvseBzfrmeNflan+ull8LgbbXfj3hT4OWo8IVyFccPVnrYoY6ue+rqnu1uAAAAAAAAAADAUkm7wJLy17pTzY0P6n57zwzvDRpT77AfdAho3fOT0+Itn351372jO93Jp7aLlb+lTEEvajQ+/FB3OhKElnJfUJE/tGn0kXq7Z2iR8G1dDX2mzz5/4OUkUZw3GeAysnILVeClF1+/2m/2OO91snKCZdq5IT6qNGjacMA84UytCaTDcwUAAAAAAAAAAFb6BZbWBZTt1Wqs/66u3g4nDC6NPWzX/UFvzEuwVFsXeMRNQcy7jcYGe9XrpSeNaaCrR0Oj3qY11q+e+w/0sC/V9z0VauvmQm8KuHENPbijjmFnI0aWNm/ZqDynTUyZ964pUfxp5MENhdp7FFGeiss2erlJZBUqMDEMKlN5BQsyHislvTduq/eJt7FmvbbtmF72YMcdb5q8HK1d5+YBAAAAAAAAAIDll36BJT2ntdleUuOK9NzQrz69rnv97uiksUhY3bdC+uR2vxNwysov1Y4EwYmRkVFFvbSi4wmDUzMdU7htswr92NJovzquP3CmmBuLDKin47o++/gjXbs3oqy17iHKkCLtYdkJ9NbkFbl5xkhkZMZ6ZG3YpvX+oJzxIT241eFcY4r8Mu3aul5r7dMyx3T/+iOFbnWr98sBDfR06savf6VP74T1OJqj4NZdKps1yFagglx/lFBAecVeck7GNDAaezfe+5Jm8vCGOiZeXLVWxS+Wq9DbcjzpV/fNz3Ste8i9WnZA+Us5RR8AAAAAAAAAAJhRRtTw0mmj89cfqftJQOtf2Kqi8R590dmrwSeTQYzMzCzlBJ5T8PkylW4IeCN+fB0KfditRGOGsgrL9dpOG4RK5RhjbECdt+/oQX9EI7Fxk8xM5axdp41btuv5J7f02e2wRmwrZpj83A3aumurgllPV4+8Ta+roszb8D3p172Oe+odiOjx6GSFMjNztOa5oDZteUHFgfhp5ZLovKq2e4MaX1usl1/ZNmVqvKQeXtdHt9yA3mwS1b/3+kdq70/lbE/uRr22Z2vc8wUAAAAAAAAAAMslLQNLAAAAAAAAAAAASD9pOBUeAAAAAAAAAAAA0hGBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBKCCwBAAAAAAAAAAAgJQSWAAAAAAAAAAAAkBICSwAAAAAAAAAAAEgJgSUAAAAAAAAAAACkhMASAAAAAAAAAAAAUkJgCQAAAAAAAAAAACkhsAQAAAAAAAAAAICUEFgCAAAAAAAAAABASggsAQAAAAAAAAAAICUElgAAAAAAAAAAAJASAksAAAAAAAAAAABICYElAAAAAAAAAAAApITAEgAAAAAAAAAAAFJCYAkAAAAAAAAAAAApIbAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKQkI2p46VVpfHxcg4ODzjI8PKzHjx9rZGREo6Ojzj48GzIzM5Wdna2cnBytXbtWubm5ys/Pdxa7DwAAAAAAAAAAzG5VBpbGxsbU19fnLF9++aWXCyS2bt06FRUVOUtWVpaXCwAAAAAAAAAA4q2qwNLQ0JAePHignp4eL8flj0yxo1QCgYDWrFnjjF5hpMqzw45Os6PUnjx5okgk4oxe80eyxSouLtbGjRuVl5fn5QAAAAAAAAAAAN+qCCzZQEFnZ6d6e3u9nMlRKMFg0AkiAYnYYFM4HJ42um39+vUqLS11ApEAAAAAAAAAAMC14gNL9+7dc4JKvueff94ZcUJAAHNlA5R2xNv9+/e9HDnBpc2bN3tbAAAAAAAAAAA821ZsYMlOe3fnzp2JqcxsMKmkpMSZ5g54Gna6vK6uLifIZNlpFLdu3cr0eAAAAAAAAACAZ96KDCw9fPhQt27dctL2vUlbtmxRQUGBsw0slIGBAd29e9d5H5O1fft2bdiwwUkDAAAAAAAAAPAsWnGBJTuS5IsvvnDSdpSSHUkCLCY7Ms4fvfTCCy84I+MAAAAAAAAAAHgWrajAUuz7lMrKyrRp0yYnDSy27u5udXR0OGneuwQAAAAAAAAAeFatmMBS7Eilbdu2qbi42EkDS6Wnp0e3b9920oxcAgAAAAAAAAA8izK9dVqz71QiqITlZvud7X+W7Y+2XwIAAAAAAAAA8CxJ+8DS0NCQbt265aTt9HcElbCcbP+z/dCy/dL2TwAAAAAAAAAAnhVpH1i6c+eOs964cSPvVEJasP3Q9kfL758AAAAAAAAAADwL0jqwdO/ePQ0ODio3N1dbt271coHlZ/uj7Ze2f9p+CgAAAAAAAADAsyBtA0uRSESdnZ1OesuWLc4aSCd+v7T91PZXAAAAAAAAAABWu7QNLPlBJTvlWEFBgZMG0ontl/6UeH5/BQAAAAAAAABgNUvLwNLQ0JB6e3uddElJibMG0pHfP21/tf0WAAAAAAAAAIDVLC0DSw8ePHDWzz//vNasWeOkgXRk+6ftp5bfbwEAAAAAAAAAWK3SLrA0Njamnp4eJ+1PMwakM7+f2n5r+y8AAAAAAAAAAKtV2gWW+vr6nPW6desUCAScNJDObD+1/dXy+y8AAAAAAAAAAKtR2gaWioqKnDWwEvj9lcASAAAAAAAAAGA1y4gaXnrZjY+Pq62tzUl/9atfVXZ2tpNeFA+v66Nb/Uo4cVlGprJycrX+hR3auj7Hy3yG0VazGh0d1ccff+ykKysrlZmZlq8vAwAAAAAAAADgqaRVYGlgYEDXrl1Tfn6+du/e7eUuph5d//i2+lWgra/s0sYskzUWUbi7Qx3d/Xo8nqPC7S9r5wa741lHW83m6tWrGhwc1K5du1RQUODlAgAAAAAAAACweqTVsAr7o7xlA0tLw7/9TGX58ZCsgIKbd+qVrYXK0oj6u75QxNu1MoR1I3Rdvd7WwlkBbdURUqjDSy8Dv9/6/RgAAAAAAAAAgNUmrQJLw8PDzjo3N9dZL6sNm1S4xqwjg+pxc1aIYY2MesmlkiZtNTCScLK+JeP3W78fAwAAAAAAAACw2qRVYOnx48fOOhAIOOvlVaCcRXzF02IZu9ev4XFvY8mkQ1v16/4jt/8sF7/f+v0YAAAAAAAAAIDVJq0CSyMjI856zRo7/GW5DbgjfzKzlONmpLkxRR7e1tXuQS15XGm52+pJv+5du63wE297mfj91u/HAAAAAAAAAACsNhlRw0svu7a2No2Pj6uyslKZmUsR8+rV9Y/b1a9ClX91p9Z7udbYw+v65Fa/tG6rXntpo5frG9NA1y3duf+lIiNuGCdrTb7Wv7BDW9cnDq2M9N7RjS96NfjEm64tI1M5awPKGs9S4au7VObmxki9jN7rH6m9f4Zp4LKn39/czbetBtV985a6+yPybmPWthoL39Hnd2PaKjNHgcKN2rqtVAX++50cHQp92K0hbyuRvE2vq2J64y4K23dtH7Z91/ZhAAAAAAAAAABWm7QKLH344YfO+vXXX3fWiy9BsGQsonB3hzq6+/U4q1DbXt6p4inBjDH13PhMt7/MVOGmLdq2uVA5dsTMnbvq7h9RYNMuVZTle8d6Ht7Qr24Pas3zm7W9tFiBLDu66Au1dzzQ0OhaFe95RdumvFZqHmV4nCDT4HMLEEiKN5+2GlRH6Jq6I2sU3FymrZsKlRkJq7ujI+l92CDVZzZI9dwmbd5WquLAuIZ6OnXbtlVWUNv27Igrw+cGmbSEgaRElr4PAwAAAAAAAACwdNJqKrxlM9qv9g8/dIICH370mW50DSmjoFQ7pgVKjIft6giPq3BrhXbagI/NW1OozTsrtClfGur5Qg+cAyf19g1oJHe9dpfZoJLNyVJgw1ZVvLJNhdmjGokfcjOPMpbMHNqq98YNJ6i0cdfL2rHJvY+sQNC5jy2FWRp6cEcdw+6xjuEOXbvTr/HCbXr5N8pU7DRWjvKKTVu9tEl5o2Hdu9XrHgsAAAAAAAAAAJZcWgWW/Onv7JRiS8pOFff663r99Z0K2tfk5OSrbOdmBaeNjBnTvQcDGltbpLIN8TuztLkwV5ljg+rr9rI8mRnmvp5E1BM/W11WscrWr9X4lHcDza+MJZNqW43d0/0vR5RVUKqt0wZXZam4fL3yxofU29nv5Un9nb0aGs8z+4rNEXHyy7SpIEsjX97XvRlm/VtOfr9dmmkcAQAAAAAAAABYemn1C3h2drazHh0dddZLr1AvFAakJwPqTjgkqF9Dj8eVuSYgc9R0a7KVoXE9GRrwMlzBzRuVH+3X7Y/b9Ksr13W7s0cRLzgS2FKhXaVu2jW/MpbeLG3V80jD45kK5CeZkC+rUIE10shQWBEnI6Lw0Ii5v4AKp0WVXOvzA8ocH9ajHi8jzfj91u/HAAAAAAAAAACsNmkVWMrJcSZ905MnU4bwLKnAC0XKzxzTwP07XsAj1pBGRqXxgQ53Krj45Va/bLxodGzEPdyXW6rdr31F5c8XKGfskXru3dZnH3+kT399R73TbnWeZSyDGdtqZEzjylC2HdWUkGkLG38x9+HOBGju295Ydo7Zk4QXVEuDW0/I77d+PwYAAAAAAAAAYLVJq8DS2rVrnXUkMj2ks2SyNmvDc1mmEmF9MTlLmyfgBEMyC8r0ujMdXOLltZ2xo3RGNGRH4ihH68t2quLV1/TayztUtj4gDT9Q+2chdQy6R7rmU8YymamtcrJM54pqPOngs4jGbCApK0d5znaePUU2M+nTHx03V8y0p6Qlv9/6/RgAAAAAAAAAgNUmrQJLubm5znp4eNhZL5eNdmSRRvRlZ6czOmhSkTN92/iTSPLgxzQDunf7tnq9LSsrENSm7bv1yqvbVJg5pAdfxL4waT5lzGZMkcjiDPNJ2lbFeQrMNGXfWK+GTZWyAgXelH8BFQSypJFh9Tnb0/UPRswVA8or9jJSYdtx6kNcNH6/9fsxAAAAAAAAAACrTUbU8NLLbmBgQNeuXVN+fr52797t5S6mXl3/uF39KlT5V3dqcgxQv258cl3hJwFt3POytsbECcY6r+pX90YV3PmKygu9zAmD6gjd0fj2iphzbBm3NbJhlyrK8r28Sd3X2nRPm1W5a5OXM58yJg20f6pr4WxtrtytiVc3Dd7W1c4C7X6qUU5zbasxc2+fqGMoT2Wv7tKmuPcmDd7+VFd7pOLdr2ib3yymnp+6mXplItMz1q1rn3RoKK9Mr5q2iruc0amrbfc0GtylV8r9yfRsHW5oZMculU0/YcFdvXpVg4OD2rVrlwoKkk7oh2fJ//bWAAAAAAAAAJBOvuet5yGtAkvj4+Nqa2tz0l/96leVnW1fwrOYepygT78KtPWVXdoYE3yI3PlMnz2IKGvdVr380kZNzr42qHtXP1fncLYKNpao7Pli5a0ZUyTcoy/udevJuh1xAaSIbn/6mXoeZylvQ6lKS4oVtCNzxiIKd9/Rna6I8ra9rJ0bYiMfcy0jxsPr+ujWgNZufFE7txYqM9KjjpsPpG0VkwGceZlHW4316MaV2wqP52ljWble2BBQ1pMh9dxr193eJwo8Pz3YNtgR0rX7EeWs26QtWzer0Lnvbt2526kBBbVtzw4VJwwSRdQRCql7xNRvV7k2BsbVb8rpGCzU7pcSBaIW1ujoqD7++GMnXVlZqczMtBoMiOVSbpZbbhIAAAAAAAAA0sJ7ZlktgSXr888/15dffqlt27apuHguc57NkROA6Y+Zvi1Lhdtf084N3ubYPV39VacGx+1Gnja9XqEyZ4c1ot6OdnU+HFRk1DnAnL5WBRu3accLBdOCGB2hjxSOrlXG2GM9GRnTuNfimTkBrdu0XTs2JYr4zK2MSWMK372mOw+GNGLLyQqosHS7diYsI0VP01ZjA+q8fUcP+iMa8W9jTb7Wv7BDW9cnflnSSO8d3fiiV4NPvBIzcxQo3Kit20pVMFOEaLBb1291qt+b+y4nz5yza6uCix1VMnp6enT79m2tW7dOL730kpeLZx6BJQAAAAAAAADpZrUFlviBHivRkgVEsbIQWAIAAAAAAACQbp4ysJR283UVFRU5a/sjfSQScdJAOrP91PZXy++/AAAAAAAAAACsRmkXWMrKypoY8fHgwQNnDaQzv5/afmv7LwAAAAAAAAAAq1XaBZasjRs3Ouv79+/ryZMnThpIR7Z/2n5q+f0WAAAAAAAAAIDVKi0DS3l5eVq/fr2T7urqctZAOvL7p+2vtt8CAAAAAAAAALCapWVgySotLXXWdpqxgYEBJw2kE9sv/Wnw/P4KAAAAAAAAAMBqlraBpUAgMPFj/d27d501kE78fmn7qe2vAAAAAAAAAACsdmkbWLI2b96s/Px8DQ8P686dO14usPxsf7T90vZP208BAAAAAAAAAHgWpHVgydq6dauztlOOdXd3O2lgOdl+6E+B5/dPAAAAAAAAAACeBWkfWMrLy9P27duddEdHh3p6epw0sBxs/7P90LL90vZPAAAAAAAAAACeFWkfWLI2bNigF154wUnfvn2b4BKWhe13tv9Ztj/afgkAAAAAAAAAwLNkRQSWrJKSEpWWljpp++M+0+JhKdn+5geVbD+0/REAAAAAAAAAgGfNigksWZs3b54YuWSnI7tz546TBhaT7Wf+9He2/9l+CAAAACyvsLq8FADMV9h8AAAA5mpFBZYsO1LEf+fSgwcPdOXKFQ0MDDjbwEKy/cr2L9vPLNvvGKkEAACAdNCu/SpVg1nPVYtqlGE+8zl3NiE1meu2eluuhSuvy1yjwZSxOn8Gd1tpzyI8leT8Z9PibSfSbtrcHlMz41HzNf2Zzqe8RP0OqWkwf0n2m38BAADmZsUFliz7bpuKigrl5+dreHhY165dc0aVPHnyxDsCmD/bj2x/sv3K9i/bz2x/451KAAAAWDxdatYbKnV+VM9QkY6Y7Yi3L95ZHVe7jqpO5V5OOmjXIVPr4+YuFiME0WLu9rj5HFKTl4OVbmGe6eL2u9WuTqdMCx43f1OS/bUBAABIZEUGlqy8vDzt3r174r1LdlTJJ598ort37yoS4X8SYe5sv7H9x/Yjf5SS7V+2n9n+BgAAACyOsFpUoUO6or06o3M65fzUfijJSIIus79ZdTrqbaeLcp3QYVXpbVPv2TVoj/mkPlJiv2rNtatMGdVezsrjjsZZ2lFJ6Wxuz7TFCblODx7Nrd8hXpUToD5FwBYAAMzJig0s+ez7bl5++WWtX7/e2b5//74+++wzff755+rp6dHo6KiTDyRi+4ftJ7a/2H5j+49l+5PtV7xPCQAAAIvvhA4prCq16bwO66CO6rTOmVRbgpEEIZ3SJVWo1izp5rDO6ILqFfS2Z3LZ3MlcBE37XDCfw2l436kKmbvGpLk80/YZ+stc+h2mqzV/cS7JTkkIAACQqhUfWLICgYDKy8ud6cqKi4udvC+//FK3b9/Wxx9/rKtXrzojUWwA4dGjR85UZ+Pj485xeDbY522fu33+th/Y/mD7he0ftp/Y/mLZ/mP7ke1Ptl8BAAAAi61FjYrosOoU+z7PgzrubJ9Rs5vhaTHbJWZvKpPgRRRSg2qUq71JRslE1OZcsdn825ZgMqyIuswnnHBP2NkTy+bE503nXtO9Yp+TTlbGVInr4tbCzbNHuHfTbO58tuvF38NkW7TGtVa7yfHbaCZhU6pb+vTyY+vp3/Nk2fEiXonNpsQuLy+R2Z7fVF3mGPeqrbMem0xoomaJr5GsD7ht7T93X/L+Fcs9sy8m7Z7lS1RmfP/06528nSbb0t7b7P0xVvLn4N93ouslqrc9Y7Y29sX299h+Yt+dVao3zHe/ZcbzfeWqMn9T2s3RjKUDAACpyogaXnrVGBsbU19fn7P4AQMgmXXr1qmoqMhZsrKyvFxgAdjfe265SQAAAKtd+52p0GoU0mlnhEVI75icE2arU7VTQksyR2XorMmNmr2uLuc9TCU6pzM66OXFsz9yn9Fxc81W58fmEtWpQfU6LPt/nLI/Oh8wVz1lrtGoQ6b8WPt0RRdjxo/Y6ccOmO2TJr/Oy3PZqeyOm/VNs8cPcrn1rZiS55d30uTaKbf87XiHdd7UOvmUaHYauRdNiVOPaze1eFGV5l5qTb3tT+mx3FonHw3j1qVNl835x82TseEEX5Vp8wumfWx+7FiOElNSu9k39f+EFp52nFVrrnvaeU5uO0532DzbM07KbbvDpsZ2crgjMWGCxG0TUtO042ztTpm7ODrtnsM6pr3meccGDkrMdfeaMlvM1aNJW95v9zpzJyHTp1qmlFhizg2Zc/2xQu7zCMXcl89/7rFlJXqm0/P8a8ab7GfJ+12bybHPtmpKvYPm6u3m6pMjnELO9/KSSQVMrg0GWUdNHU4lbRlX4udgv0etpka2l4TMlfc43+7OmH7TZtpyr/nuv216X73pwVbYnLPf9OKp92q/7SHTi2LHY4VNK9WY+5o6MaB9oiFzX/YvwEmzt8Fcz7KT3dntSnNEMrYNu5w+X+vlAACAVe49s3zPTc7HqhixFM8GB+zIk5deekmVlZXatWuXXnjhBWd6s/z8fK1Zs0aZmavy1pGEfd72udvnb/uB7Q+2X9j+YfuJ7S8ElQAAADB//qiWmUbKtJv9l8z+LjXp4kTeFWdtw0Xx9jgBgvaYH63bnfP2ej9Ex5ocnWTHJx0zOTW6rE5FzXJSblAp1jHnHU7nNGyOiJp/L+htk3vJ5NlzF0+VGk2N3OBVhSmz0/s0mj3zddbcyxvmmudNW9r76TT3ZX+GP24+s4/YCJn22qNy80z6zNnD5hrVprVaTTu8aHIv6qC5ns3vdIJU9tk1xYURGkzdG8z+U6bFbWva9rRTGdr3ZLnHVpm9neZu3bu+6Gy5dz3VWVOTE+Zz2Vwhaupz2XnSNrgYO1bKvpNrv46Ye7NTydlnbEu8bLYiptZ7EtbPBpUO6rRzXfdoG3ic/saiZBpMebZFbVvYK1wx1wqacg6Ye5xa2kKzz6BTfiD1jNNu9nNxIoiUnBvUCZh7dZ9hnxPECZsWPRnTog3mHi6Zuznn9IA+p33OmaddbVp5Jv5zyNVR5/p+y1Y63yMbdrUqTJ3fNsc2mRwburK6TPodU+LRiaCSrWuV6cWd5l/36dtrdZpeFDG1m9rG9nnaoFKVuYvJ53HOPIsqp9+XmGueND3DXuGC6ZW21faavzD2L0OyUUx7nX4584g8AACAGHbEEgBgEWw3i/0ry8LCwsLCwrLKl/PRw+Y/raRgNOCsj0bPT+w7E63VqegVb7vPpOtUFz0zsd8/97y3HbucjFaYfRW6OZF33jn2XMwx0WinDnrllkerdDp6WcNT9k9d/PLqJ+o0ubwdLY+ri1teRfTklOPsMr1uUR02x8bn+eWdjN6cyIuadEWS6yZbbppj7XUOx7Stfx1F96kz5li7uPVL3K7+4tetLnpxSv7l6NtOfnX09JT8YfPc4uvQaY6xeUfjrmEX2x7l5lr+dqI28xe37Up0wfSR2PxE7XfUed5BXYw5zl0um/KkQPToRJ5f54pofcxx7jJT3/MXv90PRs9N23cmejCubv7zODzlOLskKivRM038nGeu60z9rjZ6Ycqxfp+ujOl77vnT6zzbUmeegcx3Lr7v+fdwJjo8ked+t9w6unULmvXk/tPmOomfqVvft73vq9/fEj3PmZZhc77921Pi3Ku93tT+6tZp7m3AwsLCwsLCsmKX98zyFBi2AwAAAABPwU4Q1qrz6nPGO1zRKZ3RAWd0gJ22rkbnVG7WLjtG4aQzgmg+2k1Z1tTRR3ZcgzsGodwpKTBtdFIiiY464E37tfJGLpRPG+9VMTESZHbxo8WCKvLW/nNz2WnSrDbvOVh2nJodI1KrfW5GDDuyrN0cncp7blxF5k5ipzyz97HXS/naTI+KmBKPJiix0tSjwuxtmhgbYye6s+qecoqzRP2lyusvrROlpZ8K06JTVZiPHeE3yU4+6I57a1T8W6CSsxNOhk0LxE9gaXujLUHmb8FkP6k3RwfN9hEd03FTlh3PZkfGuSKmF7UmeaZuT24zH8vtbzJ/c6ZOTTm7Uuevg/37YHWZvxvThUx/BQAASAWBJQAAAAB4ChWyU87575mpcKbFuql6ValctSY1uW9+7BRWM7FThdmpsM6rxNTkgPYow5xzRPaNP6mHNKwS70f4dlPn5WDf25RhPrnOD+Dux31LTPrqdH6K79Jep+ZTP8edYICdtm0hdTrTotnp+xKpdMIQEfW5m05gy0o01eLTKjFXtZarvywU+y6sk+Zb2qxj5o5sSPiIabXJ920l5j4H+y6n2Gfuftz3aXVNaZd9atJh058bzffSTp0Y+440vxe9E3MN//PilF7kHlli/s6kEj62E2TaGu41d1VkanXSlHzKXGNYp51gWry5/b0AAADPMgJLAAAAALCgAip3Xpd/WvUmNfMPwHYcgRVO8KPuZdkRDyUTIQE7IimZClXrjPN2lk5Tbo1adUJ7nJ+T7XtV5haemTmQtXiqdUrnzOeMGic+icMn6abEq3miT9MMT23+IglHnCxPaCCVEEd6C5rv63nTojfNc6w137om860p0hHN/g6qfebM2Kcd+6mbErwJmxx3tJH9Zl9O8KRKdHji3PhPk/lOT0r012JS2NyB/761PTpgtiqc94+571uzI6ySPbHKRemrAABgNSKwBAAAAADLpsILnUyffi7k5LjTdPncH4RnmrAqIDst20nddEJMl026SMd1bIYzJl3yAlD75E+/VrSkQYMKc68H4z4LP85mYZU7I4S6TEtVx9Q69pP6lHypcSdaSzZd4WXnGfpjifz6+eNrFlabqYW133xcdqzZymUDwkd12nxz+nTYbDXpxAzfG/c5tJvjYp927Cd2VFGzufJZs31RV8wTCavGfCsn+b3ITq0Xe4XJT6X3TXCPjJjnnPx5XlS9uXqrcy92dNJlnTHXdafnSyZkPgAAAKkjsAQAAAAAS6ZdrYqdZsv9WVpq0qkpoxDsG1zsT70HvWm1XO77VtpTDBKUmKNP6oKiuqz4kQiRaWMeQqp3RmhUmRL9cI47OV573M/rYXNc/VP9EO0GIFbDj9nV3vOpV4Oznon7zqanvesK0yfKzdNr1NmYfmSF1Wxq0WX21jr9xNrvBSYbTA+LF/9cZxbfX8Kmb9l7Dqpm4s1A/ui6+B5q30N21kvPT5H5WIvxFqBwXDvaezrgtFvsPUfMN7cl5kj3OXSZNmiedv5UEXNerfdepX3mvCa9bc5o1LGYd1MdcP4KtJirzdw7/P5m37GUrNQqU8Lso5Omsu9vq5wIKAMAAMyGwBIAAAAALImIzupFvaG9Ko/5STmgE3rb2XdADc5P8nYqriq9o6BqnZ+GJ5VrvzPqwR0pkrrpPy6fMCUcN+W45bWYWu13ftI+qVMxJZZPBDGOeMe2mDMrTE3d8Rqz8wMC9t0wkz/Tl2ifU8oZc9WQuWqXc+2VKKDDOm8WGzrZY1rJvRt7P21ma79pr8m73usFYBrNXdtj5hskqTftZluvxjyDRlOOvVabueZeHTKlBc0TPOEeaNjxa0dNnq3fG2ax9XPrtmdKYGM2Z821a5ywh3/+XpMXMXd+ZiKsZLkBkkvmWLcsW689zqg8P9Q1s1xTVytsSokN61R6o6KazB3bvmIDIfGhrvmwQZ9y00cPePW1AdeQms1Wi6mJfU+a66y5ozecfj850qjePPlKc/wh8xzsdJPuk3e/I/a77Dtuzg2ba/nvVao0Z9pn0mjy/XFnNuh02OS5vah54lq2/fabHP9ebX87aa4QNjVy3wXltkaTKaXIex9TIMH3fWbt5i9KxJTjjzwDAACYDYElAAAAAFgiQe9n8yJv7bI/NJ9XnVp1XC+q1PlpOaBqXdTpaT8RH1St2syRT/ejeoUuqFEtphy3vAPmigGdNCXWxQWMjps62Jwm79gDJtWgTlPj1AIF+1Tj3GuD3lBzTK1tUKvEbNsAR6n5vKFT3p6Vx77d6px5LvbnffduSk1b7TXPtNQ86cl7rjR3bQOFtt3tMXvMXc9v9NI+c95FJ0xxzJRjr2WDDMNOn2k3/07tNafM0bXmKbbqHad+tm4XzVE3zbNN1Wlzhh2j5J9vw4y15q7tJGuxDpueUW3u0S/L1uugOfec6QepKDE9zI4VajF3Vj8RdvEDKva+G5x++KK5emx/mi/7PWsyvf6yV1/3vUSHvLac7JNBL0A6dXJI26buczhunqX75O13pMH864ak2nTCfNPCetvUPjZEfNK0XNC04SGz3xU0OfY5VZr6HJq4lm2/UlNO7J3a2vr9ba/XGkdMmVU6OhEIm4uILpl62rGTsSFCAACAmWREDS8NAFhI9r/rbrlJAAAAV0T2xfsBBaf8QO2LyL5nxf7cXeKEYxJpVa7eUJOGZd8D87TCpkT7o3XQlDjT1VI9LhF7z/aup5+byv2uLH472Sec+Bk/XVvG89t2pvJ8czk2meTPcqqnu0e3XySq50LcQzKzPTu73/bURKX69VqYvux/L2Zrv1SPm5kdK1lr/tNleAUHdwEAwJy9Z5bvucn5ILAEAIuFwBIAAFgUJ/SiWlWnizrq5QDAfHRpv0pVpSuqjxutCAAAVjUCSwCQpggsAQCARWHfC1OkQzqvYWfaMACYjxbV6ID5izKsc/Me8QQAAFakpwws8Y4lAAAAAFhR7HthzumkSr1psABgPspVpTPmbwlBJQAAMDeMWAKAxcKIJQAAAAAAAADphqnwZjY+Pq7BwUFnGR4e1uPHjzUyMqLR0VFnH54NmZmZys7OVk5OjtauXavc3Fzl5+c7i90HLAoCSwAAAAAAAADSDYGl6cbGxtTX1+csX375pZcLJLZu3ToVFRU5S1ZWlpcLLAACSwAAAAAAAADSDYGlSUNDQ3rw4IF6enq8HJc/MsWOUgkEAlqzZo0zeoWRKs8OOzrNjlJ78uSJIpGIM3rNH8kWq7i4WBs3blReXp6XAzwFAksAAAAAAAAA0g2BJTmBgs7OTvX29no5k6NQgsGgE0QCErHBpnA4PG102/r161VaWuoEIoF5I7AEAAAAAAAAIN08ZWBpxQ/ZuXfvnj777LOJoNLzzz+vl19+WS+99JIz+oSgEmZi+4ftJ7a/2H5j+49l+5PtV7Z/AQAAAMBcRNSiE9qjUjWq3cubKmLyj+sN5SrD+eSaY2vM0WFv/2zCpoQ39OLE2W+Yq3V5+wAAAIDFtWIDS3bau6tXrzojlSw7fdmrr76qLVu2MMoE82L7je0/th/Z/mTZ/mX7me1vAAAAADCzRu13Aj0H9I5C6lLEy4/VpbMq1YtqUKfqdErnzOeoyk3uMfPvkYTnxLJBpXJTwhVV6Iw594zq1W6uVqq9pkwAAABgsa3IqfAePnyoW7fc+aXse5NsMKCgoMDZBhbKwMCA7t6967yPydq+fbs2bNjgpIGUMBUeAADAMyakVrOE1alzOqazOqmbqnP+Z+FUZ9Woah1V0Nu2bFDqmC7ptDm7ViVe7nTHlOsc26kLMUddUpHJ22vOvmDOBgAAAGbwrE2F19XVNRFUsqNK9uzZQ1AJi8L2K9u//NFLtt/Z/gcAAAAAiVWoSgfNp1p7vZzEDscFlaxqHXLWYbn/17bELqlJEef8qaGnfao3S6sakky+Fytijmt2Pi1qm3WEFAAAADDVigos2ffdfPHFF066rKxMW7duddLAYrL9zPY3y/Y/3rsEAAAAIDXtc3jz0SVddNal5pNcnxcImj4BfJUOmH9D5jrJQ0V2Gr1S551Mh1RrPge013m/UyOT6AEAACBlKyawZEeK+O9T2rZtmzZt2uSkgaVg+5vtd5bth4xcAgAAADC7PvNJxSU16KiaVamTOpwgaDSpyNkbNp94Ae+8TvNJ5qgOqMuUckXDTt2iZjmpGlWpwjsCAAAAmM2KCCzZdyr5I5Xsj/vFxcVOGlhKtt/5wSXbH22/BAAAAID5anVGJ2WYz34dV43O6aLqvH3JVKra/NuiprixUCEdUb2XTqZdbebfClNSxUTwKmhKPElYCQAAAHOQ9oGloaGhiXcq2enICCphOdn+50+LZ/ul7Z8AAAAAMB9FOmA+tap23rfUqBrV65K7K6mATpqPG5TaoxPmrBPmGrkmXeKEnGZSripTUkjHzaeFdysBAABgntI+sHTnzh1nvXHjRqa/Q1qw/dD2R8vvnwAAAAAwV5U67XzOq0/DZl2td7RfDWr39idWrjpz/EXVqlRNTiiqVK0mp157vSOSO6WQOa/ClGGDURk6pBOmNEJMAAAAmIu0Dizdu3dPg4ODys3N1datW71cYPnZ/mj7pe2ftp8CAAAAwNMIqNabku6EGtysGQS1T6d1wXmf0gWT2ueMRLps9gS0X+XuQQmVmKOvqM8c+7YOqlnv6EXlqnGWYBYAAAAwKW0DS5FIRJ2d7itHt2zZ4qyBdOL3S9tPbX8FAAAAgKdR6oSEIupzN+eoRc3m34Pa527OKKhK1euchnXFCWYd0zvuDgAAAGBWaRtY8oNKdsqxgoICJw2kE9sv/Snx/P4KAAAAADMLqUln1eVtTQqrRa1mXT5lxJGbO/X/yBaZNnlduxrUaHKP6piXk1jYfGIFVKGDKjGpYTcDAAAAmFVaBpaGhobU29vrpEtK7P/EBdKT3z9tf7X9FgAAAMCzKqKQmr1PizMtnQ34tE7kuaGkiBp1QjUq1QGzbjNHdJlPq8ndq0Nmb1CndNw50jphcg/oDbM0eTlSgzPZ3QFTRsg5t8U56rg587DqZxivZINPReZzxFyrXWFTVtiU+47J7VKV+QAAAACpyYgaXjpt3L59Wz09PXr++eeZBg9p7+7du7p//76Ki4u1bds2Lxcw7P/V9JabBAAAwGpnAzcvxgSF4p1XVNVOKqJmHVONmkxqUkCVOqMWbwSRq8EJGbWZvDM6p8NOnh3DVKVDJnfy7ArV6aJOKuhtJxZWo46a652dUm5q5wIAAGAVec8s33OT85F2gaWxsTF99NFHTvrll19WIBBw0kC6su9X+uyzz5z0a6+9pqysLCcNEFgCAADAzMLq8oI8AZUkDO1EnCMS7fPPDZh9QfPvXDzNuQAAAFjxnjKwlHZT4fX1ua8pXbduHUElrAi2n9r+avn9FwAAAABmF1SJ90k2XihZwGny3PkEhp7mXAAAADzr0m7E0ueff64vv/zSmVLMTi22aB5e10e3+jXmbU6RkamsnFytf2GHtq7P8TLRebVN9wbHva1EslS4/TXt3OBtPkPs1I12CkcbYHrppZe8XDzzGLEEAAAAAAAAIN2sphFL4+PjTlDJCgYXeX7nDTv12uvbVJht0tkF2vra63r9dbO89rJ2lBQoe3RQD9pDuv4wYejpGdStgeFx02PWqrB0h17228tZdmr9GrMrf5PKn8GgkuX3V9t/bT8GAAAAAAAAAGA1SqvA0uDgoLPOz89XdraN+Cw2//YzNfFanKyAgpt36pWthcrSiPq7vvBmvF4pwroRuq5eb2vBdPdrUHnatOsV7dwcVCDmNUKRjnsKP1mrYGmpabNnk+2vtt9afj8GAAAAAAAAAGC1SdvA0rLbsEmFa8w6MqgeN2eFGNbIqJdcQL0DI8rfvEtl0x7NA93pGVJGwUZtLfSynlEElgAAAAAAAAAAq11aBZaGh4eddW5urrNeXgXKWYpBUwts7F6/7Ix1C239zpe1a9P08UiDt7s1EM1T8ZZNz+xoJZ/fb/1+DAAAAAAAAADAapMRNbz0srt69aoz2uM3fuM39Nxzz3m5i6lX1z9uV78KVf7VnVrv5boG1P7JNfWOFqiscpc2ebnpa0yRhx26cadHkcxE97MIxjp19Vf3FCko12s7F720BMw9h/sUWVusYBrEIh89eqRf//rXzsil3bt3e7l4pl331gAAAAAAAACQTnZ663lIq8DSJ598oidPnujVV1/VmjV2HrrFljywNPbwuj651S+t26rXXtro5frGNNB1S3fuf6nIiDs8KGtNvta/sENb1+c42/FGeu/oxhe9Gnwy5mZkZCpnbUBZ41kqfHWXytzcGKmX0Xv9I7X3e9dNJHtxAk39N3+l6/1rtPkru1WadLjSoLpv3lJ3f0TebczaVmPhO/r8bkxbZeYoULhRW7eVqmCinA6FPuzWkEz7lZVq/H6HBp6Y7Kw8bXppqzI7bqjz0Yhp5yzlPf+SKqbP4bfgbN+1fdj2XduHAf1vbw0AAAAAAAAA6eR73noe0iqw1NbWpvHxcVVWViozcylm6UsQWBqLKNzdoY7ufj3OKtS2l3eqeErQZEw9Nz7T7S8zVbhpi7ZtLlTOk37du3NX3f0jCmzaNT2I8fCGfnV7UGue36ztpcUKZNnRRV+oveOBhkbXqnjPK9o2ZcTNPMrwOEGmwecWf8TScIdCV7s1tn63XtmWLGgzqI7QNXVH1ii4uUxbNxUqMxJWd0dH0vuwAb3PbEDvuU3avK1UxYFxDfV06rZtq6ygtu3ZMfk8zLEf3RqQ1gS0futObc19oNBnnXqckaOcYJl2bS9Qz68/1b1IwZKM4LJ91/Zh23dtHwZUbpZbbhIAAAAAAAAA0sJ7ZnmKwFJavWPJ/jBvLU1QKcZov9o//FAf2uWjz3Sja0gZBaXaMS2oZDxsV0d4XIVbK7TTBnxs3ppCbd5ZoU350lDPF3rgHDipt29AI7nrtbvMBpVsTpYCG7aq4pVtKswe1ciQc9ikeZSxtMbUfbdHQxkF2pQ0qGTu+8YNJ6i0cdfL2rHJvY+sQNC5jy2FWRp6cEcdsa8jGu7QtTv9Gi/cppd/o0zFTmPlKK/YtNVLm5Q3Gta9W73usTECG3Zpa6G5+pqAckzXiQaKtXv7enNmjtZmZZjnO6L4Jl4Mfr/1+zEAAAAAAAAAAKtNWgWWlo2dKu711/X66zsVtDPw5eSrbOdmBeODShrTvQcDGltbpLIN8TuztLkwV5ljg+rr9rI8mRmmmZ9E1BM/W11WscrWr9W4ncJtwvzKWFL9d/RgIOoEfOInCZwwdk/3vxxRVkGptk6LPWWpuHy98saH1NvZ7+WZy3b2amg8z+wrNkfEyS/TpoIsjXx5X/emtGOGstdOPTojOzD9fAAAAAAAAAAA8NTSKrC0/CM+CvVCYUB6MqDuhEOC+jX0eFyZawIyR023JlsZGteToQEvwxXcvFH50X7d/rhNv7pyXbc7exTxgiOBLRXaVeqmXfMrY+mMqbMzrMdrgtpclrCGrp5HGh7PVCA/yQR0WYUKrJFGhsKKOBkRhYdGnFFHhUmiQuvzA8ocH9ajHi8jzSzbiDsAAAAAAAAAAJZIWv0Cnp2d7axHR0ed9XIIvFCk/MwxDdy/4wU8Yg1pxFRtfKDDnTYvfrnVLxsvGh0bcQ/35ZZq92tfUfnzBcoZe6See7f12ccf6dNf31HvlNFK1jzLWCoP29U9mKGC57eq0MtKaGRM43Y0kR0BlpBpC/u4zX2409SZ+7Y3lp1j9iThBdWW69Zn4/dbvx8DAAAAAAAAALDapFVgKSfHeZuQnjyZFm1ZOlmbteG5LCkS1heTs7R5Ak4wJLOgTK87U+clXl7bGTtKZ0RDdiSOcrS+bKcqXn1Nr728Q2XrA9LwA7V/FlLHoHukaz5lLJVB3e7sd6bpe2FTgmFF/d3q9tssJ8t0rqjGk8YIIxqzgaSsHOU523n2FNnM6QE9z+i4uWKmPSUt+f3W78cAAAAAAAAAAKw2aRVYWrt2rbOORJKGFpbERjuySCP6srPTGR00qciZvm38SSR58GOaAd27fVu93paVFQhq0/bdeuXVbSrMHNKDL2JfmDSfMmYzZtr06Yf5jHV/ob7HWSos3aZpr00yZXR392jADyQV5ykw05R9Y70aNlXKChR4U/4FVBDIkkaG1edsT9c/GDFXDCiv2MtIM36/9fsxAAAAAAAAAACrTVoFlnJzc5318PCws142hcXKt8GdwV59MaUqWSoN5ivz8YA6p41msgbVEQrpTnz17Tubpg5LcmUVqyA3/hHMswxPTla2qfioHnvbjsEO3ep42ncy9evO/QFF859X+QYvK8aYnSJvKEcF/r6sUq0vyNLj/k51T43OOQY7ejU4vlZFpZMjr9aXFmnt+KAe3k7QVmPd6v5yRFkF61Wa5B1My83vt34/BgAAAAAAAABgtUmrwFJ+vjsOZnAwQWBhUYxPrJ1p2SYU6oVCO44mot6OB4od65NVukWb8kfVe/NTXevo0ZAz+9mYIuFu3bhyQwMFW7V1SlzBTvE2rqHuzxW61a1wxCtoLKLwvWvqfpSlgg1Th+DMvYxJBesCyhofVvhOv1PvsUiPbt8eUm5MAGc+Ih33FH6yVsHSzZoS13Hu47pCtry1eZq8kyxt2lGmYNaAOj4N6c7DiDv668mQem59pmsPR5S3qVzbYoc+5W9T+aY8jTy8pk+v31P/xH3f07UrHRrICqpsx6aJ8sfG3ec3Pu4/PHc7OjbilmV4R2jikEXk91u/HwMAAAAAAAAAsNpkRA0vvezGx8fV1tbmpL/61a8qOzvbSS+Kh9f10a3+iQCEDYQUbn9NO/0RN2P3dPVXnRp0IhN52vR6hcqcHdaIejva1flwUJFRLziVtVYFG7dpxwsFUwMvRkfoI4Wja5Ux9lhPRsY07rV4Zk5A6zZt145NiQIRcytj0pjCd6/pzoMhjdhysgIqLN2unQnLSFWvrn/crv6k70tyrV2/S6+UF3hbnrEBdd6+owf9EY34t7EmX+tf2KGt6xO/i2ik945ufNGrwSfe08nMUaBwo7ZuK1WBf+NTnp99dltNQ0/WMauwXK8FuvVh95CbMe0ZLqzR0VF9/PHHTrqyslKZmWkVs8VyKTfLLTcJAAAAAAAAAGnhPbN8z03OR1oFlqzPP/9cX375pbZt26bi4jR9mQ4Qp6fHjgy7rXXr1umll17ycvHMI7AEAAAAAAAAIN08ZWAp7YZVFBUVOeu+vj5nDawEfn/1+y8AAAAAAAAAAKtR2gaW7KilSCTipIF0Zvup7a8WgSUAAAAAAAAAwGqWdoGlrKysiSnwHjx44KyBdOb3U9tvbf8FAAAAAAAAAGC1SrvAkrVx40Znff/+fT158sRJA+nI9k/bTy2/3wIAAACLL6wuLwUA8xU2HwAAgLlKy8BSXl6e1q9f76S7uvjPJaQvv3/a/mr7LQAAALAU2rVfpWow67lqUY0yzGc+584mpCZz3VZvy7Vw5XWZazSYMlbnz+BuK+1ZhKeSnP9sWrztRNpNm9tjamY8ar6mP9P5lJeo3yE1DeYvyX7zLwAAwNykZWDJKi0tddZ2mrGBgQEnDaQT2y/9afD8/goAAADMT5ea9YZKnR/VM1SkI2Y72Ttnz+q42nVUdSr3ctJBuw6ZWh83d7EYIYgWc7fHzeeQmrwcrHQL80wXt9+tdnU6ZVrwuPmbwhuuAQDAXKRtYCkQCEz8WH/37l1nDaQTv1/afmr7KwAAADA/YbWoQod0RXt1Rud0yvmp/VCSkQRdZn+z6nTU204X5Tqhw6rS26bes2vQHvNJfaTEftWaa1eZMqq9nJXHHY2ztKOS0tncnmmLE3KdHjyaW79DvConQH2KgC0AAJiTtA0sWZs3b1Z+fr6Gh4d1584dLxdYfrY/2n5p+6ftpwAAAMD8ndAhhVWlNp3XYR3UUZ3WOZNqSzCSIKRTuqQK1Zol3RzWGV1QvYLe9kwumzuZi6BpnwvmczgN7ztVIXPXmDSXZ9o+Q3+ZS7/DdLXmL84l2SkJAQAAUpXWgSVr69atztpOOdbd3e2kgeVk+6E/BZ7fPwEAAID5alGjIjqsOpV4OdZBHXe2z6jZzfC0mO0SszeVSfAiCqlBNcrV3iSjZCJqc67YbP5tSzAZVkRd5hNOuCfs7Illc+LzpnOv6V6xz0knK2OqxHVxa+Hm2SPcu2k2dz7b9eLvYbItWuNaq93k+G00k7Ap1S19evmx9fTvebLseBGvxGZT4kxvHZ7t+U3VZY5xr9o667HJhCZqlvgayfqA29b+c/cl71+x3DP7YtLuWb5EZcb3T7/eydtpsi3tvc3eH2Mlfw7+fSe6XqJ62zNma2NfbH+P7Sf23VmlesN891tmPN9XrirzN6XdHM1YOgAAkLLoCtDT0xP94IMPnOXBgwdeLrD0bP/z+6Ltl8CMtpvF/pVlYWFhYWFhYfGWm9oXDagkWqs+L+9KtN78Z5l0OtoZd2xUh02+zLGTeZ2qMnmHo+di8uKXYV2OnjbHlTjXlVnXRc9o2Nt/PnrYyT9lrlHh7I9d9pn6xF7rvJNfET0Zk+cuJ6MVZl+FbsbkufWdmueXd9Lce+x2/HLYlOWfk2i5aeow/bibzj3Y9rgQrZt2zZPRKzHHTl/culSY9rpi2is45dwq04bRaJ9pO3ufk/n22fltOblMP84utRPPyW3H6cvhiWu4bXfYtP9pU8bU4xK1TaLj7LM+lfCe+3Q0Wj7t2MPRaid9ftrxk4vf7nWmvOqJPjV5jfPm2pPHu88j9r78xX/usWUleqbT8/xrxi+T/Sx5v6sw7XRlWr2DZp///XOXK+Z76e4LxPSDownaPX5J/Bzs92jYO+aKU/9gXL+5HH3bOfbt6OWJvL7ohQT3WqJzU9rYLn2mb7vPLnYp8e6r01yzLlo1kV9lvhuXE/x9iV1sG7p9PvF+FhYWFhYWllW3vGeWp5D2I5asDRs26IUXXnDSt2/fVk9Pj5MGlpLtd7b/WbY/2n4JAAAAuPxRLTONlGk3+y854wyadHEi74qzLp0yXsm1x5kirD1mLEK7c95eVbqbMSZHJ9nxScdMTo0uq1NRs5zUYcW/E/SY8w6ncxo2R0TNvxf0tsm9ZPLsuYunSo2mRhedO6swZXZ6n0azZ77Omnt5w1zzvGlLez+d5r7stGjHzWf2ERsh0157VG6eSZ85e9hco9q0VqtphxdN7kUdNNez+Z2qMzW2z64p5olYDabuDWb/KdPitjVte9qpDO17stxjq8zeTnO37l1fdLbcu57qrKnJCfO5bK4QNfW57Dzps6pV7Fgp+06u/Tpi7s1OJWefsS3xstmKmFrvSVi/RtN3Duq0c1336BPmHqe/sSiZBlOebVHbFvYKV8y1gqacA+Yep5a20Owz6NQZU3vrjNNu9nMxhVF7IdMae8zTPGGOt8+wT/WmRcOmRU/GtGiDuYdL5m7OOT2gz2mfc+ZpV8/y1ib/OeTqqHN9v2Urne/Rce+oClPnt82xTSbnkpfXZdLvmBKPOvVxhcxTesNcp8p7+vZanaYXRUztpraxfZ7HTdlV5i4mn8c58yyqnH5fYq550vQMe4ULplfaVttr/sLYvwzJRjHtdfrlzCPyAAAAJq2IwJJVUlKi0tJSJ21/3GdaPCwl29/8oJLth7Y/AgAAAC479dSLekO1Zr1HuToW84P9WR1Ro9y3l5SbI06pznzOqNrJmZkbDLI/Hfv6nB+Fy6f8pN6lQ044aY8pp8sJbtif0C/opCoTBKt89U5o4KBXRkBVJudtc92wuUrq4Ya5CyhoauXXq8hJ20/Qq8n87DOtdNG0aYVzFTtV4AkniNOiVmf/zOrMubYtbB0C5rx6k2NDdRWmDS+arRInv8SkTjjHXzRt7LOBpuNq01FT1tGJIEHAXO2MDpvUO84P+/bsEnO3Lv+e7V1PVWKeW8i5jr2PoFnXO1fpMvcxOU2Z+06uoCnxgnlublsGzLEXzFG2ZxwzZ/kipgfa+tm7Omf6n/+8K00pTc61U3PQnH3etIttC8u+5avR5MqUeSambovB9hi/f7i9J1HbJVZrWuWcuVf3GQZNH3fbJrZnuO/7qnZ6gMs+v5MmZ+Yy6s333b4b7aJ5xn672JY9Z8613yP//Wj2KdrvVqOpjW2pFtOOrebaZ8xxvibz1NpMnn2m7tN3e80Z87FtfMr7G+L3N/s8L0x5HjZsWOukJ9nv9VFznP0bcsWUZkNRB8zfigzTh/wgl6vClBoxf18AAABSs2ICS9bmzZsnRi51dHTozp07ThpYTLaf2f5m2f5n+yEAAADgC8mO/TivPme8wxWd0hnnx9u9ekOlqtE5lZu1y45ROGk+qf+gH6vd+3HZ/SnZZ8c1uD9g24BTqdk7dX9iiY464IW7Vt7IhXLFh9DsD+Wpih8tFvRCQMGJ5+aygQmrzXsOlh2nZgNCtdrnZsSwI8vazdGzj5ryFZk7mRrMqDBXmarN9KiIKfFoghIrTT0qzN6mibBBi+mZVt20oMPcJOovVV5/aY0LUqSTCtOiU1WYjw24TaoyH3fcmw3Npvq02sz3PGxaoHZa3yt3SpD5WzDZT+rN0UGzfUTHdNyUZcez2ZFxrojpRa1Jnqnbk9vMx3L7mw0u2uDnXJQ6fx3s3wery/zdmC60yCFCAACweqyowJJlR4ps377dST948EBXrlzRwMCAsw0sJNuvbP+y/cyy/Y6RSgAAAIhXITvlnD+6oUJ2WqybqleV7AilmzH75sdOYTUTO1WYnQrrvEpMTQ5ojzLMOUfUoJmm5UukxPsRvt3UeTk0OHXPMPdrfwB3P6mMOFpOnc5P8V3a69R86seOLLFHLGxrdprSbNBqj7sZp9IJQ0yOPbGBLSvRVItPq8Rc1Vqu/rJQap0xRtVq1jFzRzYkfMS0Wtjbm4z7HOxoxdhn7n4OOEd0TWmXfc4IsVbZ8Yt26kR3aj+X34veibmG/3lxSi9yjywxf2dSCR/bCTJtDfeauyoytbJjlk6ZawzrtBNMize3vxcAAOBZtuICS5Z9t01FRYXy8/M1PDysa9euOaNKnjx54h0BzJ/tR7Y/2X5l+5ftZ7a/8U4lAAAApCagctXppE6r3qRm/gHYjiOwwgl+1HWn6LLTfrmmToE3VYWqdcacYccvnVaNWnXCmZavSPa9KnMLz8wcyFo81Tqlc+ZzRo0Tn8Thk3RT4tU80adphqc2f5GEI06WJzSQSogjvQXN9/W8adGb5jnWmm9dk/nWFOmIZp8Ucp85M/Zpx37qpgRvwibHHW1kv9mXEzypEh2eODf+02S+05MS/bWYFDZ34L9vbY8OmK0Kc3dX5L5vzY6wSvbEKhelrwIAgNVoRQaWrLy8PO3evXvivUt2VMknn3yiu3fvKhLh/2eDubP9xvYf24/8UUq2f9l+ZvsbAAAAsPAqvNDJ9OnnQk6OO02Xz/1BeKYJq+x7WWp1UjedENNlky7ScR2b4YxJl7wA1D75068VLWnQoMLc68G4T7rPGFDujBDqMi1l39GT6JP6lHypcSdaSzZd4WXnGfpjifz6+eNrFlabqYW133xcdqzZymUDwkd12nxz+nTYbDXpxAzfG/c5tJvjYp927Cd2VFGzufJZs31RV8wTCavGfCsn+b3ITq0Xe4XJj/++NPfIiHnOyZ+nfS/YcdMP7L3Y0UmXZd/p5k7Pl0zIfAAAAFK3YgNLPvu+m5dfflnr1693tu/fv6/PPvtMn3/+uXp6ejQ6OurkA4nY/mH7ie0vtt/Y/mPZ/mT7Fe9TAgAAwMJqV6tip9lyf5a2L+8/NWUUgn2Di/2p96A3rZbLfd9Ke4pBghJz9EldUFSXFT8SITJtzENI9c4IjSpToh/OcSfHa4/7eT1sjqt/qh+i3QDEavgxu9p7PvVqcNYzcd/Z9LR3XWH6RLl5eo06G9OPrLCaTS26zN5ap59Y+73AZIPpYfHin+vM4vtL2PQte89B1Uy8GcgfXRffQ+17yM566fkpMh9rMd4CFI5rR3tPB5x2i73niPnmtsQc6T6HLtMGzdPOnypizqv13qu0z5zXpLfNGY06FvNuqgPOX4EWc7WZe4ff3+w7lpKVWmVKmH100lT2/W2VEwFlAACA2az4wJIVCARUXl7uTFdWXFzs5H355Ze6ffu2Pv74Y129etUZiWIDCI8ePXKmOhsfH3eOw7PBPm/73O3zt/3A9gfbL2z/sP3E9hfL9h/bj2x/sv0KAAAAWDgRndWLekN7VR7zk3JAJ/S2s++AGpyf5O1UXFV6R0HVOj8NTyrXfmfUgztSJHXT/5ftCVPCcVOOW16LqdV+5yftkzoVU2L5RBDjiHdsizmzwtTUHa8xOz8gYN8NM/kzfYn2OaWcMVcNmat2OddeiQI6rPNmsaGTPaaV3Lux99Nmtvab9pq8671eAKbR3LU9Zr5BknrTbrb1aswzaDTl2Gu1mWvu1SFTWtA8wRPugYYdv3bU5Nn6vWEWWz+3bnumBDZmc9Zcu8YJe/jn7zV5EXPnZybCSpYbILlkjnXLsvXa44zK80NdM8s1dbXCppTYsE6lNyqqydyx7Ss2EBIf6poPG/QpN330gFdfG3ANqdlstZia2Pekuc6aO3rD6feTI43qzZOvNMcfMs/BTjfpPnn3O2K/y77j5tywuZb/XqVKc6Z9Jo0m3x93ZoNOh02e24uaJ65l22+/yfHv1fa3k+YKYVMj911Qbms0mVKKvPcxBRJ832fWbv6iREw5/sgzAACA2WREDS+9aoyNjamvr89Z/IABkMy6detUVGT+g9csWVlZXi6wAOx/h95ykwAAAO6P2KU6oLD5nwlXdHNKaKZFx6f8GF2iarXq/LTwTauO6Q21a9jsm8//Ecq+yv+A2nRB9eZKhzQ5PqJEJ9WsuilhAvsDf6v2mxJjjztjzrdvpbHXuWnuxP/xvUYZOmtqHJtngwxF5gp2dMUZU2s7xZhlRz3ZAJUfTirX2+a8em9runbTNi+aNrJhHDuxl5+7x+SGTG7U5MZy7/OsOTo6cXQ8/xg7cWBdTI0Tl2W1mDs8YOp90txhnZfnatYRc7WmiQCAVaKDJt+OUnGDJfauj5izbVjJNdkPErfdZF3i6xg27VrrXH8yIGf7TLOp8WR5vi5Trh3HMvkUK8zVbO960eyZqY388k/rorn2QdMCk+XV6pzJd4Mlk0KmhaqmHPe2ObfWLPY6sWUlbucu09tKTa+z5102PWIyINVg+tHxmGCY358StV2yZ+v3GRtC9Z9hs7nqUXP1yRpP//61mHY6YFowvp8meg42tFNramfbpk0ntFfvTLsX/29B0ZTr2edUbUqZOs3hQVPXJlPX2Kdq+9shkxsr0XGpsEHtXFPjC6Y9YyfeBAAAq9p7Zvmem5yPVRlYimVHqgwODjrL8PCwHj9+rJGREWcKNEYtPTsyMzOVnZ2tnJwcrV27Vrm5ucrPz3cWuw9YFPa/YAksAQCAKSKyL94PKJgwLBSRfc+K/WG6JOnPw63K1RtqignSPI2wKdEGQ4KmxJmulupxidh7tnc9/dxU7ndl8dvJPuHEz/jp2jKe37Yzleeby7HJJH+WUz3dPbr9IlE9F+Iekpnt2dn9tqcmKtWv18L0Zf97MVv7pXrczOxYyVrzny7DOuXlAACAZwCBJQBIUwSWAADAojihF9WqOl3UUS8HAOajS/tVqipdUf3E+CwAAPAMILAEAGmKwBIAAFgUdhqtIh3SeaauAvAU3CkDI+Yvybl5j3gCAAAr0lMGlpgDDAAAAABWlICqdU4nVepNgwUA81GuKp0xf0sIKgEAgLlhxBIALBZGLAEAAAAAAABIN4xYAgAAAAAAAAAAwFIgsAQAAAAAAAAAAICUEFgCAAAAAAAAAABASggsAQAAAAAAAAAAICUElgAAAAAAWEARteiE9qhUjWr38qaKmPzjekO5ynA+uebYGnN02Ns/m7Ap4Q29OHH2G+ZqXd4+AAAAYHERWAIAAAAAYEE0ar8T6DmgdxRSlyJefqwunVWpXlSDOlWnUzpnPkdVbnKPmX+PJDwnlg0qlZsSrqhCZ8y5Z1SvdnO1Uu01ZQIAAACLLSNqeGkAwEIqN8stNwkAAIBnQUitZgmrU+d0TGd1UjdV5/zPwqnOqlHVOqqgt23ZoNQxXdJpc3atSrzc6Y4p1zm2UxdijrqkIpO315x9wZwNAAAAzOA9s3zPTc4HI5YAAAAAAFgQFarSQfOp1l4vJ7HDcUElq1qHnHVYw846sUtqUsQ5f2roaZ/qzdKqhiST78WKmOOanU+L2mYdIQUAAABMRWAJAAAAAIBF0T6HNx9d0kVnXWo+yfV5gaCA82+sKh0w/4bMdZKHiuw0eqXOO5kOqdZ8Dmiv836nRibRAwAAQMoILAEAAAAAsCj6zCcVl9Sgo2pWpU7qcIKg0aQiZ2/YfOIFvPM6zSeZozqgLlPKFQ07dYua5aRqVKUK7wgAAABgNgSWAAAAAABYBq3O6KQM89mv46rROV1UnbcvmUpVm39b1BQ3FiqkI6r30sm0q838W2FKqpgIXgVNiScJKwEAAGAOCCwBAAAAALAMinTAfGpV7bxvqVE1qtcld1dSAZ00HzcotUcnzFknzDVyTbrECTnNpFxVpqSQjptPC+9WAgAAwDwRWAIAAAAAYBlU6rTzOa8+DZt1td7RfjWo3dufWLnqzPEXVatSNTmhqFK1mpx67fWOSO6UQua8ClOGDUZl6JBOmNIIMQEAAGAuCCwBAAAAALDMAqr1pqQ7oQY3awZB7dNpXXDep3TBpPY5I5Eumz0B7Ve5e1BCJeboK+ozx76tg2rWO3pRuWqcJZgFAAAATCKwBAAAAABAGih1QkIR9bmbc9SiZvPvQe1zN2cUVKXqdU7DuuIEs47pHXcHAAAAMCsCSwAAAAAALJmQmnRWXd7WpLBa1GrW5VNGHLm5Uyeri0ybvK5dDWo0uUd1zMtJLGw+sQKq0EGVmNSwmwEAAADMisASAAAAAABPLaKQmr1PizMtnQ34tE7kuaGkiBp1QjUq1QGzbjNHdJlPq8ndq0Nmb1CndNw50jphcg/oDbM0eTlSgzPZ3QFTRsg5t8U56rg587DqZxivZINPReZzxFyrXWFTVtiU+47J7VKV+QAAAACpyYgaXhpp7q233vJSwMJ79913vRQWjP2/mt5ykwAAAFjtbODmxZigULzziqraSUXUrGOqUZNJTQqoUmfU4o0gcjU4IaM2k3dG53TYybNjmKp0yOROnl2hOl3USQW97cTCatRRc72zU8pN7VwAAACsIu+Z5Xtucj4ILK0gNrDEj/9YDPStRUJgCQAAADMKq8sL8gRUkjC0E3GOSLTPPzdg9gXNv3PxNOcCAABgxXvKwBJT4QEAAAAAsCyCKvE+ycYLJQs4TZ47n8DQ05wLAACAZx2BJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBKMqKGl0aae+utt/Tuu+96W8DCoW8tkv/trQEAAAAAAAAgnXzPW88DgaUVhB//sVjoWwAAAAAAAACAVDAVHgAAAAAAAAAAAFJCYAmrQEThri51mSX087Nq+O4e7f/rdm8fAAAAAAAAAABYKASWnmXXG7QnI0MZyZbvtngHLpDrjdqfY667r0GhUS9vQVxU/W9WqtIsVQdrdPwnIYW9PQAAAAAAAAAAYOEQWHqW7azTlWhU9jVb0b7TqjZZJf/torttlx/bnIUT+c9WXbIBpV9eVFvEzVsYVTp5t1Odduk7r8NeLgAAAAAAAAAAWFgEluAKlipoVkUbS9ztRRD49hndbD2nC5+f0eHnvEwAAAAAAAAAALBiEFjCEgqo/JsHVbUz4G0DAAAAAAAAAICVhMASAAAAAAAAAAAAUkJgCUsnElb7B01q+lmXlwEAAAAAAAAAAFaStAwsZWRkTFsW2mJc85kRaVfzn76hPUX+88nViwcbdCns7Z+iXQ2vZii3tFSlpUV68TePqOHTYW/fDOZUxjx5ZbyY65VRtFc1f39J4VFv/wrzgx/8QN3d3d7WdHbf3/7t33pbAAAAAAAAAADMXdoFluwP/NFodMqykNwgBUGleQu36Ej5izp0aZ9OhYbdZzRwUbV3j2v/vhNqmxaUKVfdJ1ENd3aqs++8Dnu5M5pzGfPglXE8fFTnH7j9bPjSCeX+3X4V/XaDQissuPTDH/5QdXV1+q3f+i11mraOZ/Psvu9///v60Y9+5OUCAAAAAAAAADA3K3YqvPkGhxYjWPXsCOvsdw+oqbheV35Rr6qSgJv9XKWqf6dEutqs1ltu1vwtRRkRtfzJIZ355jld/l8HVfGcmxvYfVCnf35K+355XCd+GnEzV4hvfetb2r59u27cuKFvfOMbU0Yu2bTNs/t27typN99809sDAAAAAAAAAMDcrIjAUjoGgp7JkU+fntSJ96WDf1animwvz1P+X06o7vsnVL3dy5ivpSijo0n1Pwmq7k8PKuhlTSg7pNpvSs3/1KyVFFoqKSnRL37xCye4dPPmTe3fv98JKNnFpm2e3WePsccCAAAAAAAAADAfaRdYskGkmQI2sQGd+OCOvz1b3mxiz5nLeatd16VLaleVqr/pjSKKEfz6UZ38weFpwaC5Wooy1HZRl7RXe3Z721OUqLTMrP71otrcjBWjrKzMCRyVl5c7gaR9+/Y5i03v2LGDoBIAAAAAAAAA4Kml5YglP7iUKKgTO5VdbNqKzfclypuJLdMeG7vMVo9nxXDYvrunRKWLGJtYijLar9qQUYtqCqYGEP3lwE/M7kif+pyjVxYbXPr3f/93J7jU3t7uLDao9G//9m/OPgAAAAAAAAAAnkbaToU3U1BnJvHn2PSzFgBaNNnuKKLIqLNaHEtQRvnOSvNvtc4MTA0gTl3OmCNWpi1btug//uM/nECSnf6OoBIAAAAAAAAAYKGk/TuW7I/8cw0uPS1bXuwCV/lv7ldArbr8kZeRQNv7Lery0vOxFGXo1b2q0GVdueptr0I2kGSDS36ACQAAAAAAAACAhZB2gaWFCOT4wSi72PRcTY5amVxgfOOo6rZ3qeEHzQp7WVNcbVD9f5bqqWaxW4oydtao7ptdavzxJS8jjinj0P8MeRsrl50Oz45eAgAAAAAAAABgoaT9iKVULPaookTX9wNXq0a40wnk9D2YaSxQhep/dlIV/3RIe/+gWaFwxM2OhBX66RHtfTOk2v/bTjOXRCQi5wyzTm4pyihR7T+c1t6f7NeLf9Ckto6YMn52XPv/UKr/bxVuHgAAAAAAAAAAmJCWgSU/aOMviUYM2byZ9luJ8v1z4tO+2Ov6S7Lrr3jXG7THv8+iI2oxWV1/vX/y3r9rc+LsrtPlBxdVN1qv/Rtz3eMKylXzfqUa//O0qp/zjpvQroZXvevlHlKzyQn9xZ6JMmred4+aYs5ltKjGu16iMvb8dbt7WKzdtbrw+WW9nd2o6pdiyvinCjX9a50qsr3jAAAAAAAAAADAhIzoKo2a2EDBaru1t956S++++663BSwc+hYAAAAAAAAAIBWrYiq8WP4olVU7yggAAAAAAAAAAGCZrLrAkg0oEVQCAAAAAAAAAABYeKsusAQAAAAAAAAAAIDFQWAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBKCCwBAAAAAAAAAAAgJQSWAAAAAAAAAAAAkBICSwAAAAAAAAAAAEgJgSUAAAAAAAAAAACkhMASAAAAAAAAAAAAUpIRNbw00txbb73lpYCF9+6773opAAAAAAAAAAASI7AEAAAAAAAAAACAlDAVHgAAAAAAAAAAAFJCYAkAAAAAAAAAAAApIbAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASElG1PDSSHNvvfWWl0Kq3n33XS8FAAAAAAAAAACeFoGlFcQGlgiUpI72AgAAAAAAAABgYTEVHgAAAAAAAAAAAFJCYAkAAAAAAAAAAAApIbAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwhElXG3Tkb9q9jRmMdqn5T99QaU6GMjIylFtZo8Zfhr2dAAAAAAAAAABgtSKw9Izr+qBZzT89q4Y/ekOlrx7XxVFvRzKjITXsK9Wh94KqDw0rGh3WlT+J6MS+ctW8T3AJAAAAAAAAAIDVjMDSM66zrUUt/9qqUEe7umYLKhntf3NIxz8I6uj751S7M2ByAir//XM698cRnf3ucbU8co8DAAAAAAAAAACrD4GlZ1zlH57W6X80S8spHfbykhptVcNfhKSv1+vEN7w8T9Uf1Kkk3KT697q8HAAAAAAAAAAAsNoQWELqftakxohUcbBaJV7WhK/t1wGzuvQv50VoCQAAAAAAAACA1YnAElLWdqnV/BtQ1b5yN2OKClW8YlY/b9VlNwMAAAAAAAAAAKwyaRVYysjImLYgfXR22LFI5SqfNlzJKldFpV23KXTdyVgQP/jBD9Td3e1tTWf3/e3f/q23BQAAAAAAAAAAFlNaBZai0ejE2l+WMrhEMGsm7Qq12XW5ShMGlhbeD3/4Q9XV1em3fuu31NnZ6eVOsnl23/e//3396Ec/8nIBAAAAAAAAAMBiSfup8BYruJTomn5gCzMJKvicl1xk3/rWt7R9+3bduHFD3/jGN6aMXLJpm2f37dy5U2+++aa3BwAAAAAAAAAALBbesTRPz+7opi512hnxlkBJSYl+8YtfOMGlmzdvav/+/U5AyS42bfPsPnuMPRYAAAAAAAAAACyuFRlY8oM68cGd2O34fb5UjrH8fcn2P3v8dyh1KvzIyVgSZWVlTuCovLzcCSTt27fPWWx6x44dBJUAAAAAAAAAAFhCKy6wZAM9se9gip0qz5/KLvaY+MCQnx+fjhVfRvw1rGTnrmalZTaA0672hCOW/HcwVapip5OxYGxw6d///d+d4FJ7uynfLDao9G//9m/OPgAAAAAAAAAAsDRW5VR4sQGfZIGhmTxrAaNUVe6rMv9GdPE/292MWKMhXf7UrL9Zpb1uzoLasmWL/uM//sMJJNnp7wgqAQAAAAAAAACw9FbVVHhYZL9bo8Nm1fbPLZo2aOmXLWo2q33fOqDFmpjOBpJscMkPMAEAAAAAAAAAgKW1IgNL/jR0sQsWyGjESySQXa36v6qQftmgUx94eY6wmv+hSZHgUTX88eK+78hOh2dHLwEAAAAAAAAAgKWX9oElOyJptsBR/Kil2O35nJ8Ke858zktX4X9t0SWzDv1VvZq7kgeXyr9/Tie/3q53fueQmq6b40Yjavv7Q6r9p6Bqf1yvfdnegQAAAAAAAAAAYNVJq8CSH6jxgzZ2iQ8K2e3Y/bMdkyyolOgYm45dW4nyVpOW77ptUPR/NMp5c1L4rA6V5jp5Ne87h0yVXaG6X3Tq3O+HdaLCHJeTq/3/q0T1l0I6/WbQOwgAAAAAAAAAAKxGaRVYsgGe+CWRuRwzk/hj/O3Z8qxEeStR9Y8n7y9+OfOmd1C87BId/MEFdY64xw23ndHRrxNUAgAAAAAAAABgtVuR71hKZrWPLgIAAAAAAAAAAFhOqyqwFDvaBgAAAAAAAAAAAAtrVQWWAAAAAAAAAAAAsHgILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBKMqKGl0aae+utt7wUUvXuu+96KQAAAAAAAAAA8LQILAEAAAAAAAAAACAlTIUHAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSkhE1vDSW2VtvveWlgNm9++67XgoAAAAAAAAAgKVBYCmN2MASwQKkgr4CAAAAAAAAAFgOTIUHAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwhIVztUF7MzK0969DXgYAAAAAAAAAAFhNCCxhYYyG1PCd42r72kmd+X6FlwkAAAAAAAAAAFYTAkvPkqsNOvI37d7Gwgr99SEd/6BSJ/+pThXZXmZKwmr57nG1eFszCf+yUTWVucrIyFBGTqne+NNmdY16OxMZ7VLzn76h0hxzvDknt7JGjb8MezsTm3MZAAAAAAAAAAA8QwgsrXJdHzSr+adn1fBHb6j01eO6uBhBkv/vmPb/WUhV/9iiup1e3kwehdT6U1OvH57QgcpSHfhJp7cjufD7NSrfd0yh321V30hU0c5m7fvFIZXua1Ao0T3ZEVT7SnXovaDqQ8OKRod15U8iOrGvXDXvJw4uzbkMAAAAAAAAAACeMQSWVrnOtha1/GurQh3tizPy5lGLjrzZqPCbZ3TuD0u8zFk8atelfzX1amtX362IlzkDU8bR//Oswt88rZa/3KegHRFVvE/1/3JK+z44rpq/nz4Kq/1v7AiqoI6+f061OwMmJ6Dy3z+nc38c0Vk7QuqRe9yEeZQBAAAAAAAAAMCzhsDSKlf5h6d1+h/N0nJKh728hRNWy/9Vo6bIYZ378WEFvdxZlVTrbVunfzyjM382+/uYut6r19lIQEf/vFZTQldltTr2ptT2Zw1qjQ2ajbaq4S9C0tfrdeIbXp6n6g/qVBJuUv17XV6Oa85lAAAAAAAAAADwDCKwhHkLv39UNT+RDv9zow6mHFWaq3ad+cdLZn1Q1b/t5kwKaN83K6VIk5p/6WVZP2tSY0SqOFg9NUhkfW2/DpjVpX85r8nQ0jzKAAAAAAAAAADgGURgCfMTbnamjtN3zqjxzUWLKkmPLqn1U7P+ZpX22unp4pTvtCOeImq9NDlVXdulVvNvQFX7yt2MKSpU8YpZ/bxVl92MeZUBAAAAAAAAAMCzKC0DSxkZGdMWpJOwzn73kM4GanXmH6pTnwJvPro65YRzykqnjz6yKvbKhn1Cn4TcbaOzw45FKld5whPKVVFp120KXXcy5lXGQvjBD36g7u5ub2s6u+9v//ZvvS0AAAAAAAAAAJZf2gWWbBApGo1OWZbLbEGtZzXo1fWjQ6p5P6ij759W9XNe5mIJXZYN55SUlbrbs2pXqM2uy1WaMEqUwJzLeHo//OEPVVdXp9/6rd9SZ2enlzvJ5tl93//+9/WjH/3IywUAAAAAAAAAYHmt2qnwnjbgY8/3A1uJrjXb/lXreoOq/6hVFX95Uae+4eUtgaKiuY6LCio4x6DX3MuYv29961vavn27bty4oW984xtTRi7ZtM2z+3bu3Kk333zT2wMAAAAAAAAAwPJKq8CSH6yJlyhvKc1WfqL99l5WXcBpNKSGbx9X29dO6tx/s5PDLZ32u3Z6u7noUuccT5l7GfNXUlKiX/ziF05w6ebNm9q/f78TULKLTds8u88eY48FAAAAAAAAACAdrKgRS4mCNfF5sdvx+3x+fqL9sXnz2b+ahf76kI5/UKmTP6lTRbaXudi89xtFwn3u9qz8dyh1KvzIyZjdnMtYGGVlZU7gqLy83Akk7du3z1lseseOHQSVAAAAAAAAAABpZ0UFllIZzWS3/bzYtM8Ggvx8f4kNDvl58WnfbPt9M+1bkT44oQN/FlLlD86pbreXtxRKSlVu1x2dSjieyHs/UsWrkyOoSstsMKZd7QlP8N/BVKmKnU7GvMpYKDa49O///u9OcKm93dTZLDao9G//9m/OPgAAAAAAAAAA0smqfccSFtDoJR37nXfU/s3Tavm+E4JZOs/tU9UrZv3zS7rs5kzRftVGiQKq2jdZr8p9VebfiC7+Z7ubEWs0pMufmvU3q7TXzZlXGQtpy5Yt+o//+A8nkGSnvyOoBAAAAAAAAABIVwSWMIuIWv6oWo3hap1prtXST8xWroPftSOFzqjl527OpC61/iwkBWp18OtelvW7NTpsVm3/3DJ9BNIvW9RsVvu+dSDmXuZRxgKzgSQbXPIDTAAAAAAAAAAApKO0CizZqeMSvbPoWXqP0aIbjXiJ1ITfr1XNexEdbj6jw0EvcxFERr1EAuV/fFKHAxE1/s1Zhb08x9UmNfxcqvq7E6qKfedTdrXq/6pC+mWDTn3g5TnCav6HJkWCR9Xwx1NDZHMuYxHY6fDs6CUAAAAAAAAAANJV2o9Y8t+JlEwqQaflCEzZMtMpIBb+1xZdMuvQX9WruSvF4FK4RUe/e1b6zjk1fmsRokqRdrXY0UA6q/q/bkseXHquWo3/XKuS92tU9WeXFDbHRbpadOzNEwp9/aRO/f70cVTl3z+nk19v1zu/c0hN1839jkbU9veHVPtPQdX+uF774oNE8ygDAAAAAAAAAIBnTdoFlvxRS/4SH1RKtt+mY8UeF3uN+PPj9/t58WnfbPvTTct33ToW/R+Nct44FD6rQ6W5Tl7N+84hSYTV/H8d0lkd1pm/q9aChpWuN2iPbbvcF3XMm3qu7c/2KjfH5L3a4NYzTvDN0wpdOqWKn1WpyByXu6VWoTfPqfMXdapINJIou0J1v+jUud8P60SFud+cXO3/XyWqvxTS6TcT382cywAAAAAAAAAA4BmTEY2NqmBZvfXWW3r33Xe9LSA5+goAAAAAAAAAYDmk/VR4AAAAAAAAAAAASA8ElgAAAAAAAAAAAJASAksAAAAAAAAAAABICYElAAAAAAAAAAAApITAEgAAAAAAAAAAAFJCYAkAAAAAAAAAAAApIbAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASElG1PDSWGZvvfWWlwJm9+6773opAAAAAAAAAACWBoElAAAAAAAAAAAApISp8AAAAAAAAAAAAJASAksAAAAAAAAAAABICYElAAAAAAAAAAAApITAEgAAAAAAAAAAAFJCYAkAAAAAAAAAAAApIbAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmAJAAAAAAAAAAAAKSGwBAAAAAAAAAAAgJQQWAIAAAAAAAAAAEBK0i+wdKtRB7aUqtQuFcfU8sjLj9H6p97+LQfUeMvLBAAAAAAAAAAAwKJKv8DSaETtHV3qssvVRh37yzZvx6RIj7e/o12RUS8TAAAAAAAAAAAAiyrtp8Jr/5/H1dThbQAAAAAAAAAAAGDZrIB3LLXq2J+1KOJtpWQ0oq4PWtT802ZnafmgK+HIpki4S11dZunxrm7OC/3cPaf5Z23qii30UUit/vU+CnuZiUU62tQyS9mOSFjhBFP9AQAAAAAAAAAApKO0DixVf+ugs4785JjqP3WSM4u0q/mP9io3J1elv3lAh/7PQ85y4DdLlbvlkM7GjXxq/ZNSlZbadzUdV2vHWR3akqs9Ve45h6r3qrT8iFrCUvhnx/RiwR694V+vskilf9CiaeGlSEiN1basvTowS9mRnx1RaW6RigoytP/v271cAAAAAAAAAACA9JXeI5Z+77je3m4T7Xrnz85OD+TE+6BBtT9qmxjdFCgJeimjq1k1321Sl7c5RaRRb5TXqDl+Z1eTavbtUXl1o6nBVF3vHdPJ2GDXaEgNv71Hx37mXSRQopKSgJu2Zb96TJdiRi61/tNkXS79Y/O06wMAAAAAAAAAAKSbtA4stXeU6PhfVrsb7x/Vif/PTSb1jRM6+buVqv3nKxoeiWq4s0/RvvM67MeXft6gM9e99DT79HZrp4ajw+o8e1heSEjhqyGFv/62LnQOKzrcqTPf9ve0q/lnk+Gg9r85pOMfuOnKvzTlm2M7zTnDl+pUbjPDjar/6eTcekXFMUGv0qByvSQAAAAAAAAAAEC6Su/A0t0uBX+v3hu1FFbjHzfMMrKnRLUtl3X69yoUyPayAvtV820/iBPS5ZCXjHP4ny+q/pslCphPybdr5E7CZx3WuV/Uq8qOPgqU6PC3J/eEPvEv1qamH3jpkrfV9N9N+e6WAl8/qqNfc9MtP2t1E8a+v7ysc39eq9rvn9LFs7Wm5gAAAAAAAAAAAOktrQNLkXCflF05OWrp0xM68X5EFa9WuNsJRdT1QZNO/Je9Ks3NUEZukQ78cNZJ9Ew53jqBiSBVMo9CavPntetpVPUW+94mf9mv+o+8faHQZGAsUK6D/+O0Tv/gqPYVe3kAAAAAAAAAAABpLL3fseQJfvuUTr5iUxGd/csmdWX744HihC/pxL5clf7mEb3zfpu6Iubcskrtey3J8Qulq3MyYDQaVldH15Ql7L9bqYAp7wAAAAAAAAAAwMq1IgJLUrnqfnhUzoR2vzyuM58nGrEUUcufVOmdX7pblf/9gjqHo+q7e1lnDjtvOVo828tV6SX1tXpd7rTvV0qwNNcw5R0AAAAAAAAAAFixVkhgyfjGCZ38pk1E1PjDs07WVK1q/knETe6u15m/rJJ9LdKSyK7UXmdElfHBeV0cLVFJSYIlGFOh0bAu/f1xHfmLZrV71QYAAAAAAAAAAEhnKyewpBLV/tXbSj72qEgBP25ztVWt1220JqKu94/pjf875OYvmnLV/pn3Hihd0rHfPKSmT8OmdGPU1OHnDTr+3sRkeY7Q/9ir/X/SoKb/55BePHzWPRYAAAAAAAAAACCNraDAkvG1Ezr1nWTDkPbp4O/7+1p17KVcZWTkqvS/NE6+/2gRBb99Rue/40zWJ3U168irRcrNyFBGjqlD1XE1/MExnQ27u632WzG1+rxTnV4SAAAAAAAAAAAgXa2swJICqv6LeiV6w5JV9Zetevvr3oan5FundKX9tKq87cUTVPWP23Xxrw6qJNvL8mWX6OCfH9W+mJjY/t877L4zyqj8bvUMI7EAAAAAAAAAAADSQ0bU8NKrRqSnS+FRKfBciYLPeZlLKqJwlzcVngIKlgTNv9NFwraeQZUUJxuFBQAAAAAAAAAAkD5WZWAJAAAAAAAAAAAAC2+FTYUHAAAAAAAAAACA5UJgCQAAAAAAAAAAACkhsAQAAAAAAAAAAICUEFgCAAAAAAAAAABASggsAQAAAAAAAAAAICUElgAAAAAAAAAAAJASAksAAAAAAAAAAABICYElAAAAAAAAAAAApGRFBJYyMjK81MKw15vtmgtdZqxUysfiWezn/yw/32f1vgEAAAAAAADgWZH2gSX7Q3U0Gp1IL/YP10tRxkq1GG0zl2suRvnxlqKM1cx+V2k/AAAAAAAAAFi90jqwZH+g9oNKVmz6adjrJLvWTPtiPc2P56mWsZwS3d9i1Hku11yo8mdq/5n2xZrp+ad6jdXK3jvBJQAAAAAAAABYndI2sBQfVAKwchBcAgAAAAAAAIDVKSOahtGbmYJK/j7/R+vY41LJ87etZGVYfjnxYs/3zXSdeDOVH1vXVOsZK1HdfLHXiD9upn1W/P7Z6jfT9a3ZyphJfPmx56WSF1v2TGX65cSLPd+XqDwr/vzYuqRyXKxEdUkm/vyZ6hdbJ99M5/vir5OMPW6m/QAAAAAAAACAlSXtAkuz/RAd/0N4/PGJzk81L9Z899v8RFIt3z8/dt9sdYkVe2yidKJrpZpn2Xwrdl/ssbNda7b9s7HHWsmul+r1ZytzsfbbfCt2X+yxic6braxYqZw/U3lzPd9KdE6s2fYDAAAAAAAAAFaOtJsKz/4AbX+Inknsj9SpHL+UbH0SLXMx1+OX2nLXL7Z8m06n55+K5e4PfpvNJ+CT6JyZrjGfMgAAAAAAAAAA6Sst37Hk//CNxeEHFfwF6cPv+0/zfJb7fJ89l6ASAAAAAAAAAKwuaRlYsgguLR7btvEL0kf8s5nr9yD+fLvMxdOebxFUAgAAAAAAAIDVKW0DS5b9YTrRj+qxebP9gD3XH+XnYynKWEyz1X8x728+1449x6ZX0/Of7Vp2/1zLiz/ebts2S/b9ihd7TKJzkl0fAAAAAAAAALD6ZERXwC/AsT9U+2n/x+xE1Y/9oTv+2Nh9vthrzLY/1kx1SGam68fumykvmfhjY+uXKO1Ldt3Yc6z461uz5Vnx148/J76cmdhjZztnpuvH7vPFXmO2/bES1WGm82P3JcubrfxEZcaLv0ay8+PL9s10vm+mcxMdDwAAAAAAAABYHVZEYAkAAAAAAAAAAADLL62nwgMAAAAAAAAAAED6ILAEAAAAAAAAAACAlBBYAgAAAAAAAAAAQEoILAEAAAAAAAAAACAlBJYAAAAAAAAAAACQEgJLAAAAAAAAAAAASAmBJQAAAAAAAAAAAKSEwBIAAAAAAAAAAABSQmBptRuNKNzVpXDE214pVmq9AQAAAAAAAABYxQgsLYsW1WRkKMNZaszWYgnpnVdzVVRaapYjannkZae9lVpvAAAAAAAAAABWt7QOLEW6WtX43b0qyvGDMGbJ3auav7+k8Kh30LPOG9nTlWh0z60WnbvqpcPn1Pqpl15K/3pcpVtKU1uqG9Vuz0mHegMAAAAAAAAAgGnSNrDU9U+HVF76ho79pG1qECnSprN/sl9FW46oJezlPcs+qldFaalKzXL0X7083/Za1f9hiZMMfOOEar/mJJdWpFNdHV2pLXe9yFg61BsAAAAAAAAAAEyTETW8dNoIv1+j8v9yVhNxo93VqvuDGu1/7oqa/u4dtVyVgt85r/YfVyvoHbKy2KnwDuiskz6s89EzqnbS82DaKsO0lXX4/43qzJtOMn181KQj/3DJ2zDCl3Xmp21yQkjbq1T7O+VOtmPLQdX/ebXckBIAAAAAAAAAAEg36RdYetSimo0HdNYbvFL5l1d08b9XKOBuGhGF/r5R7d+pU7UXVYqE/WngAgoWBxXIdrINO01c2A1i2H0lZp+TnswPPFei4HMm8Sik1p+FnGBWoHy/qr5WMlFm5GqrWj5x9qj8m9WqLHbzfXMvP4XAUiSs0KVWhXrczWBFlfa/4p/vivR0qe0HJv9/hpztgz/uVOPv2JRfVuLy7XnuKLD4+rpm2h/paFPrpXa37Ww7vWbaKe78GV1v0J6Xjsup8XfOK/rjRCG1ZO1m9vhtnR1USbHJHTX94Rctbjs9V67936xUiX9w7DN9qUrVryUPQ6Z6X5Gwudpz09sMAAAAAAAAAIBngg0spZPOf6yygS53+fqpaKeXP5Pz3/GOV0X05OdepuN89LB/LR02W77J/MB/vRDtPHswWjJxnLuU/P75aN9IX/T8fy2fki+VRGv/3z7vOq6nKX9qvvHgYvTkt0q8fXHL19+OXvSL/vxktCLRMc7iXzNxORf+OODlBaJHf+FlTrgYPRrwznnlZPSmlxsdvhI99bsJ6lVyMHrmrndMKmLr/Z0pdx4jeftMtHXgaPTC3TPRgyX+cd5SUhs9b9qor+VotDw23yzOM/WuMyHl+xqOnv9D77jsfdFTU54zAAAAAAAAAADPhrR7x9LFn7d6Kan6T2oXfVq0yA/fUOnhZnV5276u92q0/9VyHfj7di/H16WmPznpjrhZBF3/ckLH/2WyNsGJ4TfGL99R9V/ETCs3T1W/V+uNAIroXEubk5rwwXmdc4cKqeK7B+VMVDcaUsNv79Gxn3n1CpSoxK9XV7NqXj2mS7HvwVoKkUa9UV6j5mkPrkk1+/aovLpR057ce8d08lNvw5rTfbXqzI+840YvqfFf4q8OAAAAAAAAAMDql2aBpXa1f+IlVaGq34wJqiwWGzj4+tu60Dms6HCnznzbLzOs0NWw9v35BXUORzXceUaH/V23mtVy3UsvsJLff1t1rx3UydZODUej6rP1Cp1Upbc//Pen1GLrvP2oLnaa+v6em2/ZqfA6TV5nZ6OqvLyEfvugav34yfutUwIwbc1NXpCtQrXfct9/1P43h3T8AyfpTE04bNqp09Rr+FKdG3gKN6r+p140aknt09tOOw2r8+zhienywldDCid8pu1q/tnk3c7tvopUGjOTXmlxrpcCAAAAAAAAAODZkWaBpZBCsSNKlsRhnftFvarsSJVAiQ5/+6CXb3z7nC7+jyrnnT2BksOqmQjihHR5sYYsZVfpZNs51X1z8h1PKjuo2m96abUpdMussu27h0oUnDjI1DFoR9zYZfKdRAmZMg7+vnfEp01qngiStav1fW9Uziu1OrjTJtrU9APvZkveVlPM+64CXz+qo19z0y0/mxxptlQO//NF1TvtFFDJt2s0+eSSP9PQJ/6Dm+t97VP9f57T239Yq7q/u6hz31nssXQAAAAAAAAAAKSfNAssVajiFS+5hALZXiJe9ozhmcUzGlHoZw06UrVHRTkZyih4Ucd+7u1bIJPT4YXU8nMvmNTVqhYvsDcxDd6jkNq83eppVPWWUpVOLPtV/5G3LxSaNvXcokv23Iykz9Q3j/sK7Dyo+n88rZP/dZ+Cs10fAAAAAAAAAIBVKM0CS7kKbvSSizkqKJ11nNWhLbnaU31cTT8PKTwaUMnuKu3b7u1fKN+smZgOr/X9VtkJ37rePyt3fM7kNHjq6pwMGI2G1dXRNWUJ++8gKgiap7eCrNb7AgAAAAAAAABgEaVZYKlE+7/hBTSM5p+2OAGPZ0e7Gn63Rs3OSJqgDv6vKxoeGVZn6IJO/LZzwALap5o/9CJL759X66h0+RfetG8T0+AZ28sn3u+kr9XrsvMOpwRLc415eivIar0vAAAAAAAAAAAWUZoFlqSKP6rTPi8d+UmNat8Pe1uTwj9v1Nmr3sYUIYU+95L/f3v3FxrXde8L/CfQgRE4MAIfmIEGopBCZWyoTAKRSB8i0wuRyIVa1FCLBFLZgXOkBk4kCmncPLR2Cj12A6fRLSTRKTTYhRqr0OAx3ID8kCIFjrEOJGQKDZWhBg0cQ+bB4IEr0J0/e/QvirXl2PVI+nxgRb+99t5r7z1+y5e1Vk15Mb5894P0NZ9/YzrON/eY+sFUXHqpe+sl3b6G3mPN5fCmo/DxXBQu1g9Wl8Grae+Jw83lCa9djtml5j5OG9razZ52gnv5rltzMfnqiTh1cWGPBZ4AAAAAANDQcsFSfGM0zv0omxyU48L/7oq+VyejcG0hilenY/LlA9HZPxbDvcNRSFKb/DdW55RMvTEWhU9LUbxyKga7T0Qh6X+Qtv/8rjjwraSM6nd9Wm4EFZlsdNb7quZmYuZW9e9SJeZ/fSSG3290b9TV3Z1UEYWL01GqDlQpJ+Nt5ZnmcniVuPTaeFyq37RmGby6rhj5yUBSz8XYU0Mx1Xzf6ruVrp6Lid/+w3dXug+2+13FOPVUX4y9NRVnjj0ew79P9QsDAAAAAMCu0nrBUlXvr2bjve+uhktzb43F4FOPx4H+oRh7N9l4qVyMYn3JuIie4yOrM2yuTcbgoXwcGDgThVJP9D/74GfSbP/5XdG1kgfNxcShoThf+5bcYBx9utEbNybjyD+3Rds/dcThV2a+cuZT9/Ors4vK7w9FvqMtOjqHY/p20nlXq8vhlf48F/Wfc+0yeInsD87H5ReSf4/SdJw41BkdbY13y/dPxLkfjsWFf+zUsPtie9+1EAs3krKqeHMxqQAAAAAAYO9oyWAp2rtj5P8uxt/+MBI9m+QyuWdfj8v/cz3Gm7N+Dp6Oy79qLqCXyA3E25/MxnvPr87oeWC2/fxMHH/zbPSuLHM3G/Of1/7mYvTi+Ti6blOfTPT82+X4YmY0WbZug9qz3xnYsA9QIWY+Ssot9L44vu7edcvgrcjGwO8WYvaXRyO3cWm+9lwc/elo9D74/O4B2M539cVwM4SKnhh5/su/EgAAAAAA7HZty1VJ3bIqt0pRXmrUmX25yO5r1F9yuxyl27UlyjKRzWU3D2IepO0+v1K9vpxcv796/Uq4UYlyqbEsWyZb/d40H7Iy1ha/0de2+m4P7Xd+INJ8V+OaeKC/LwAAAAAAtK4dESwBAAAAAADw8LXmUngAAAAAAAC0HMESAAAAAAAAqQiWAAAAAAAASEWwBAAAAAAAQCqCJQAAAAAAAFIRLAEAAAAAAJCKYKkFtLW1JRV+CwAAAAAAaF2CpYesFqQsLy8nR9R+C+ESAAAAAAC0ppYOlr4qYKj1r23b9XXuvZ9qz79bqHS399vq/bc6v5UHNf7GezYbQ7gEAAAAAACtqSWDpbuFFrX+WvCwtm0nhNh4/3bu/UepvdPd3mvtN2x23Vbnt/Kgxm/et517AAAAAACA1tGSwVIztPhHeFhBRzNk2czdvn/jfRvff6vzW3nQ46+1cay1vs64AAAAAADAg7Hj9lj6qiACAAAAAACAB2vHBUsb3W3Wy4NSe+bGxvbUfjMhIQAAAAAA7Cw7NlhqBjpfN5zYbijUfObaRjq132rjv9l2f38AAAAAAODh2bHBUjPU2W4w0byn2daGHPfqfoyxV6z9rZq//3b/DQEAAAAAgIdjxy+Fdy/BRO2eZtuu5vPWNran9pvdy28PAAAAAAA8XDsuWGqFIGdtMNUMmrbrXu/bK4RPAAAAAADQenb8jKWNaoHEVoHN2vPbDTC2GvtB2xhIbXz/rc7Xjtee3+jrjr+V7V4PAAAAAAC0jrblFvy//GuDi6a1r7nx/GbntvqstNdttNW7bVdtvI33p3nGVu//Vee3uq/pXsffSu2+tfdsPK7ZrA8AAAAAAHj4WjJY2msEKav8FgAAAAAA0LoESwAAAAAAAKSy6/ZYAgAAAAAA4MEQLAEAAAAAAJCKYAkAAAAAAIBUBEsAAAAAAACkIlgCAAAAAAAgFcESAAAAAAAAqQiWAAAAAAAASGVXBEuVW6UolSvJEQAAAAAAAA/Cjg+Wir84EB3/nI98Zz5OXFkNlwovtkVbW6MNf5B0AgAAAAAAcM9aL1i6MRmDj+Yjf9c2ETP1ixeiMF2sVxHluPThfFIDAAAAAABwv7VesLRUiYWbpSjdtS1GY25SV4y8MRK59mqZ6Y1TL/fWewEAAAAAALj/2parkro1fH4uDnxzIurzkDI9cfSFw9FZP7FWb4y+MxI9ydFmakvhDb7fqI//aTnOP9+oAQAAAAAAuDetHSwdPBt/+2Q8uuonNlcpl6KcbK2UyeYim2nUaYKlys35mJlbqM9+ynT1Rf+3c5GpzX7aqFKO8lI2svuSYwAAAAAAgD2o9ZbC26aZV/KRzzfa6IdJ51YqxZgcyEfHo4dj8NhQDFXb4FO146G4cDO5JlG5ciLyHZ3R+Uhb9P16IekFAAAAAADYe3Z8sLRtS8U4950DMXal1DjO5CKXS6Y5laZj+NBYzC01Dmtmfj8VyZUx9850iJYAAAAAAIC9qrWDpVvXo3BxOqbXtfmVoOdeLLw1FBPXGnXPm5/FnTuLsbh4J+7MJUvulSfj9MVkbb2qzv3ZpKrKZ6MjKQEAAAAAAPaa1g6WShdiLFmqbrWdjevJ6e2bj6lf1Xdvisi9HlOvdUcyVykyT4/G6JONunBlplFU9b55PS79dCRG/u3tmL0wErmkHwAAAAAAYK/ZW0vh3S7GfHO6063JGHg0H/mV1hen/zs5VyyuLnmX6YqjP3sv3vvVaPTuT/oAAAAAAAD2oNYOlg6ejb8tL8fyunY+BpLT21ZaXA2MlspRulla18rNvZUeseQdAAAAAADARntrxtJjXdGTlPHk6bi+WNtfaZM2PWzJOwAAAAAAgA32VrDU3hOHDyb1tcsxu5SLXG6Tlm3uvFS1VI65X0/EiTemY6GS9AEAAAAAAOxBuzZY6uruTqqIhb8Uo1wPhbpi5CfNhfTmYuypoZj6tBz1U0uVKF09FxO/XVksr674s8PR98q5mPr5UDx+/ELjWgAAAAAAgD1o9wZLj60GS3M/PhBD75fqdfYH5+PyC9l6HaXpOHGoMzra2qLtnzoi3z8R5344FhfKjdM1CzfWBE1/XYzFpAQAAAAAANhrdm2wlPn+6Tj7dHJQNftJMyDKxsDvFmL2l0cj1550NbXn4uhPR6N3zUp4fd8/Xr2joefFgehKagAAAAAAgL2mbbkqqXehSpRLjaXuMvtykd3X6F21er56RWRz2ep/v6xSLkV5KRu5/ZudBQAAAAAA2Bt2ebAEAAAAAADA/bJrl8IDAAAAAADg/hIsAQAAAAAAkIpgCQAAAAAAgFQESwAAAAAAAKQiWAIAAAAAACCVlgyW2trakgoAAAAAAIBW0XLBUi1UWl5erv9tBkyCJgAAAAAAgIevbbmW4rSwZtAEAAAAAADAw9XyeywJlQAAAAAAAFpDywdLAAAAAAAAtAbBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhl5wdLS5Uol0pRriTHAAAAAAAAPBA7PFgqxplDHdGZz1fbiSjcTroBAAAAAAC471ovWLoxGYOP5iNfawOTsZB0b+pGIS79JanLl2Lm06QGAAAAAADgvmu9YGmpEgs3S1Gqtb9vsb7dYyNx+mSuXmaeORUjT9ZLAAAAAAAAHoAdvhReNgbeWYzl5eW489F4dLcn3QAAAAAAANx3OzxYqkS5VIpSvZWrR6sq5aT/VrO3EqVrhZi+OF1tM1G8235MlXIUr9auS64tf8XMqaXq87/qHAAAAAAAwC6zw4OlmRjN5yNfb6PVo1UzryT9j07EzK1CTHR3RP6pwRg6NlRtR+LAI4fjXHN/pjXKVyaq5zrjQH/tuuTazo7oe2Muysk1dZ9PRl9HR3RWz+VfLqwLtQAAAAAAAHajHR4spVCZjCP5wU1CpPmYeGkySslRTfmD4egaOBfFpcZx9hu5yCbL6839vC+G3l29euGPkzGXXFd69/y6UAsAAAAAAGA32v3BUs1SV4z+aTHu1PZimj8d3Ul3fFyImeaSeLcLMXrsQmNWUvZ4XPqf5fji74vxxZ0v4r3n61fEzBtvx3yjjI79+aSqyuajMykBAAAAAAB2qz0RLB2f/lu8/XwuMtU68+3jMXKw0R+xEIvJJKTKB5NxIVnPbuD/TMXR/Y062rMx8i8jjbo0HTOfN8rcC5di9j/GY+Tk63Hpv05Hb6MbAAAAAABg19obM5aS5ewauqK7JynXKH5yPakiZl7tivyjtf2ZkvbD88mZYhT/mpTt2ej90dl4753TcfSJWmQFAAAAAACwu+2NYCmFxZur+ydVSqUoVY9XWimZyhSZyDySlAAAAAAAAHuMYCnR1b2y81Ic/8NiLC5u3s4+nVwEAAAAAACwxwiWEt0HV9fHm776WeRyuU1aNjIry+pVYuHiqTjx6mTM3Uq6AAAAAAAAdrHWDpZuXY/CxemY3tg+LEZzcbr75rmJeP2xRln5zZHoe2MmSreT43Ixpl89E4XkuKby++F4/NiZmHprLPqeOhXFpB8AAAAAAGC3au1gqXQhxo4NxdDG9mohFpNL7pv2njh95Ww05y3N/fxI5B9pi7a2tujoPBBDb52KsTfnk7O1PZnWREk3FmIhKQEAAAAAAHYrS+Gt9a3xuP73SzHy7UzSsSrz7ZF4/VhXchTR9fzISgiVfWE4+pIaAAAAAABgt2pbrkpq1rpdjtLtxoJ7mX25yO6rl+vVr4nI1vZeSroAAAAAAAB2K8ESAAAAAAAAqVgKDwAAAAAAgFQESwAAAAAAAKQiWAIAAAAAACAVwRIAAAAAAACpCJYAAAAAAABIRbAEAAAAAABAKoIlAAAAAAAAUhEsAQAAAAAAkErLBkttbW31BgAAAAAAQGtoyWCpFigtLy/Xm3AJAAAAAACgNbRcsNQMlZqESwAAAAAAAK3BHksAAAAAAACkIlgCAAAAAAAgFcESAAAAAAAAqQiWAAAAAAAASEWwBAAAAAAAQCqCJQAAAAAAAFJpuWBpeXk52trakqOo17U+AAAAAAAAHq625RZNbZrhklAJAAAAAACgNbRssAQAAAAAAEBrsccSAAAAAAAAqQiWAAAAAAAASEWwBAAAAAAAQCqCJQAAAAAAAFIRLAEAAAAAAJCKYAkAAAAAAIBUBEsAAAAAAACkIlgCAAAAAAAgld0VLFXKUSqVo7KUHAMAAAAAAHDf7J5g6fZ0DD3SGfl8Z3T0nouFpBsAAAAAAID7o/WCpRuTMfhoPvIb21ODMfHv0zFfqiQXbvBRIaabM5WuFWKmnNQPVSXKpVKUqq18O+kCAAAAAADYoVovWFqqxMLNUpQ2tmuFOPfjoTic74gDL09HaeNyd98dj7PPZOpl7uR4DGXr5cN1ezqG8/nIV1vfb8yhAgAAAAAAdrbWXgov0xNHT47EyMmB6FkTFBXfHYruHxZi3aSk9u4Y/+hOLC8vx+I7A9EKuVKUFi3JBwAAAAAA7Bpty7UkppV8fi4OfHMiirX64Nn42yfj0VU/EVH+86no/86ZmE+Oj164E5d+0JilVJvpVL5VjvpCee3ZyO3/cn8mm4tstbtSmo+ZjxaqfV3R9/2eyDWurKpe++lszBQbkVW2uz/6DmYjGWkT66/PdPVF/5O5+vWVcinKH4xG/sXp+rnu16rXvVL7kkxk91fHbK93N1TfsfhRIYq3ageZ6Ortj55vbPLU2+Uo3a5/SWRztfeqROnaTMwuVPuqzx54ovobJSsFNr91rfo71c+vf4dKufr++za8EwAAAAAAwAatPWNpg+wzp2PmwkByFDH97vkoJXVtb6a+ZNm5/LOTqzOF1vQP/7EcxXePVOvDMXhsKIZenFq97lYhJro7ovPQkRiqnau2I4c6o6P3VMxtsl9T+eNzMZhff/3gU/no6D4RhZuFGOmsPjMJlWqKv+hrvFu+LyZvJJ1V5SsTcaCjIw70N8YYOjYYhx/tiPzAuS89d+E3zTGGY7pcjKn/Va2fGqzfN/y7hSi/fzQ5n4+Bd1d+mUQppp5rnMt3nYrZel8lCi9X37mz+p0d1ff6vN4JAAAAAACwqR0VLNVknx+OlWjp6kzMbtxr6S4Wfj8cgy/PrC6h992+6Kn9LRdi+JuDce4v9d7qQ3KRa66l9/GZ6Ds6tRpgVZU/GI6u3okobMxuavZ1R9fqFKi7qo8zcC6Km3xD6cpE9H1383PVL4nzLw7GiQ9Xk6f+3p7IvTQWR5PjuQuX1r1zlC7H9MeNMvPS0eivz06aifPNAGppLib/aOE+AAAAAADgq+24YCn29Ub/waSO+VhYM/tnK8UPCrHQ3hvjhc9icXExFn93tL6cXOGVobhQz2iycXz6i1j+onrui+X44j+TCOvqqXj7WqOM24UYPXZhJZzqeW02vvh/y/W9nb746HLMfjge3e39MVkdf/a17uSqxlJ49WcuzsboY9WOdeMkz62OsXznszj9ZL0z4tpEDP6svijgBsUofLAQ8fR4XP6kNuZinP9epvrbHI/Rf0nWv/t4Ms6vmYFUuTodM/UqEyPf769XEZ2RX7MZVX5/R1IBAAAAAAB82c4Llr6WbIxenY2zz3VHLleblZSJuD0dk+8nGxM9PxlT31tNWrIvjcZIvSrF9NXGbJ7KB5NxIbk8nn47Cm/2RjbZmyj7zED01m+v7YFUHb+z3t3QWT2u9dX2Rqpev26c7615bqY7Xv/t6yv7Si28e35lT6l1sqMx+9HZGDjYGLe5n1L/D8eTPaOKMbUyA6kS0xcLjTIzEsPPNsqI3jj9X5fi9ZMjMf4fs3HphZRTrQAAAAAAgD1pbwVL35qI0WeSuukvn8X1pIwPx6Pr0XzkV9pInE9OFYuNmUPFT1auju6jA0mIs33rxuntjSQXajg4GAPNjtJcXN9kyb3uH49GbxJorfPkcIzUZkRVFX833dhDamkmLv+x3hWZk8PR2yjrMk8cjdPvvBdnf7QakAEAAAAAAGxm5wVLS/Nx/dOkjp7oSkKUVNoz6wOcmsWF1b2IKqUo3VzfmpOKMvsa048Wq31NPU805xVt393HyUXXE0kZi1G+nZRrVb9lc90x9moSHX06FdO15fA+LsR0vSMTI8fWxkoAAAAAAADp7bhgqXxxKi4kdTzbH31fd5bNNw/Eyk5IL1xK9kHapL3ZCGS6ulf3TZr/y2b7H6WzbpzPm0vWNRWjuBKedUV+m9Oici+NxdF61VgOb75wqRGQ5cZjZOOMLQAAAAAAgJR2VLBU+XQyhv412SuoauDk0D0vRbfiiZ7oScq4OBOf7W/uhbS+Zfc1Luk+uHJ1FN85H/NLycE2rRtnurA6a6rmamFlCb442B+9ybNT23c8Rv+lMaOpeGUqLn3cGD330tDqtzbdmovJV0/EqYsLK7OzAAAAAAAANtPawdKt61G4OB3T75+LE/356Dg0FjPl5NyTZ+PtH2STg6+hfSAmXkuWoqtMxpHvnIqZm0nEUilH8eJEnLmyJnJ5bixGm4+9cSb6B87FzI1SlEoLMfPvg3H45cJKQNP1xJrw6A8XYr62pN3tclRqYdRzE/F6cxm/j8di4CczsVCqjvPpVAwdnVwZo/+V4biXBff6vz/SWPbv6pk4c7VWdMXI8Y2xUjFOPdUXY29NxZljj8fw70VLAAAAAADAV2vtYKl0IcaODcXQixMxdXXNnJ6nX4/ZD8fvKXDZTM/PLsfZJ5ODj8/EkUc7oq2tLdo6OuPAsXNx6l9Pr85Mau+Ntz98feXZ5Q8n4khXPvL5x+PIjwsx/+5YnL6WnPzuYLIkXdW1U3H4keqYj3TH6f+uHrf3xOkrZ1dmEM3/4kg8nq+Oc+hETCfhWfaFy3Hp5D3OyXp2NMbX7j/12PEYPpjUKxZi4UZSVhVvLiYVAAAAAADAl+2cpfAyueh+diTenlmMO3Ono/c+TFZa0d4d43OLcelkT2OWz1qZnhh5Y2h9iPXk6fjsr5di5FvJ8YpM9Jw8GyPNAGff8ZiaGY/udftAlaLwUbKn0rfG4/rfq+N8e8NT23Nx9JezsfCfA3Hvn9kdY6829oWq6XpheHUvqRV9MfxC8wnV73z+fkV1AAAAAADAbtS2XJXU1CxVonyr3FiKrj0buf1fiprWqZRLUa5fnIns/mxk1oVIibVjZqpjZr88Zqpxtmn+J/k4/IvaTK/uOPvXz2L8iUb/etV3K5Uj9q3uIwUAAAAAALAZwdIuVfn0TPQdOhXztYNn34vFmZG4x0X1AAAAAAAA6gRLu8z8Gwdi4N2FKJXq05+qeuJs8XqMf2nZPgAAAAAAgO3ZOXsskUIprv+5uCZUysXIn2aESgAAAAAAwH1hxtKuUo7ilZko3q7V2eh+rj+67ZsEAAAAAADcJ4IlAAAAAAAAUrEUHgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAKkIlgAAAAAAAEhFsAQAAAAAAEAqgiUAAAAAAABSESwBAAAAAACQimAJAAAAAACAVARLAAAAAAAApCJYAgAAAAAAIBXBEgAAAAAAAClE/H9LeYfE5WWmKAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"gpt2sampleTimer.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f72407",
   "metadata": {},
   "source": [
    "I think Snake cannot give me enough detail about how I can better see calls and instuctions running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4a310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
